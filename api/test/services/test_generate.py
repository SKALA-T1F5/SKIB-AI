import logging

from api.question.schemas.question import (
    DifficultyLevel,
    GenerationType,
    GradingCriterion,
    QuestionResponse,
    QuestionType,
)
from api.test.schemas.test_generation_status import (
    TestGenerationStatus,
)
from api.websocket.services.springboot_notifier import (
    notify_test_generation_progress,
    notify_test_generation_result,
)
from src.agents.question_generator.agent import QuestionGeneratorAgent

logger = logging.getLogger(__name__)


async def test_generation_background(
    task_id: str, test_id: int, request_data: dict
) -> dict:
    """
    í…ŒìŠ¤íŠ¸ ìƒì„± ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ë¡œì§ (ê¸°ì¡´ generate_test_questions ë¡œì§ì„ ì—…ê·¸ë ˆì´ë“œ)
    """
    try:
        logger.info(f"ğŸš€ í…ŒìŠ¤íŠ¸ ìƒì„± ì‹œì‘: test_id={test_id}, task_id={task_id}")

        # 1. í…ŒìŠ¤íŠ¸ ì„¤ê³„ì•ˆ ë¡œë“œ
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.LOADING_TEST_PLAN
        )

        # 2. í…ŒìŠ¤íŠ¸ ê³„íš ë°˜ì˜
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.REFLECTING_TEST_PLAN
        )

        # ê¸°ì¡´ ë¡œì§ ì‹œì‘: QuestionGeneratorAgent ì´ˆê¸°í™” ë° ë°ì´í„° ë³€í™˜

        agent = QuestionGeneratorAgent()

        # ë¬¸ì„œë³„ ì„¤ì •ì„ document_test_plan_data í˜•íƒœë¡œ ë³€í™˜
        document_plans = []
        for doc_config in request_data["document_configs"]:
            document_plan = {
                "document_name": doc_config["document_name"],
                "document_id": doc_config["documentId"],
                "keywords": doc_config["keywords"],
                "recommended_questions": {
                    "objective": doc_config["configured_objective_count"],
                    "subjective": doc_config["configured_subjective_count"],
                },
            }
            document_plans.append(document_plan)

        # 3. ë¬¸ë§¥ ê²€ìƒ‰
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.RETRIEVING_CONTEXTS
        )

        # test_plan ë°ì´í„° êµ¬ì„±
        total_test_plan_data = {
            "test_summary": request_data["summary"],
            "difficulty": request_data["difficulty_level"].lower(),
            "total_objective": sum(
                doc["configured_objective_count"]
                for doc in request_data["document_configs"]
            ),
            "total_subjective": sum(
                doc["configured_subjective_count"]
                for doc in request_data["document_configs"]
            ),
        }

        document_test_plan_data = {"document_plans": document_plans}

        # 4. ë¬¸ë§¥ ì „ì²˜ë¦¬
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.PREPROCESSING_CONTEXTS
        )

        # 5. ë¬¸ì œ ìƒì„± (ê¸°ì¡´ Agent í˜¸ì¶œ)
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.GENERATING_QUESTIONS
        )

        result = agent.generate_enhanced_questions_from_test_plans(
            total_test_plan_data=total_test_plan_data,
            document_test_plan_data=document_test_plan_data,
        )

        if result.get("status") == "failed":
            raise Exception(result.get("error", "ë¬¸ì œ ìƒì„± ì‹¤íŒ¨"))

        # 6. ë¬¸ì œ í›„ì²˜ë¦¬
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.POSTPROCESSING_QUESTIONS
        )

        # ìƒì„±ëœ ë¬¸ì œë¥¼ QuestionResponse í˜•íƒœë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§)
        questions = []
        generated_questions = result.get("questions", [])

        for q in generated_questions:
            question_type = QuestionType(q.get("type"))

            # ë‚œì´ë„ ë§¤í•‘
            difficulty_map = {
                "easy": DifficultyLevel.easy,
                "medium": DifficultyLevel.normal,
                "hard": DifficultyLevel.hard,
            }
            difficulty = difficulty_map.get(
                q.get("difficulty", "medium"), DifficultyLevel.normal
            )

            valid_criteria_fields = {"score", "criteria", "example", "note"}
            raw_criteria = q.get("grading_criteria", [])

            grading_criteria = [
                GradingCriterion(
                    **{k: v for k, v in criterion.items() if k in valid_criteria_fields}
                )
                for criterion in raw_criteria
                if isinstance(criterion, dict)
            ]

            question_response = QuestionResponse(
                type=question_type,
                generationType=GenerationType(q.get("generation_type").upper()),
                difficulty_level=difficulty,
                question=q.get("question", ""),
                options=(
                    q.get("options")
                    if question_type == QuestionType.objective
                    else None
                ),
                answer=q.get("answer", ""),
                explanation=q.get("explanation"),
                grading_criteria=(
                    grading_criteria
                    if question_type == QuestionType.subjective
                    else None
                ),
                documentId=q.get("document_id", 0),
                document_name=q.get("document_name", ""),
                keywords=q.get("source_keywords", []),
                tags=q.get("tags", []),
            )
            questions.append(question_response)

        # 7. ê²°ê³¼ ìµœì¢…í™”
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.FINALIZING_RESULTS
        )

        # ìµœì¢… ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        final_result = {
            "test_id": test_id,
            "questions": [
                q.model_dump(by_alias=True, exclude_none=False) for q in questions
            ],
            "total_questions": len(questions),
            "metadata": {
                "name": request_data["name"],
                "summary": request_data["summary"],
                "difficulty_level": request_data["difficulty_level"],
                "limited_time": request_data["limited_time"],
                "pass_score": request_data["pass_score"],
                "is_retake": request_data["is_retake"],
            },
        }

        logger.info(f"ğŸ§ª Test ID: {final_result.get('test_id')}")
        logger.info(f"ğŸ“Š Total Questions: {final_result.get('total_questions')}")
        logger.info(f"ğŸ“ Metadata: {final_result.get('metadata')}")

        await notify_test_generation_result(
            task_id=task_id, test_id=test_id, result_data=final_result
        )

        # 8. ì™„ë£Œ
        await notify_test_generation_progress(
            task_id, test_id, TestGenerationStatus.COMPLETED
        )

        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: test_id={test_id}")

        return {"status": "success", "test_id": test_id, "questions": questions}

    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: test_id={test_id}, error={str(e)}")

        # ì‹¤íŒ¨ ì•Œë¦¼
        from api.websocket.schemas.task_progress import TaskStatus
        from api.websocket.services.progress_tracker import save_task_progress

        await save_task_progress(
            task_id=task_id,
            status=TaskStatus.FAILED,
            progress=0.0,
            message=f"í…ŒìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}",
        )

        raise e
