"""
í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ ë¶„í•´í•˜ê³  (parse_pdf_to_docling_blocks)
LangChain Documentë¡œ ë³€í™˜ ë° ë¶„í•  (block_to_documents, split_docs)
GPT-4o Vision ë©”ì‹œì§€ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (docling_blocks_to_vision_messages)
ê° ì²­í¬ì— ëŒ€í•´:
- ë²¡í„° ì„ë² ë”© ìƒì„± (SentenceTransformer)
- Vision ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± (generate_question)
- ì§ˆë¬¸ê³¼ ë©”íƒ€ë°ì´í„° ì €ì¥ (save_question_result)
ìµœì¢…ì ìœ¼ë¡œëŠ” PDF í•œ ê°œì— ëŒ€í•´ ë¬¸í•­ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

from agents.question_generator.unified_parser import parse_pdf_unified
from agents.question_generator.chunking import block_to_documents, split_docs
from agents.question_generator.generate_questions import generate_question
from agents.question_generator.save_results import save_question_result
from agents.question_generator.preprocess_docling import docling_blocks_to_vision_messages
from agents.question_generator.change_name import normalize_collection_name
from db.vectorDB.weaviate_utils import upload_chunk_to_collection
from sentence_transformers import SentenceTransformer
import os
import sys
import time

# ì„ë² ë”© ëª¨ë¸ ë¡œë”© (bge ëª¨ë¸ ì‚¬ìš©)
embedding_model = SentenceTransformer("BAAI/bge-base-en")


def run_pipeline(pdf_path: str, collection_name: str):
    # 1. PDFë¥¼ í†µí•© íŒŒì„œë¡œ ë³€í™˜
    print("ğŸ“„ í†µí•© íŒŒì„œ ì‚¬ìš©")
    blocks = parse_pdf_unified(pdf_path, collection_name)

    # 2. Docling ë¸”ë¡ì„ Vision API ì…ë ¥ í˜•ì‹ì˜ ë©”ì‹œì§€ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„°ë¡œ ë³€í™˜
    # ì´ í•¨ìˆ˜ëŠ” ì´ì œ ê° ì²­í¬ì— ëŒ€í•œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # ì˜ˆ: [{'messages': [...], 'metadata': {'pages': [...], 'source_text_combined': "..."}}, ...]
    processed_vision_chunks = docling_blocks_to_vision_messages(blocks)


    source_file_name = os.path.basename(pdf_path)

    # 4. ê° processed_vision_chunkì— ëŒ€í•´ ì§ˆë¬¸ ìƒì„± ë° ì €ì¥ ë°˜ë³µ
    for i, vision_data in enumerate(processed_vision_chunks):
        messages_for_api = vision_data['messages']
        chunk_metadata = vision_data['metadata']

        # chunk_obj êµ¬ì„± ì‹œ, processed_vision_chunksì—ì„œ ë°˜í™˜ëœ ë©”íƒ€ë°ì´í„° í™œìš©
        page_numbers = chunk_metadata.get("pages", [])
        # í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì˜ˆ: "p3, p4-5")í•˜ê±°ë‚˜ ì²« í˜ì´ì§€ë§Œ ì‚¬ìš© ë“± ê²°ì • í•„ìš”
        page_info_for_chunk = str(page_numbers[0]) if page_numbers else "N/A"
        
        section_titles = chunk_metadata.get("sections", [])
        section_info_for_chunk = ", ".join(section_titles) if section_titles else ""

        # ì²­í¬ ë©”íƒ€ë°ì´í„° ê°ì²´ êµ¬ì„± (save_question_result ë° DB ì—…ë¡œë“œìš©)
        chunk_obj_for_saving = {
            "chunk_id": f"{collection_name}_vision_c{i}", # ID ì²´ê³„ ë³€ê²½ ê°€ëŠ¥
            "chunk_type": "vision_processed_chunk", # ì²­í¬ íƒ€ì… ëª…ì‹œ
            "section_title": section_info_for_chunk,
            "source_text": chunk_metadata.get("source_text_combined", ""), # ê²°í•©ëœ ì›ë³¸ í…ìŠ¤íŠ¸
            "project": collection_name,
            "source": source_file_name,
            "page": page_info_for_chunk, # í˜ì´ì§€ ì •ë³´ ì‚¬ìš©
        }

        # ë²¡í„° ì„ë² ë”©ì€ source_text_combined ì „ì²´ì— ëŒ€í•´ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ
        if chunk_obj_for_saving["source_text"]:
            vector = embedding_model.encode(chunk_obj_for_saving["source_text"]).tolist()
            upload_chunk_to_collection(chunk_obj_for_saving, vector, collection_name)
        else:
            vector = [] # ë¹ˆ í…ìŠ¤íŠ¸ì˜ ê²½ìš° ë¹ˆ ë²¡í„°

        # GPT-4o Vision APIë¥¼ í†µí•´ ì§ˆë¬¸ ìƒì„±
        # generate_question í˜¸ì¶œ ì‹œ sourceì™€ pageëŠ” chunk_obj_for_savingì˜ ê°’ì„ ì‚¬ìš©
        # num_objectiveì™€ num_subjectiveëŠ” generate_question í•¨ìˆ˜ì˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì—¬ê¸°ì„œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        questions_list = generate_question(
            messages=messages_for_api, 
            source=source_file_name, 
            page=page_info_for_chunk,
            num_objective=3,  # ì˜ˆì‹œ: ê°ê´€ì‹ 3ê°œ
            num_subjective=3  # ì˜ˆì‹œ: ì£¼ê´€ì‹ 3ê°œ
            # difficultyëŠ” generate_questionì˜ ê¸°ë³¸ê°’(3) ì‚¬ìš© ë˜ëŠ” chunk_obj_for_saving ë“±ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
        )
        
        # ìƒì„±ëœ ë¬¸í•­ê³¼ ë©”íƒ€ë°ì´í„° ì €ì¥
        # save_question_resultëŠ” ì´ì œ chunk_infoì™€ questions_listë¥¼ ë°›ìŠµë‹ˆë‹¤.
        save_question_result(chunk_info=chunk_obj_for_saving, questions_list=questions_list)
        
        time.sleep(1) # API í˜¸ì¶œ ê°„ ì§€ì—° ì‹œê°„ ìœ ì§€

    print(f"âœ… ë¬¸ì„œ '{collection_name}' ë¬¸ì œ ìƒì„± ì™„ë£Œ")




# í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python -m agents.question_generator.run_pipeline <pdf_path>")
        sys.exit(1)

    pdf_path = sys.argv[1]
    
    filename = os.path.splitext(os.path.basename(pdf_path))[0]
    collection_name = normalize_collection_name(filename)
    
    run_pipeline(pdf_path, collection_name)
