"""
ChromaDB íŒŒì´í”„ë¼ì¸ - ë¬¸ì„œ ì²˜ë¦¬ë¶€í„° ì—…ë¡œë“œê¹Œì§€ì˜ í†µí•© ì›Œí¬í”Œë¡œìš°
"""

import logging
from typing import Any, Dict, List

from .client import get_client
from .search import ChromaDBSearcher
from .upload import ChromaDBUploader, DuplicateAction
from .utils import get_collection_info, list_collections

logger = logging.getLogger(__name__)


class ChromaDBPipeline:
    """ChromaDB í†µí•© íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self, 
        embedding_model: str = "BAAI/bge-base-en",
        duplicate_action: DuplicateAction = DuplicateAction.SKIP
    ):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            embedding_model: ìž„ë² ë”© ëª¨ë¸ëª…
            duplicate_action: ì¤‘ë³µ ë°œê²¬ ì‹œ ì²˜ë¦¬ ë°©ì‹
        """
        self.client = get_client()
        self.uploader = ChromaDBUploader(embedding_model, duplicate_action)
        self.searcher = ChromaDBSearcher(embedding_model)
        self.duplicate_action = duplicate_action
        logger.info(f"ðŸš€ ChromaDB íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ðŸ”„ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì‹: {duplicate_action.value}")

    def process_and_upload_document(
        self,
        document_blocks: List[Dict[str, Any]],
        collection_name: str,
        source_file: str,
        recreate_collection: bool = False,
        duplicate_action: DuplicateAction = None,
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ë¸”ë¡ ì²˜ë¦¬ ë° ì—…ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ í¬í•¨)

        Args:
            document_blocks: ë¬¸ì„œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            source_file: ì†ŒìŠ¤ íŒŒì¼ëª…
            recreate_collection: ì»¬ë ‰ì…˜ ìž¬ìƒì„± ì—¬ë¶€
            duplicate_action: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì‹ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            logger.info(f"ðŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œìž‘: {source_file} â†’ {collection_name}")

            # ì»¬ë ‰ì…˜ ìž¬ìƒì„± ì²˜ë¦¬
            if recreate_collection:
                from .utils import create_or_get_collection

                create_or_get_collection(collection_name, recreate=True)

            # ë¬¸ì„œ ë¸”ë¡ ì—…ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ ì ìš©)
            action = duplicate_action or self.duplicate_action
            uploaded_count = self.uploader.upload_document_blocks(
                document_blocks, collection_name, source_file, duplicate_action=action
            )

            # ê²°ê³¼ ì •ë³´ ìˆ˜ì§‘
            collection_info = get_collection_info(collection_name)

            result = {
                "status": "success",
                "collection_name": collection_name,
                "source_file": source_file,
                "uploaded_count": uploaded_count,
                "total_blocks": len(document_blocks),
                "collection_total": collection_info.get("count", 0),
                "upload_success_rate": (
                    uploaded_count / len(document_blocks) if document_blocks else 0
                ),
            }

            logger.info(
                f"âœ… ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}/{len(document_blocks)}ê°œ"
            )
            return result

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e),
                "uploaded_count": 0,
                "total_blocks": len(document_blocks),
            }

    def search_and_analyze(
        self,
        query: str,
        collection_name: str,
        n_results: int = 5,
        metadata_filter: Dict[str, Any] = None,
        include_analysis: bool = True,
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ë° ê²°ê³¼ ë¶„ì„

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            metadata_filter: ë©”íƒ€ë°ì´í„° í•„í„°
            include_analysis: ë¶„ì„ ì •ë³´ í¬í•¨ ì—¬ë¶€

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë° ë¶„ì„ ì •ë³´
        """
        try:
            logger.info(f"ðŸ” ê²€ìƒ‰ ì‹œìž‘: '{query}' in {collection_name}")

            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            results = self.searcher.search_similar(
                query=query,
                collection_name=collection_name,
                n_results=n_results,
                where=metadata_filter,
            )

            response = {
                "query": query,
                "collection_name": collection_name,
                "results": results,
                "result_count": len(results),
            }

            # ë¶„ì„ ì •ë³´ ì¶”ê°€
            if include_analysis and results:
                analysis = {
                    "avg_similarity": sum(r["similarity"] for r in results)
                    / len(results),
                    "max_similarity": max(r["similarity"] for r in results),
                    "min_similarity": min(r["similarity"] for r in results),
                    "source_distribution": {},
                    "type_distribution": {},
                }

                # ì†ŒìŠ¤ ë° íƒ€ìž… ë¶„í¬ ê³„ì‚°
                for result in results:
                    metadata = result.get("metadata", {})
                    source = metadata.get("source", "unknown")
                    chunk_type = metadata.get("chunk_type", "unknown")

                    analysis["source_distribution"][source] = (
                        analysis["source_distribution"].get(source, 0) + 1
                    )
                    analysis["type_distribution"][chunk_type] = (
                        analysis["type_distribution"].get(chunk_type, 0) + 1
                    )

                response["analysis"] = analysis

            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return response

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                "query": query,
                "collection_name": collection_name,
                "results": [],
                "result_count": 0,
                "error": str(e),
            }

    def bulk_process_documents(
        self, documents: List[Dict[str, Any]], base_collection_name: str = None
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œ ì¼ê´„ ì²˜ë¦¬

        Args:
            documents: ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                      [{"blocks": [...], "source_file": "...", "collection_name": "..."}, ...]
            base_collection_name: ê¸°ë³¸ ì»¬ë ‰ì…˜ ì´ë¦„ (ê°œë³„ ì»¬ë ‰ì…˜ëª…ì´ ì—†ì„ ë•Œ ì‚¬ìš©)

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []

        for i, doc in enumerate(documents):
            try:
                blocks = doc.get("blocks", [])
                source_file = doc.get("source_file", f"document_{i}")
                collection_name = doc.get(
                    "collection_name", base_collection_name or f"collection_{i}"
                )

                result = self.process_and_upload_document(
                    blocks, collection_name, source_file
                )

                results.append(result)

            except Exception as e:
                logger.error(f"âŒ ë¬¸ì„œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                results.append(
                    {
                        "status": "error",
                        "error": str(e),
                        "source_file": doc.get("source_file", f"document_{i}"),
                    }
                )

        # ì „ì²´ í†µê³„
        total_uploaded = sum(r.get("uploaded_count", 0) for r in results)
        total_blocks = sum(r.get("total_blocks", 0) for r in results)
        success_count = sum(1 for r in results if r.get("status") == "success")

        logger.info(
            f"ðŸ“Š ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(documents)}ê°œ ë¬¸ì„œ, {total_uploaded}/{total_blocks}ê°œ ë¸”ë¡"
        )

        return results

    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""
        try:
            from .utils import get_collection_stats

            stats = get_collection_stats()
            client_info = self.client.get_info()

            status = {
                "client_status": (
                    "connected" if self.client.test_connection() else "disconnected"
                ),
                "client_info": client_info,
                "database_stats": stats,
                "collections": list_collections(),
            }

            return status

        except Exception as e:
            logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}


# íŽ¸ì˜ í•¨ìˆ˜ë“¤
def quick_upload(
    document_blocks: List[Dict[str, Any]],
    collection_name: str,
    source_file: str = "document",
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ë¬¸ì„œ ì—…ë¡œë“œ"""
    pipeline = ChromaDBPipeline()
    return pipeline.process_and_upload_document(
        document_blocks, collection_name, source_file
    )


def quick_search(
    query: str, collection_name: str, n_results: int = 5
) -> List[Dict[str, Any]]:
    """ë¹ ë¥¸ ê²€ìƒ‰"""
    pipeline = ChromaDBPipeline()
    result = pipeline.search_and_analyze(
        query, collection_name, n_results, include_analysis=False
    )
    return result.get("results", [])
