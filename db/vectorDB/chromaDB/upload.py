"""
ChromaDB ë¬¸ì„œ ì—…ë¡œë“œ ë° ì„ë² ë”© ìƒì„±
"""

import hashlib
import logging
from enum import Enum
from typing import Any, Dict, List, Optional

from sentence_transformers import SentenceTransformer

from .client import get_client
from .utils import create_or_get_collection

logger = logging.getLogger(__name__)


class DuplicateAction(Enum):
    """ì¤‘ë³µ ë°œê²¬ ì‹œ ì²˜ë¦¬ ë°©ì‹"""
    SKIP = "skip"          # ì¤‘ë³µ ì‹œ ìŠ¤í‚µ
    OVERWRITE = "overwrite"  # ì¤‘ë³µ ì‹œ ë®ì–´ì“°ê¸°
    ERROR = "error"        # ì¤‘ë³µ ì‹œ ì—ëŸ¬ ë°œìƒ


def generate_content_hash(content: str, metadata: Optional[Dict] = None) -> str:
    """
    ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ê³ ìœ  í•´ì‹œ ID ìƒì„±
    
    Args:
        content: ë¬¸ì„œ ë‚´ìš©
        metadata: ë©”íƒ€ë°ì´í„° (íŒŒì¼ëª…, í¬ê¸° ë“±)
    
    Returns:
        str: ê³ ìœ  í•´ì‹œ ID
    """
    # ë‚´ìš© ê¸°ë°˜ í•´ì‹œ
    content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì •ë³´ (ìˆëŠ” ê²½ìš°)
    if metadata:
        # íŒŒì¼ëª…, í˜ì´ì§€, ë¸”ë¡ íƒ€ì… ë“±ìœ¼ë¡œ ì„¸ë¶„í™”
        extra_info = f"{metadata.get('source_file', '')}"
        extra_info += f"_p{metadata.get('page', 0)}"
        extra_info += f"_{metadata.get('element_type', 'text')}"
        extra_info += f"_{metadata.get('element_index', 0)}"
        
        extra_hash = hashlib.sha256(extra_info.encode('utf-8')).hexdigest()[:8]
        return f"{content_hash}_{extra_hash}"
    
    return content_hash


def check_document_exists(collection, chunk_id: str) -> bool:
    """
    ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    
    Args:
        collection: ChromaDB ì»¬ë ‰ì…˜
        chunk_id: í™•ì¸í•  ë¬¸ì„œ ID
    
    Returns:
        bool: ì¡´ì¬ ì—¬ë¶€
    """
    try:
        result = collection.get(ids=[chunk_id])
        return len(result.get('ids', [])) > 0
    except Exception as e:
        logger.warning(f"ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


class ChromaDBUploader:
    """ChromaDB ì—…ë¡œë“œ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self, 
        embedding_model: str = "BAAI/bge-base-en",
        duplicate_action: DuplicateAction = DuplicateAction.SKIP
    ):
        """
        ì—…ë¡œë” ì´ˆê¸°í™”

        Args:
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
            duplicate_action: ì¤‘ë³µ ë°œê²¬ ì‹œ ì²˜ë¦¬ ë°©ì‹
        """
        self.client = get_client()
        self.embedding_model = SentenceTransformer(embedding_model)
        self.duplicate_action = duplicate_action
        logger.info(f"ğŸ§® ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {embedding_model}")
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì‹: {duplicate_action.value}")

    def upload_chunk(
        self,
        content: str,
        collection_name: str,
        metadata: Dict[str, Any] = None,
        chunk_id: str = None,
        duplicate_action: Optional[DuplicateAction] = None,
    ) -> bool:
        """
        ë‹¨ì¼ ì²­í¬ ì—…ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ í¬í•¨)

        Args:
            content: í…ìŠ¤íŠ¸ ë‚´ìš©
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            metadata: ë©”íƒ€ë°ì´í„°
            chunk_id: ì²­í¬ ID (ì—†ìœ¼ë©´ í•´ì‹œ ê¸°ë°˜ ìë™ ìƒì„±)
            duplicate_action: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì‹ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            ì—…ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            collection = create_or_get_collection(
                collection_name, self.client.get_client()
            )

            if not content or not isinstance(content, str):
                logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì½˜í…ì¸ ")
                return False

            # ë©”íƒ€ë°ì´í„° ì •ë¦¬ (ChromaDBëŠ” íŠ¹ì • íƒ€ì…ë§Œ ì§€ì›)
            clean_metadata = self._clean_metadata(metadata or {})
            clean_metadata.update(
                {"project": collection_name, "upload_method": "chromadb_uploader"}
            )

            # ì²­í¬ ID ìƒì„± (í•´ì‹œ ê¸°ë°˜)
            if not chunk_id:
                chunk_id = f"{collection_name}_{generate_content_hash(content, metadata)}"

            # ì¤‘ë³µ ê²€ì‚¬ ë° ì²˜ë¦¬
            action = duplicate_action or self.duplicate_action
            if check_document_exists(collection, chunk_id):
                if action == DuplicateAction.SKIP:
                    logger.info(f"â­ï¸ ì¤‘ë³µ ë¬¸ì„œ ìŠ¤í‚µ: {chunk_id}")
                    return True
                elif action == DuplicateAction.ERROR:
                    logger.error(f"âŒ ì¤‘ë³µ ë¬¸ì„œ ë°œê²¬: {chunk_id}")
                    raise ValueError(f"ì¤‘ë³µ ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {chunk_id}")
                elif action == DuplicateAction.OVERWRITE:
                    logger.info(f"ğŸ”„ ì¤‘ë³µ ë¬¸ì„œ ë®ì–´ì“°ê¸°: {chunk_id}")
                    # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
                    try:
                        collection.delete(ids=[chunk_id])
                    except Exception as e:
                        logger.warning(f"ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")

            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_model.encode(content).tolist()

            # ChromaDBì— ì¶”ê°€
            collection.add(
                documents=[content],
                metadatas=[clean_metadata],
                embeddings=[embedding],
                ids=[chunk_id],
            )

            logger.debug(f"âœ… ì²­í¬ ì—…ë¡œë“œ ì„±ê³µ: {chunk_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def batch_upload(
        self, 
        chunks: List[Dict[str, Any]], 
        collection_name: str, 
        batch_size: int = 50,
        duplicate_action: Optional[DuplicateAction] = None
    ) -> Dict[str, int]:
        """
        ë°°ì¹˜ ì—…ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ í¬í•¨)

        Args:
            chunks: ì²­í¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            batch_size: ë°°ì¹˜ í¬ê¸°
            duplicate_action: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì‹ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            ì—…ë¡œë“œ í†µê³„ {"successful": int, "failed": int, "total": int, "skipped": int, "overwritten": int}
        """
        try:
            collection = create_or_get_collection(
                collection_name, self.client.get_client()
            )

            successful = 0
            failed = 0
            skipped = 0
            overwritten = 0
            action = duplicate_action or self.duplicate_action

            for i in range(0, len(chunks), batch_size):
                batch = chunks[i : i + batch_size]

                try:
                    documents = []
                    metadatas = []
                    embeddings = []
                    ids = []

                    for j, chunk in enumerate(batch):
                        content = chunk.get("content", "")
                        if not content or not isinstance(content, str):
                            failed += 1
                            continue

                        # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                        metadata = self._clean_metadata(chunk.get("metadata", {}))
                        metadata.update(
                            {
                                "chunk_type": chunk.get("type", "text"),
                                "source": chunk.get("source", "unknown"),
                                "project": collection_name,
                            }
                        )

                        # í•´ì‹œ ê¸°ë°˜ ID ìƒì„±
                        chunk_id = chunk.get("id")
                        if not chunk_id:
                            chunk_id = f"{collection_name}_{generate_content_hash(content, metadata)}"

                        # ì¤‘ë³µ ê²€ì‚¬ ë° ì²˜ë¦¬
                        if check_document_exists(collection, chunk_id):
                            if action == DuplicateAction.SKIP:
                                skipped += 1
                                logger.debug(f"â­ï¸ ì¤‘ë³µ ë¬¸ì„œ ìŠ¤í‚µ: {chunk_id}")
                                continue
                            elif action == DuplicateAction.ERROR:
                                failed += 1
                                logger.error(f"âŒ ì¤‘ë³µ ë¬¸ì„œ ë°œê²¬: {chunk_id}")
                                continue
                            elif action == DuplicateAction.OVERWRITE:
                                overwritten += 1
                                logger.debug(f"ğŸ”„ ì¤‘ë³µ ë¬¸ì„œ ë®ì–´ì“°ê¸°: {chunk_id}")
                                # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
                                try:
                                    collection.delete(ids=[chunk_id])
                                except Exception as e:
                                    logger.warning(f"ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")

                        # ì„ë² ë”© ìƒì„±
                        embedding = self.embedding_model.encode(content).tolist()

                        documents.append(content)
                        metadatas.append(metadata)
                        embeddings.append(embedding)
                        ids.append(chunk_id)

                    if documents:
                        collection.add(
                            documents=documents,
                            metadatas=metadatas,
                            embeddings=embeddings,
                            ids=ids,
                        )
                        successful += len(documents)
                        logger.info(
                            f"âœ… ë°°ì¹˜ {i//batch_size + 1} ì—…ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ"
                        )

                except Exception as e:
                    logger.error(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                    failed += len(batch)

            result = {
                "successful": successful, 
                "failed": failed, 
                "total": len(chunks),
                "skipped": skipped,
                "overwritten": overwritten
            }

            logger.info(f"ğŸ“Š ë°°ì¹˜ ì—…ë¡œë“œ ì™„ë£Œ: {successful}/{len(chunks)}ê°œ ì„±ê³µ, {skipped}ê°œ ìŠ¤í‚µ, {overwritten}ê°œ ë®ì–´ì“°ê¸°")
            return result

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "successful": 0, 
                "failed": len(chunks), 
                "total": len(chunks),
                "skipped": 0,
                "overwritten": 0
            }

    def upload_document_blocks(
        self,
        blocks: List[Dict[str, Any]],
        collection_name: str,
        source_file: str = "document",
        duplicate_action: Optional[DuplicateAction] = None,
    ) -> int:
        """
        ë¬¸ì„œ ë¸”ë¡ë“¤ì„ ì—…ë¡œë“œ (ì´ë¯¸ì§€ ë¸”ë¡ í¬í•¨, ì¤‘ë³µ ë°©ì§€)

        Args:
            blocks: ë¬¸ì„œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            source_file: ì†ŒìŠ¤ íŒŒì¼ëª…
            duplicate_action: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì‹

        Returns:
            ì—…ë¡œë“œëœ ì²­í¬ ìˆ˜
        """
        chunks = []

        for i, block in enumerate(blocks):
            block_type = block.get("type", "text")

            # ì´ë¯¸ì§€ ë¸”ë¡ íŠ¹ë³„ ì²˜ë¦¬
            if block_type == "image":
                content = self._process_image_block(block)
            else:
                content = str(block.get("content", ""))

            # ë¹ˆ ì½˜í…ì¸  ìŠ¤í‚µ
            if not content.strip():
                logger.warning(f"ë¹ˆ ì½˜í…ì¸  ë¸”ë¡ ìŠ¤í‚µ: {block_type} ë¸”ë¡ {i}")
                continue

            chunk = {
                "content": content,
                "type": block_type,
                "source": source_file,
                "id": f"{collection_name}_{i}",
                "metadata": {
                    **block.get("metadata", {}),
                    "block_index": i,
                    "chunk_id": f"{collection_name}_{i}",
                },
            }
            chunks.append(chunk)

        result = self.batch_upload(chunks, collection_name, duplicate_action=duplicate_action)
        return result["successful"]

    def _process_image_block(self, block: Dict[str, Any]) -> str:
        """
        ì´ë¯¸ì§€ ë¸”ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦

        Args:
            block: ì´ë¯¸ì§€ ë¸”ë¡ ì •ë³´

        Returns:
            ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ë‚´ìš©
        """
        try:
            metadata = block.get("metadata", {})

            # ì´ë¯¸ì§€ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            image_info = []

            # ê¸°ë³¸ ì •ë³´
            if metadata.get("page"):
                image_info.append(f"í˜ì´ì§€ {metadata['page']}ì˜ ì´ë¯¸ì§€")

            # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
            if metadata.get("width") and metadata.get("height"):
                image_info.append(f"í¬ê¸°: {metadata['width']}x{metadata['height']}")

            # íŒŒì¼ ê²½ë¡œì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            image_path = block.get("content", "")
            if isinstance(image_path, str) and image_path:
                import os

                filename = os.path.basename(image_path)
                image_info.append(f"íŒŒì¼ëª…: {filename}")

                # íŒŒì¼ëª…ì—ì„œ ì˜ë¯¸ ì¶”ì¶œ
                if "table" in filename.lower():
                    image_info.append("í‘œ í˜•íƒœì˜ ì´ë¯¸ì§€")
                elif "chart" in filename.lower():
                    image_info.append("ì°¨íŠ¸ ë˜ëŠ” ê·¸ë˜í”„ ì´ë¯¸ì§€")
                elif "diagram" in filename.lower():
                    image_info.append("ë‹¤ì´ì–´ê·¸ë¨ ì´ë¯¸ì§€")
                elif "screenshot" in filename.lower():
                    image_info.append("í™”ë©´ ìº¡ì²˜ ì´ë¯¸ì§€")
                else:
                    image_info.append("ì¼ë°˜ ì´ë¯¸ì§€")

            # OCR í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ í¬í•¨
            if metadata.get("ocr_text"):
                image_info.append(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {metadata['ocr_text']}")

            # ì´ë¯¸ì§€ ì„¤ëª…ì´ ìˆë‹¤ë©´ í¬í•¨
            if metadata.get("description"):
                image_info.append(f"ì„¤ëª…: {metadata['description']}")

            # ìº¡ì…˜ì´ ìˆë‹¤ë©´ í¬í•¨
            if metadata.get("caption"):
                image_info.append(f"ìº¡ì…˜: {metadata['caption']}")

            content = " | ".join(image_info) if image_info else "ì´ë¯¸ì§€ ë¸”ë¡"

            logger.debug(f"ì´ë¯¸ì§€ ë¸”ë¡ ë³€í™˜: {content[:100]}...")
            return content

        except Exception as e:
            logger.warning(f"ì´ë¯¸ì§€ ë¸”ë¡ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"ì´ë¯¸ì§€ ë¸”ë¡ (ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]})"

    def _clean_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì •ë¦¬ (ChromaDB í˜¸í™˜ì„±)"""
        clean_metadata = {}

        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)):
                clean_metadata[key] = value
            elif value is not None:
                clean_metadata[key] = str(value)

        return clean_metadata


# í¸ì˜ í•¨ìˆ˜ë“¤
def upload_documents(
    blocks: List[Dict[str, Any]], collection_name: str, source_file: str = "document"
) -> int:
    """ë¬¸ì„œ ë¸”ë¡ ì—…ë¡œë“œ í¸ì˜ í•¨ìˆ˜"""
    from .collection_utils import get_safe_collection_name
    
    # Collection ì´ë¦„ ì •ê·œí™”
    normalized_collection_name = get_safe_collection_name(collection_name)
    
    uploader = ChromaDBUploader()
    return uploader.upload_document_blocks(blocks, normalized_collection_name, source_file)


def upload_chunks(
    chunks: List[Dict[str, Any]], collection_name: str, batch_size: int = 50
) -> Dict[str, int]:
    """ì²­í¬ ë°°ì¹˜ ì—…ë¡œë“œ í¸ì˜ í•¨ìˆ˜"""
    from .collection_utils import get_safe_collection_name
    
    # Collection ì´ë¦„ ì •ê·œí™”
    normalized_collection_name = get_safe_collection_name(collection_name)
    
    uploader = ChromaDBUploader()
    return uploader.batch_upload(chunks, normalized_collection_name, batch_size)


def batch_upload(
    chunks: List[Dict[str, Any]], collection_name: str, batch_size: int = 50
) -> Dict[str, int]:
    """ë°°ì¹˜ ì—…ë¡œë“œ í¸ì˜ í•¨ìˆ˜ (ë³„ì¹­)"""
    return upload_chunks(chunks, collection_name, batch_size)
