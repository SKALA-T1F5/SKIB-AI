"""
JSON íŒŒì¼ì˜ main_topics í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import os
from typing import Any, Dict, List

from sentence_transformers import SentenceTransformer

from .weaviate_utils import get_client


class TopicSearcher:
    def __init__(self, embedding_model_name: str = "BAAI/bge-base-en"):
        """
        TopicSearcher ì´ˆê¸°í™”

        Args:
            embedding_model_name: ì„ë² ë”©ì— ì‚¬ìš©í•  ëª¨ë¸ëª…
        """
        self.client = get_client()
        self.embedding_model = SentenceTransformer(embedding_model_name)

    def extract_main_topics_from_json(self, json_file_path: str) -> List[str]:
        """
        JSON íŒŒì¼ì—ì„œ main_topics ì¶”ì¶œ

        Args:
            json_file_path: JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            main_topics ë¦¬ìŠ¤íŠ¸
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            main_topics = data.get("content_analysis", {}).get("main_topics", [])
            print(f"ğŸ“Š ì¶”ì¶œëœ ì£¼ìš” í† í”½: {len(main_topics)}ê°œ")
            for i, topic in enumerate(main_topics, 1):
                print(f"  {i}. {topic}")

            return main_topics

        except Exception as e:
            print(f"âŒ JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return []

    def search_by_keyword(
        self, keyword: str, collection_name: str, limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œë¡œ VectorDBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            collection_name: ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ëª…
            limit: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # í‚¤ì›Œë“œë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            query_vector = self.embedding_model.encode(keyword).tolist()

            # Weaviateì—ì„œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            collection = self.client.collections.get(collection_name)

            response = collection.query.near_vector(
                near_vector=query_vector, limit=limit, return_metadata=["score"]
            )

            results = []
            for obj in response.objects:
                result = {
                    "uuid": str(obj.uuid),
                    "score": obj.metadata.score if obj.metadata else 0.0,
                    "properties": obj.properties,
                    "keyword": keyword,
                }
                results.append(result)

            return results

        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_multiple_topics(
        self, topics: List[str], collection_name: str, limit_per_topic: int = 3
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ì—¬ëŸ¬ í† í”½ì— ëŒ€í•´ VectorDB ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            topics: ê²€ìƒ‰í•  í† í”½ ë¦¬ìŠ¤íŠ¸
            collection_name: ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ëª…
            limit_per_topic: í† í”½ë‹¹ ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

        Returns:
            í† í”½ë³„ ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        all_results = {}

        print(f"\nğŸ” VectorDB ê²€ìƒ‰ ì‹œì‘ (ì»¬ë ‰ì…˜: {collection_name})")
        print(f"ğŸ“ ê²€ìƒ‰í•  í† í”½: {len(topics)}ê°œ")

        for i, topic in enumerate(topics, 1):
            print(f"\n[{i}/{len(topics)}] ê²€ìƒ‰ ì¤‘: '{topic}'")

            results = self.search_by_keyword(topic, collection_name, limit_per_topic)
            all_results[topic] = results

            if results:
                print(f"  âœ… ì°¾ì€ ê²°ê³¼: {len(results)}ê°œ")
                for j, result in enumerate(results, 1):
                    score = result.get("score", 0.0)
                    chunk_id = result.get("properties", {}).get("chunk_id", "N/A")
                    print(f"    {j}. ì ìˆ˜: {score:.3f}, ID: {chunk_id}")
            else:
                print(f"  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        return all_results

    def get_available_collections(self) -> List[str]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ ëª©ë¡ ë°˜í™˜

        Returns:
            ì»¬ë ‰ì…˜ëª… ë¦¬ìŠ¤íŠ¸
        """
        try:
            collections = list(self.client.collections.list_all().keys())
            print(f"ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {collections}")
            return collections
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []


def search_topics_from_json(
    json_file_path: str, collection_name: str = None, limit_per_topic: int = 3
) -> Dict[str, Any]:
    """
    JSON íŒŒì¼ì˜ main_topicsë¥¼ ê¸°ë°˜ìœ¼ë¡œ VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰

    Args:
        json_file_path: JSON íŒŒì¼ ê²½ë¡œ
        collection_name: ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ëª… (Noneì´ë©´ ìë™ ì¶”ì •)
        limit_per_topic: í† í”½ë‹¹ ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    searcher = TopicSearcher()

    # 1. JSONì—ì„œ main_topics ì¶”ì¶œ
    topics = searcher.extract_main_topics_from_json(json_file_path)

    if not topics:
        return {"error": "main_topicsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "json_file": json_file_path}

    # 2. ì»¬ë ‰ì…˜ëª… ìë™ ì¶”ì • (íŒŒì¼ëª… ê¸°ë°˜)
    if collection_name is None:
        from utils.change_name import normalize_collection_name

        filename = os.path.splitext(os.path.basename(json_file_path))[0]
        # "_complete_test" ê°™ì€ ì ‘ë¯¸ì‚¬ ì œê±°
        filename = filename.replace("_complete_test", "").replace(
            "_keywords_summary", ""
        )
        collection_name = normalize_collection_name(filename)
        print(f"ğŸ¯ ìë™ ì¶”ì •ëœ ì»¬ë ‰ì…˜ëª…: {collection_name}")

    # 3. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ í™•ì¸
    available_collections = searcher.get_available_collections()
    if collection_name not in available_collections:
        return {
            "error": f'ì»¬ë ‰ì…˜ "{collection_name}"ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
            "available_collections": available_collections,
            "json_file": json_file_path,
        }

    # 4. í† í”½ë³„ ê²€ìƒ‰ ìˆ˜í–‰
    search_results = searcher.search_multiple_topics(
        topics, collection_name, limit_per_topic
    )

    # 5. ê²°ê³¼ ìš”ì•½
    total_results = sum(len(results) for results in search_results.values())
    successful_topics = [topic for topic, results in search_results.items() if results]

    result_summary = {
        "json_file": json_file_path,
        "collection_name": collection_name,
        "topics_searched": len(topics),
        "topics_with_results": len(successful_topics),
        "total_results_found": total_results,
        "search_results": search_results,
        "summary": {
            "successful_topics": successful_topics,
            "empty_topics": [
                topic for topic, results in search_results.items() if not results
            ],
        },
    }

    print(f"\nğŸ“Š ê²€ìƒ‰ ì™„ë£Œ!")
    print(f"  - ê²€ìƒ‰ëœ í† í”½: {len(topics)}ê°œ")
    print(f"  - ê²°ê³¼ê°€ ìˆëŠ” í† í”½: {len(successful_topics)}ê°œ")
    print(f"  - ì´ ê²€ìƒ‰ ê²°ê³¼: {total_results}ê°œ")

    return result_summary


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
    test_json_path = "data/outputs/ìë™ì°¨ ë¦¬í¬íŠ¸_complete_test.json"

    if os.path.exists(test_json_path):
        print("ğŸš€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        results = search_topics_from_json(test_json_path)

        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        output_path = "data/search_outputs/topic_search_results.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {output_path}")
    else:
        print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_json_path}")
