import asyncio
import logging
from typing import List

from konlpy.tag import Okt
from langgraph.graph import StateGraph
from langsmith import traceable
from openai import AsyncOpenAI

from api.trainee_assistant.schemas.trainee_assistant import Question
from config.settings import settings
from db.redisDB.session_manager import append_message, load_message_history
from db.vectorDB.chromaDB.search import search_similar
from src.agents.trainee_assistant.prompt_1 import (
    build_prompt_from_docs,
    system_prompt_no_context,
)
from src.pipelines.trainee_assistant.state import ChatState

logger = logging.getLogger(__name__)

openai_client = AsyncOpenAI(api_key=settings.api_key)


okt = Okt()


@traceable(
    run_type="tool",
    name="Extract Keywords",
    metadata={"tool_type": "keyword_extraction"}
)
def extract_keywords(text: str, top_k: int = 5) -> List[str]:
    words = [
        word
        for word, pos in okt.pos(text)
        if pos in ["Noun", "Alpha", "Verb"] and len(word) > 1
    ]
    from collections import Counter

    return [word for word, _ in Counter(words).most_common(top_k)]


@traceable(
    run_type="chain",
    name="Vector Search Node",
    metadata={"pipeline": "trainee_assistant", "node_type": "vector_search"}
)
def vector_search_node(state: ChatState) -> ChatState:
    question_data = next(
        (q for q in state["test_questions"] if q.id == state["question_id"]), None
    )
    if not question_data:
        logger.warning("âŒ ì§ˆë¬¸ IDì— í•´ë‹¹í•˜ëŠ” í…ŒìŠ¤íŠ¸ ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"chroma_docs": [], "document_name": None, "question_data": None}

    document_name = question_data.documentName
    keywords = extract_keywords(state["question"])

    logger.info(f"ğŸ” ChromaDBì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰: document_name={document_name}")
    docs = search_similar(
        query=state["question"], collection_name=document_name, n_results=5
    )

    MIN_SIMILARITY = 0.75
    filtered_docs = [
        doc
        for doc in docs
        if doc["similarity"] >= MIN_SIMILARITY
        and any(k in doc["content"] for k in keywords)
    ]

    if filtered_docs:
        logger.info(f"ğŸ“„ ê´€ë ¨ ë¬¸ì„œ {len(filtered_docs)}ê°œ ë°œê²¬")
    else:
        logger.warning("âš ï¸ ë¬¸ì„œëŠ” ìˆìœ¼ë‚˜ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•¨")

    return {
        "chroma_docs": filtered_docs,
        "document_name": document_name,
        "question_data": question_data,
    }


@traceable(
    run_type="chain",
    name="Generate Answer Node",
    metadata={"pipeline": "trainee_assistant", "node_type": "answer_generation", "model": "gpt-4o"}
)
async def generate_answer_node(state: ChatState) -> ChatState:
    user_question = state["question"]
    question_data = state["question_data"]

    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ íŒŒì•… (ë‹¨ìˆœ ì •ë³´ ì¡°íšŒ)
    if question_data:
        keyword_map = {
            ("ë¬¸ì œ",): (lambda q: f"ë¬¸ì œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{q.question}"),
            ("ìœ í˜•", "íƒ€ì…"): (lambda q: f"ì´ ë¬¸ì œì˜ ìœ í˜•ì€ '{q.type.value}'ì…ë‹ˆë‹¤."),
            ("ë‚œì´ë„",): (lambda q: f"ì´ ë¬¸ì œì˜ ë‚œì´ë„ëŠ” '{q.difficultyLevel}'ì…ë‹ˆë‹¤." if q.difficultyLevel else "ë‚œì´ë„ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."),
            ("ë³´ê¸°", "ì„ íƒì§€"): (lambda q: f"ì„ íƒì§€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{', '.join(q.options)}" if q.options else "ê°ê´€ì‹ ë¬¸ì œê°€ ì•„ë‹ˆê±°ë‚˜ ì„ íƒì§€ê°€ ì—†ìŠµë‹ˆë‹¤."),
            ("ì •ë‹µ", "ë‹µ"): (lambda q: f"ì •ë‹µì€ '{q.answer}'ì…ë‹ˆë‹¤."),
            ("í•´ì„¤",): (lambda q: q.explanation or "í•´ì„¤ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."),
            ("ì±„ì  ê¸°ì¤€",): (lambda q: f"ì±„ì  ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{json.dumps([item.dict() for item in q.gradingCriteria], ensure_ascii=False, indent=2)}" if q.gradingCriteria else "ì£¼ê´€ì‹ ë¬¸ì œê°€ ì•„ë‹ˆê±°ë‚˜ ì±„ì  ê¸°ì¤€ì´ ì—†ìŠµë‹ˆë‹¤."),
            ("ë¬¸ì„œ ID", "ë¬¸ì„œ ì•„ì´ë””"): (lambda q: f"ê´€ë ¨ ë¬¸ì„œ IDëŠ” '{q.documentId}'ì…ë‹ˆë‹¤."),
            ("ë¬¸ì„œ ì´ë¦„", "ë¬¸ì„œëª…"): (lambda q: f"ê´€ë ¨ ë¬¸ì„œ ì´ë¦„ì€ '{q.documentName}'ì…ë‹ˆë‹¤."),
            ("í‚¤ì›Œë“œ",): (lambda q: f"ê´€ë ¨ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{', '.join(q.keywords)}" if q.keywords else "ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤."),
            ("íƒœê·¸",): (lambda q: f"ê´€ë ¨ íƒœê·¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{', '.join(q.tags)}" if q.tags else "ê´€ë ¨ íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."),
            ("ìƒì„± íƒ€ì…", "ìƒì„± ìœ í˜•"): (lambda q: f"ë¬¸ì œ ìƒì„± ìœ í˜•ì€ '{q.generationType.value}'ì…ë‹ˆë‹¤."),
        }

        for keywords, response_func in keyword_map.items():
            if any(keyword in user_question for keyword in keywords):
                answer = response_func(question_data)
                await append_message(state["user_id"], "user", user_question)
                await append_message(state["user_id"], "assistant", answer)
                logger.info("ğŸ“ (Direct) ëŒ€í™” ë‚´ìš© Redis ì €ì¥ ì™„ë£Œ")
                return {"answer": answer}

    # If no direct answer was generated, proceed with LLM call
    history = await load_message_history(state["user_id"])
    history.append({"role": "user", "content": user_question})

    if state.get("chroma_docs"):
        prompt = build_prompt_from_docs(
            user_question, state["chroma_docs"], question_data
        )
        prompt_role = {"role": "user", "content": prompt}
        answer_prefix = ""
    else:
        warning = "âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
        prompt_role = {
            "role": "user",
            "content": f"{warning}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_question}",
        }
        answer_prefix = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ LLMì´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n\n"

    logger.info("ğŸ¤– GPT í˜¸ì¶œ ì‹œì‘")
    response = await openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": system_prompt_no_context.strip(),
            },
            *history,
            prompt_role,
        ],
    )
    answer = response.choices[0].message.content
    logger.info("ğŸ’¬ GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

    answer = answer_prefix + answer

    if state.get("chroma_docs"):
        source_info = f"\n\nğŸ“ (ì¶œì²˜: ë¬¸ì„œ '{state['document_name']}')"
        answer += source_info

    await append_message(state["user_id"], "user", user_question)
    await append_message(state["user_id"], "assistant", answer)
    logger.info("ğŸ“ ëŒ€í™” ë‚´ìš© Redis ì €ì¥ ì™„ë£Œ")

    return {"answer": answer}


@traceable(
    run_type="chain",
    name="Build Trainee Assistant Pipeline",
    metadata={"pipeline": "trainee_assistant", "graph_type": "langgraph"}
)
def build_langgraph():
    builder = StateGraph(ChatState)

    builder.add_node("vector_search_node", vector_search_node)
    builder.add_node("generate_answer_node", generate_answer_node)

    builder.set_entry_point("vector_search_node")
    builder.add_edge("vector_search_node", "generate_answer_node")

    return builder.compile()
