import json
import logging
from typing import List

from konlpy.tag import Okt
from langgraph.graph import StateGraph
from langsmith import traceable
from langsmith.wrappers import wrap_openai
from openai import AsyncOpenAI

from config.settings import settings
from db.redisDB.session_manager import append_message, load_message_history
from db.vectorDB.chromaDB.search import search_similar
from src.agents.trainee_assistant.prompt_1 import (
    build_prompt_from_docs,
    system_prompt_no_context,
)
from src.pipelines.trainee_assistant.state import ChatState

logger = logging.getLogger(__name__)

okt = Okt()


def get_openai_client():
    api_key = settings.api_key
    if not api_key:
        raise ValueError("OPENAI_API_KEY is not set.")
    return wrap_openai(AsyncOpenAI(api_key=api_key))


openai_client = get_openai_client()


@traceable(
    run_type="tool",
    name="Extract Keywords",
    metadata={"tool_type": "keyword_extraction"},
    metadata={"tool_type": "keyword_extraction"},
)
def extract_keywords(text: str, top_k: int = 5) -> List[str]:
    words = [
        word
        for word, pos in okt.pos(text)
        if pos in ["Noun", "Alpha", "Verb"] and len(word) > 1
    ]
    from collections import Counter

    return [word for word, _ in Counter(words).most_common(top_k)]


# --- Graph Nodes ---


async def route_question(state: ChatState) -> str:
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„° ë…¸ë“œ"""
    user_question = state["question"]
    question_data = next(
        (q for q in state["test_questions"] if q.id == state["question_id"]), None
    )

    if not question_data:
        logger.warning("âŒ ì§ˆë¬¸ IDì— í•´ë‹¹í•˜ëŠ” í…ŒìŠ¤íŠ¸ ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return "end"  # or some error handling state

    # Update state with the found question_data
    state["question_data"] = question_data

    prompt = f"""ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” ë¼ìš°íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ë¬¸ì œ ì •ë³´]ì™€ [ì‚¬ìš©ì ì§ˆë¬¸]ì„ ë³´ê³ , ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë‹¤ìŒ ë‘ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

[ë¬¸ì œ ì •ë³´]
- ë¬¸ì œ: {question_data.question}
- ìœ í˜•: {question_data.type.value}
- ì •ë‹µ: {question_data.answer}
- í•´ì„¤: {question_data.explanation}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_question}

[ë¶„ë¥˜]
1. `direct_answer`: ì‚¬ìš©ìê°€ ë¬¸ì œì˜ ì •ë‹µ, ë³´ê¸°, í•´ì„¤, ìœ í˜• ë“± ì œê³µëœ [ë¬¸ì œ ì •ë³´]ì— ëŒ€í•´ ì§ì ‘ì ìœ¼ë¡œ ë¬»ê³  ìˆìŠµë‹ˆë‹¤.
   (ì˜ˆ: "ì •ë‹µì´ ë­ì•¼?", "í•´ì„¤ ë³´ì—¬ì¤˜", "ì´ ë¬¸ì œ ë¬´ìŠ¨ ìœ í˜•ì´ì•¼?")
2. `document_search`: ì‚¬ìš©ìê°€ ë¬¸ì œì˜ ë°°ê²½, ê°œë…, ì´ìœ  ë“± [ë¬¸ì œ ì •ë³´]ì— ì§ì ‘ì ìœ¼ë¡œ ëª…ì‹œë˜ì§€ ì•Šì€, ë” ê¹Šì€ ë‚´ìš©ì„ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ë´ì•¼ í•©ë‹ˆë‹¤.
   (ì˜ˆ: "ì™œ ì´ê²Œ ì •ë‹µì´ì•¼?", "AGS Trouble shooting ê°€ì´ë“œê°€ ë­ì•¼?", "íƒ„ì†Œë°°ì¶œê¶Œì´ ë­ì•¼?")

ì˜¤ì§ `direct_answer` ë˜ëŠ” `document_search` ë‘˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

    response = await openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    route = response.choices[0].message.content.strip()
    logger.info(f"ğŸš¦ ë¼ìš°íŒ… ê²°ì •: {route}")
    return route


async def generate_direct_answer_node(state: ChatState) -> ChatState:
    """ë¬¸ì œ ë°ì´í„°(question_data)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    user_question = state["question"]
    question_data = state["question_data"]

    prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ë¬¸ì œ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì‚¬ìš©ì ì§ˆë¬¸]ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ë¬¸ì œ ì •ë³´]
{json.dumps(question_data.dict(), ensure_ascii=False, indent=2)}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_question}

ì ˆëŒ€ë¡œ [ë¬¸ì œ ì •ë³´]ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”."""

    response = await openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
    )
    answer = response.choices[0].message.content
    logger.info("ğŸ’¬ (Direct) GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

    await append_message(state["user_id"], "user", user_question)
    await append_message(state["user_id"], "assistant", answer)
    logger.info("ğŸ“ (Direct) ëŒ€í™” ë‚´ìš© Redis ì €ì¥ ì™„ë£Œ")

    return {"answer": answer}


def vector_search_node(state: ChatState) -> ChatState:
    """ê´€ë ¨ ë¬¸ì„œë¥¼ ë²¡í„°DBì—ì„œ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ"""
    document_name = state["question_data"].documentName
    logger.info(f"ğŸ” ChromaDBì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰: document_name={document_name}")
    docs = search_similar(
        query=state["question"], collection_name=document_name, n_results=5
    )

    MIN_SIMILARITY = 0.75
    filtered_docs = [doc for doc in docs if doc["similarity"] >= MIN_SIMILARITY]

    if filtered_docs:
        logger.info(f"ğŸ“„ ê´€ë ¨ ë¬¸ì„œ {len(filtered_docs)}ê°œ ë°œê²¬")
    else:
        logger.warning("âš ï¸ ë¬¸ì„œëŠ” ìˆìœ¼ë‚˜ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•¨")

    return {"chroma_docs": filtered_docs, "document_name": document_name}


async def generate_document_based_answer_node(state: ChatState) -> ChatState:
    """ë²¡í„°DB ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    user_question = state["question"]
    history = await load_message_history(state["user_id"])
    history.append({"role": "user", "content": user_question})

    if state.get("chroma_docs"):
        prompt = build_prompt_from_docs(
            user_question, state["chroma_docs"], state["question_data"]
        )
        prompt_role = {"role": "user", "content": prompt}
        answer_prefix = ""
    else:
        warning = "âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
        prompt_role = {
            "role": "user",
            "content": f"{warning}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_question}",
        }
        answer_prefix = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ LLMì´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n\n"

    logger.info("ğŸ¤– (Doc-Based) GPT í˜¸ì¶œ ì‹œì‘")
    response = await openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt_no_context.strip()},
            *history,
            prompt_role,
        ],
    )
    answer = response.choices[0].message.content
    logger.info("ğŸ’¬ (Doc-Based) GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

    answer = answer_prefix + answer
    if state.get("chroma_docs"):
        answer += f"\n\nğŸ“ (ì¶œì²˜: ë¬¸ì„œ '{state['document_name']}')"

    await append_message(state["user_id"], "user", user_question)
    await append_message(state["user_id"], "assistant", answer)
    logger.info("ğŸ“ (Doc-Based) ëŒ€í™” ë‚´ìš© Redis ì €ì¥ ì™„ë£Œ")

    return {"answer": answer}


# --- Graph Builder ---
@traceable(
    run_type="chain",
    name="Build Trainee Assistant Pipeline",
    metadata={"pipeline": "trainee_assistant", "graph_type": "langgraph"},
)
def build_langgraph():
    builder = StateGraph(ChatState)

    builder.add_node("route_question", route_question)
    builder.add_node("generate_direct_answer_node", generate_direct_answer_node)
    builder.add_node("vector_search_node", vector_search_node)
    builder.add_node(
        "generate_document_based_answer_node", generate_document_based_answer_node
    )

    builder.set_entry_point("route_question")

    builder.add_conditional_edges(
        "route_question",
        lambda x: x,
        {
            "direct_answer": "generate_direct_answer_node",
            "document_search": "vector_search_node",
        },
    )

    builder.add_edge("vector_search_node", "generate_document_based_answer_node")
    builder.add_edge("generate_direct_answer_node", END)
    builder.add_edge("generate_document_based_answer_node", END)

    return builder.compile()
