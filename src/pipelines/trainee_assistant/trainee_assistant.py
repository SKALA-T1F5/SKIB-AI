import asyncio
import logging
from typing import List

from konlpy.tag import Okt
from langgraph.graph import StateGraph
from langsmith import traceable
from openai import AsyncOpenAI

from api.trainee_assistant.schemas.trainee_assistant import Question
from config.settings import settings
from db.redisDB.session_manager import append_message, load_message_history
from db.vectorDB.chromaDB.search import search_similar
from src.agents.trainee_assistant.agent import answer_based_on_question_data
from src.agents.trainee_assistant.prompt_1 import (
    build_prompt_from_docs,
    system_prompt_no_context,
)
from src.pipelines.trainee_assistant.state import ChatState

logger = logging.getLogger(__name__)

openai_client = AsyncOpenAI(api_key=settings.api_key)
okt = Okt()


@traceable(
    run_type="tool",
    name="Extract Keywords",
    metadata={"tool_type": "keyword_extraction"},
)
def extract_keywords(text: str, top_k: int = 5) -> List[str]:
    words = [
        word
        for word, pos in okt.pos(text)
        if pos in ["Noun", "Alpha", "Verb"] and len(word) > 1
    ]
    from collections import Counter

    return [word for word, _ in Counter(words).most_common(top_k)]


@traceable(
    run_type="chain",
    name="Vector Search Node",
    metadata={"pipeline": "trainee_assistant", "node_type": "vector_search"},
)
def vector_search_node(state: ChatState) -> ChatState:
    question_data = next(
        (q for q in state["test_questions"] if q.id == state["question_id"]), None
    )

    if not question_data:
        logger.warning("âŒ ì§ˆë¬¸ IDì— í•´ë‹¹í•˜ëŠ” í…ŒìŠ¤íŠ¸ ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"chroma_docs": [], "document_name": None}

    document_name = question_data.documentName
    keywords = extract_keywords(state["question"])

    logger.info(f"ğŸ” ChromaDBì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰: document_name={document_name}")
    docs = search_similar(
        query=state["question"], collection_name=document_name, n_results=5
    )

    MIN_SIMILARITY = 0.75
    filtered_docs = [
        doc
        for doc in docs
        if doc["similarity"] >= MIN_SIMILARITY
        and any(k in doc["content"] for k in keywords)
    ]

    if filtered_docs:
        logger.info(f"ğŸ“„ ê´€ë ¨ ë¬¸ì„œ {len(filtered_docs)}ê°œ ë°œê²¬")
    else:
        logger.warning("âš ï¸ ë¬¸ì„œëŠ” ìˆìœ¼ë‚˜ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•¨")

    return {"chroma_docs": filtered_docs, "document_name": document_name}


def generate_direct_answer_or_explanation(intent: str, question_data: Question) -> str:
    if intent == "answer":
        return f"ğŸ“Œ í•´ë‹¹ ë¬¸ì œì˜ ì •ë‹µì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{question_data.answer}"
    elif intent == "explanation" and question_data.explanation:
        return f"ğŸ’¡ í•´ë‹¹ ë¬¸ì œì˜ í•´ì„¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{question_data.explanation}"
    return None


@traceable(
    run_type="chain",
    name="Generate Answer Node",
    metadata={
        "pipeline": "trainee_assistant",
        "node_type": "answer_generation",
        "model": "gpt-4o",
    },
)
async def generate_answer_node(state: ChatState) -> ChatState:
    history = await load_message_history(state["user_id"])
    history.append({"role": "user", "content": state["question"]})

    question_data = next(
        (q for q in state["test_questions"] if q.id == state["question_id"]), None
    )

    if question_data:
        answer = await answer_based_on_question_data(state["question"], question_data)
        await append_message(state["user_id"], "user", state["question"])
        await append_message(state["user_id"], "assistant", answer)
        return {"answer": answer}

    if state.get("chroma_docs"):
        prompt = build_prompt_from_docs(state["question"], state["chroma_docs"])
        prompt_role = {"role": "user", "content": prompt}
    else:
        warning = "âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
        prompt_role = {
            "role": "user",
            "content": f"{warning}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{state['question']}",
        }

    logger.info("ğŸ¤– GPT í˜¸ì¶œ ì‹œì‘")
    response = await openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": system_prompt_no_context.strip(),
            },
            *history,
            prompt_role,
        ],
    )
    answer = response.choices[0].message.content
    logger.info("ğŸ’¬ GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

    if state.get("chroma_docs"):
        source_info = f"\n\nğŸ“ (ì¶œì²˜: ë¬¸ì„œ '{state['document_name']}')"
        answer += source_info

    await append_message(state["user_id"], "user", state["question"])
    await append_message(state["user_id"], "assistant", answer)
    logger.info("ğŸ“ ëŒ€í™” ë‚´ìš© Redis ì €ì¥ ì™„ë£Œ")

    return {"answer": answer}


@traceable(
    run_type="chain",
    name="Build Trainee Assistant Pipeline",
    metadata={"pipeline": "trainee_assistant", "graph_type": "langgraph"},
)
def build_langgraph():
    builder = StateGraph(ChatState)

    builder.add_node("vector_search_node", vector_search_node)
    builder.add_node("generate_answer_node", generate_answer_node)

    builder.set_entry_point("vector_search_node")
    builder.add_edge("vector_search_node", "generate_answer_node")

    return builder.compile()
