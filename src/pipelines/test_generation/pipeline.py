"""
Test Generation Pipeline - LangGraph ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
- BasePipeline ìƒì†
- Celery Task ê¸°ë°˜ ë¶„ì‚° ì²˜ë¦¬
- Document SubGraphë¥¼ í†µí•œ ë°°ì¹˜ë³„ ë¬¸ì œ ìƒì„±
- í’ˆì§ˆ ê¸°ë°˜ ì¡°ê±´ë¶€ ë¶„ê¸°
"""

import logging
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

from langgraph.graph import END, StateGraph
from langsmith import traceable

from src.pipelines.base.pipeline import BasePipeline
from src.pipelines.test_generation.state import TestGenerationState

logger = logging.getLogger(__name__)


class TestGenerationPipeline(BasePipeline[TestGenerationState]):
    """í…ŒìŠ¤íŠ¸ ìƒì„± ì „ìš© LangGraph Pipeline"""

    def _get_state_schema(self) -> type:
        """State ìŠ¤í‚¤ë§ˆ ë°˜í™˜"""
        return TestGenerationState

    def _get_node_list(self) -> List[str]:
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ë…¸ë“œ ëª©ë¡"""
        return ["load_test_plans", "create_smart_batches", "process_document_batches", "collect_results"]

    def _get_default_state(self) -> Dict[str, Any]:
        """ê¸°ë³¸ State ì„¤ì •"""
        return {
            "pipeline_id": str(uuid.uuid4()),
            "session_id": str(uuid.uuid4()),
            "current_step": "load_test_plans",
            "processing_status": "pending",
            "progress_percentage": 0.0,
            "started_at": datetime.now().isoformat(),
            "retry_count": 0,
            "total_batches": 0,
            "completed_batches": 0,
            "batch_quality_scores": {},
            "regeneration_attempts": {},
            "current_batch_processing": [],
            "batch_processing_strategy": "parallel",
        }

    def _build_workflow(self) -> StateGraph:
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(TestGenerationState)

        # ë©”ì¸ ë…¸ë“œë“¤ ì¶”ê°€
        workflow.add_node("load_test_plans", self._load_test_plans_node)
        workflow.add_node("create_smart_batches", self._create_smart_batches_node)
        workflow.add_node(
            "process_document_batches", self._process_document_batches_node
        )
        workflow.add_node("collect_results", self._collect_results_node)
        workflow.add_node("error_handler", self._error_handler_node)

        # Document SubGraph ì¶”ê°€
        document_subgraph = self._build_document_subgraph()
        workflow.add_node("document_subgraph", document_subgraph)

        # ì›Œí¬í”Œë¡œìš° ì—°ê²°
        workflow.set_entry_point("load_test_plans")

        workflow.add_edge("load_test_plans", "create_smart_batches")
        workflow.add_edge("create_smart_batches", "process_document_batches")
        workflow.add_edge("process_document_batches", "collect_results")
        workflow.add_edge("collect_results", END)

        # ì—ëŸ¬ ì²˜ë¦¬
        workflow.add_edge("error_handler", END)

        return workflow

    def _build_document_subgraph(self) -> StateGraph:
        """Document ì²˜ë¦¬ SubGraph êµ¬ì„±"""
        subgraph = StateGraph(TestGenerationState)

        # SubGraph ë…¸ë“œë“¤
        subgraph.add_node("vector_search", self._vector_search_node)
        subgraph.add_node("generate_questions", self._generate_questions_node)
        subgraph.add_node("review_questions", self._review_questions_node)
        subgraph.add_node("regenerate_questions", self._regenerate_questions_node)
        subgraph.add_node("approve_batch", self._approve_batch_node)
        subgraph.add_node("retry_strategy", self._retry_strategy_node)

        # SubGraph ì›Œí¬í”Œë¡œìš°
        subgraph.set_entry_point("vector_search")

        subgraph.add_edge("vector_search", "generate_questions")
        subgraph.add_edge("generate_questions", "review_questions")

        # ì¡°ê±´ë¶€ ë¶„ê¸° - í’ˆì§ˆ ê¸°ë°˜
        subgraph.add_conditional_edges(
            "review_questions",
            self._route_after_review,
            {
                "approve": "approve_batch",
                "regenerate": "regenerate_questions",
                "retry_strategy": "retry_strategy",
            },
        )

        subgraph.add_edge("regenerate_questions", "review_questions")
        subgraph.add_edge("retry_strategy", "vector_search")
        subgraph.add_edge("approve_batch", END)

        return subgraph.compile()

    # ============ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ë…¸ë“œë“¤ ============

    @traceable(name="load_test_plans")
    async def _load_test_plans_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """Test Plan ë¡œë“œ ë° Redis ì €ì¥"""
        self.logger.info("ğŸ“‹ Test Plan ë¡œë“œ ì‹œì‘")

        try:
            # ê¸°ì¡´ TestPlanHandler í™œìš©
            from src.agents.question_generator.tools.test_plan_handler import (
                TestPlanHandler,
            )

            handler = TestPlanHandler()

            # Stateì—ì„œ test_config í™•ì¸
            test_config = state.get("test_config", {})

            if (
                "total_test_plan_data" in test_config
                and "document_test_plan_data" in test_config
            ):
                # ì§ì ‘ ì „ë‹¬ëœ ë°ì´í„° ì‚¬ìš©
                total_plan = test_config["total_test_plan_data"]
                document_plan = test_config["document_test_plan_data"]
                self.logger.info("ğŸ“‹ ì§ì ‘ ì „ë‹¬ëœ Test Plan ë°ì´í„° ì‚¬ìš©")
            else:
                # ìë™ìœ¼ë¡œ ìµœì‹  íŒŒì¼ ë¡œë“œ
                total_plan, document_plan = handler.load_latest_test_plans()
                if not total_plan or not document_plan:
                    raise Exception("Test plan íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.logger.info("ğŸ“‹ íŒŒì¼ì—ì„œ Test Plan ë¡œë“œ ì™„ë£Œ")

            return {
                **state,
                "total_test_plan": total_plan,
                "document_test_plan": document_plan,
                "current_step": "create_smart_batches",
                "progress_percentage": 20.0,
            }

        except Exception as e:
            self.logger.error(f"âŒ Test Plan ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                **state,
                "processing_status": "failed",
                "error_message": str(e),
                "current_step": "error_handler",
            }

    @traceable(name="create_smart_batches")
    async def _create_smart_batches_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ë¬¸ì„œë³„ ë°°ì¹˜ ìƒì„± ë° ì²˜ë¦¬ ì „ëµ ê²°ì •"""
        self.logger.info("ğŸ¯ ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ìƒì„± ì‹œì‘")

        try:
            document_plan = state["document_test_plan"]
            document_plans = document_plan.get("document_plans", [])

            processing_batches = []

            for i, doc_plan in enumerate(document_plans):
                batch = {
                    "batch_id": i + 1,
                    "document_id": doc_plan.get("document_id"),
                    "document_name": doc_plan.get("document_name", f"document_{i+1}"),
                    "keywords": doc_plan.get("keywords", []),
                    "target_questions": {
                        "objective": doc_plan.get("recommended_questions", {}).get(
                            "objective", 3
                        ),
                        "subjective": doc_plan.get("recommended_questions", {}).get(
                            "subjective", 2
                        ),
                    },
                    "difficulty": doc_plan.get("difficulty_level", "medium"),
                    "priority": doc_plan.get("priority", "medium"),
                }
                processing_batches.append(batch)

            # ì²˜ë¦¬ ì „ëµ ê²°ì • (ë°°ì¹˜ ìˆ˜ì— ë”°ë¼)
            if len(processing_batches) <= 2:
                strategy = "parallel"
            elif len(processing_batches) <= 5:
                strategy = "parallel"  # ìš°ì„  ë³‘ë ¬ë¡œ ì‹œë„
            else:
                strategy = "hybrid"  # í° ë°°ì¹˜ëŠ” í˜¼í•© ì „ëµ

            self.logger.info(
                f"âœ… {len(processing_batches)}ê°œ ë°°ì¹˜ ìƒì„±, ì „ëµ: {strategy}"
            )

            return {
                **state,
                "processing_batches": processing_batches,
                "total_batches": len(processing_batches),
                "batch_processing_strategy": strategy,
                "current_step": "process_document_batches",
                "progress_percentage": 40.0,
            }

        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                **state,
                "processing_status": "failed",
                "error_message": str(e),
                "current_step": "error_handler",
            }

    @traceable(name="process_document_batches")
    async def _process_document_batches_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """Document SubGraphë¥¼ í†µí•œ ë°°ì¹˜ ì²˜ë¦¬ ê´€ë¦¬"""
        self.logger.info("ğŸ”„ Document ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")

        try:
            processing_batches = state["processing_batches"]
            strategy = state["batch_processing_strategy"]

            if strategy == "parallel":
                # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
                current_processing = [batch["batch_id"] for batch in processing_batches]
            else:
                # ìˆœì°¨ ë˜ëŠ” ì œí•œëœ ë³‘ë ¬ ì²˜ë¦¬
                current_processing = (
                    [processing_batches[0]["batch_id"]] if processing_batches else []
                )

            return {
                **state,
                "current_batch_processing": current_processing,
                "current_step": "document_subgraph",
                "progress_percentage": 60.0,
            }

        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ê´€ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                **state,
                "processing_status": "failed",
                "error_message": str(e),
                "current_step": "error_handler",
            }

    @traceable(name="collect_results")
    async def _collect_results_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ ìˆ˜ì§‘ ë° ìµœì¢… í…ŒìŠ¤íŠ¸ ìƒì„±"""
        self.logger.info("ğŸ“Š ê²°ê³¼ ìˆ˜ì§‘ ì‹œì‘")

        try:
            pipeline_id = state["pipeline_id"]
            total_batches = state["total_batches"]

            # Redisì—ì„œ ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ ìˆ˜ì§‘
            from db.redisDB.testgen_session_manager import (
                load_batch_questions,
                save_final_test,
            )

            all_questions = []
            successful_batches = 0
            total_quality_scores = []

            for batch_id in range(1, total_batches + 1):
                questions = await load_batch_questions(pipeline_id, batch_id)
                if questions:
                    all_questions.extend(questions)
                    successful_batches += 1

                    # í’ˆì§ˆ ì ìˆ˜ ìˆ˜ì§‘
                    batch_quality = state.get("batch_quality_scores", {}).get(
                        batch_id, 0.0
                    )
                    if batch_quality > 0:
                        total_quality_scores.append(batch_quality)

            # ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±
            final_test_data = {
                "total_questions": len(all_questions),
                "questions": all_questions,
                "metadata": {
                    "pipeline_id": pipeline_id,
                    "successful_batches": successful_batches,
                    "total_batches": total_batches,
                    "average_quality_score": (
                        sum(total_quality_scores) / len(total_quality_scores)
                        if total_quality_scores
                        else 0.0
                    ),
                    "questions_by_type": {
                        "objective": len(
                            [q for q in all_questions if q.get("type") == "OBJECTIVE"]
                        ),
                        "subjective": len(
                            [q for q in all_questions if q.get("type") == "SUBJECTIVE"]
                        ),
                    },
                    "completed_at": datetime.now().isoformat(),
                },
            }

            # Redisì— ìµœì¢… ê²°ê³¼ ì €ì¥
            await save_final_test(pipeline_id, final_test_data)

            self.logger.info(f"âœ… ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_questions)}ê°œ ë¬¸ì œ")

            return {
                **state,
                "processing_status": "completed",
                "completed_batches": successful_batches,
                "current_step": "completed",
                "progress_percentage": 100.0,
                "completed_at": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                **state,
                "processing_status": "failed",
                "error_message": str(e),
                "current_step": "error_handler",
            }

    # ============ Document SubGraph ë…¸ë“œë“¤ ============

    @traceable(name="vector_search_subgraph")
    async def _vector_search_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """VectorDB ê²€ìƒ‰ (Celery Task í˜¸ì¶œ)"""
        self.logger.info("ğŸ” Vector Search ë…¸ë“œ ì‹œì‘")

        try:
            from src.pipelines.test_generation.celery_tasks import vector_search_task

            current_batch_id = state["current_batch_processing"][0]  # ì²« ë²ˆì§¸ ë°°ì¹˜ ì²˜ë¦¬
            batch_info = None

            for batch in state["processing_batches"]:
                if batch["batch_id"] == current_batch_id:
                    batch_info = batch
                    break

            if not batch_info:
                raise Exception(f"ë°°ì¹˜ {current_batch_id} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # Celery Task ì‹¤í–‰
            task = vector_search_task.delay(
                pipeline_id=state["pipeline_id"],
                batch_id=current_batch_id,
                keywords=batch_info["keywords"],
                document_name=batch_info["document_name"],
            )

            # ê²°ê³¼ ëŒ€ê¸° (ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì§§ì€ ëŒ€ê¸°)
            result = task.get(timeout=300)  # 5ë¶„ ëŒ€ê¸°

            if result["status"] != "success":
                raise Exception(f"Vector search ì‹¤íŒ¨: {result.get('error')}")

            self.logger.info(
                f"âœ… Vector Search ì™„ë£Œ: {result['contexts_count']}ê°œ ì»¨í…ìŠ¤íŠ¸"
            )

            return {**state, "current_step": "generate_questions"}

        except Exception as e:
            self.logger.error(f"âŒ Vector Search ì‹¤íŒ¨: {e}")
            return {
                **state,
                "processing_status": "failed",
                "error_message": str(e),
                "current_step": "error_handler",
            }

    @traceable(name="generate_questions_subgraph")
    async def _generate_questions_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ë¬¸ì œ ìƒì„± (Celery Task í˜¸ì¶œ)"""
        self.logger.info("ğŸ¤– Question Generation ë…¸ë“œ ì‹œì‘")

        try:
            from src.pipelines.test_generation.celery_tasks import (
                question_generation_task,
            )

            current_batch_id = state["current_batch_processing"][0]
            batch_info = None

            for batch in state["processing_batches"]:
                if batch["batch_id"] == current_batch_id:
                    batch_info = batch
                    break

            # Celery Task ì‹¤í–‰
            task = question_generation_task.delay(
                pipeline_id=state["pipeline_id"],
                batch_id=current_batch_id,
                target_questions=batch_info["target_questions"],
                document_metadata={
                    "document_name": batch_info["document_name"],
                    "document_id": batch_info["document_id"],
                    "keywords": batch_info["keywords"],
                    "difficulty": batch_info["difficulty"],
                },
            )

            result = task.get(timeout=600)  # 10ë¶„ ëŒ€ê¸°

            if result["status"] != "success":
                raise Exception(f"Question generation ì‹¤íŒ¨: {result.get('error')}")

            # í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
            batch_quality_scores = state.get("batch_quality_scores", {})
            batch_quality_scores[current_batch_id] = result["quality_score"]

            self.logger.info(
                f"âœ… Question Generation ì™„ë£Œ: {result['questions_generated']}ê°œ ë¬¸ì œ"
            )

            return {
                **state,
                "batch_quality_scores": batch_quality_scores,
                "current_step": "review_questions",
            }

        except Exception as e:
            self.logger.error(f"âŒ Question Generation ì‹¤íŒ¨: {e}")
            return {
                **state,
                "processing_status": "failed",
                "error_message": str(e),
                "current_step": "error_handler",
            }

    async def _review_questions_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ë¬¸ì œ í’ˆì§ˆ ê²€í† """
        current_batch_id = state["current_batch_processing"][0]
        quality_score = state.get("batch_quality_scores", {}).get(current_batch_id, 0.0)

        self.logger.info(
            f"ğŸ“Š í’ˆì§ˆ ê²€í† : ë°°ì¹˜ {current_batch_id}, ì ìˆ˜ {quality_score:.3f}"
        )

        return {**state, "current_step": "route_decision"}

    async def _regenerate_questions_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ë¬¸ì œ ì¬ìƒì„±"""
        self.logger.info("ğŸ”„ ë¬¸ì œ ì¬ìƒì„± ì‹œì‘")

        current_batch_id = state["current_batch_processing"][0]
        regeneration_attempts = state.get("regeneration_attempts", {})
        regeneration_attempts[current_batch_id] = (
            regeneration_attempts.get(current_batch_id, 0) + 1
        )

        return {
            **state,
            "regeneration_attempts": regeneration_attempts,
            "current_step": "generate_questions",  # ë¬¸ì œ ìƒì„±ìœ¼ë¡œ ë‹¤ì‹œ ì´ë™
        }

    async def _approve_batch_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ë°°ì¹˜ ìŠ¹ì¸ ë° ì™„ë£Œ"""
        current_batch_id = state["current_batch_processing"][0]
        completed_batches = state.get("completed_batches", 0) + 1

        self.logger.info(f"âœ… ë°°ì¹˜ {current_batch_id} ìŠ¹ì¸ ì™„ë£Œ")

        return {
            **state,
            "completed_batches": completed_batches,
            "current_step": "completed",
        }

    async def _retry_strategy_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ì¬ì‹œë„ ì „ëµ ì ìš©"""
        self.logger.info("ğŸ”„ ì¬ì‹œë„ ì „ëµ ì ìš©")

        return {**state, "current_step": "vector_search"}  # Vector searchë¶€í„° ë‹¤ì‹œ ì‹œì‘

    async def _error_handler_node(
        self, state: TestGenerationState
    ) -> TestGenerationState:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        error_message = state.get("error_message", "Unknown error")
        self.logger.error(f"âŒ Pipeline ì—ëŸ¬: {error_message}")

        return {
            **state,
            "processing_status": "failed",
            "completed_at": datetime.now().isoformat(),
        }

    # ============ ì¡°ê±´ë¶€ ë¶„ê¸° ë¼ìš°í„°ë“¤ ============

    def _route_after_review(self, state: TestGenerationState) -> str:
        """í’ˆì§ˆ ê²€í†  í›„ ë¶„ê¸° ê²°ì •"""
        current_batch_id = state["current_batch_processing"][0]
        quality_score = state.get("batch_quality_scores", {}).get(current_batch_id, 0.0)
        regeneration_attempts = state.get("regeneration_attempts", {}).get(
            current_batch_id, 0
        )

        # ë¶„ê¸° ë¡œì§
        if quality_score >= 0.7:
            return "approve"
        elif regeneration_attempts >= 2:
            return "retry_strategy"  # ìµœëŒ€ ì¬ìƒì„± ì‹œë„ ì´ˆê³¼ì‹œ ë‹¤ë¥¸ ì „ëµ
        else:
            return "regenerate"

    # ============ BasePipeline í•„ìˆ˜ ë©”ì„œë“œë“¤ ============

    async def run(
        self, input_data: Dict[str, Any], session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Pipeline ì‹¤í–‰"""
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {**self._get_default_state(), **input_data}

            if session_id:
                initial_state["session_id"] = session_id

            self.logger.info(
                f"ğŸš€ Test Generation Pipeline ì‹œì‘: {initial_state['pipeline_id']}"
            )

            # LangGraph ì‹¤í–‰
            final_state = await self.compiled_graph.ainvoke(
                initial_state, config={"recursion_limit": 50}
            )

            # ê²°ê³¼ ë°˜í™˜
            return {
                "status": final_state.get("processing_status", "completed"),
                "pipeline_id": final_state["pipeline_id"],
                "total_questions": final_state.get("total_questions", 0),  # ì •í™•í•œ ê°’
                "processing_time": self._calculate_processing_time(final_state),
                "batch_results": final_state.get("batch_quality_scores", {}),
                "state": final_state,
            }

        except Exception as e:
            self.logger.error(f"âŒ Pipeline ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "pipeline_id": input_data.get("pipeline_id", "unknown"),
            }

    def _calculate_processing_time(self, final_state: Dict[str, Any]) -> float:
        """ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°"""
        try:
            from datetime import datetime

            start_time = datetime.fromisoformat(final_state["started_at"])
            end_time = datetime.fromisoformat(
                final_state.get("completed_at", datetime.now().isoformat())
            )
            return (end_time - start_time).total_seconds()
        except:
            return 0.0
