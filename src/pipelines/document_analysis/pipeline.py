"""
Document Analysis Pipeline
ë¬¸ì„œ ë¶„ì„ ì—ì´ì „íŠ¸ë§Œì„ ìœ„í•œ ë…ë¦½ì ì¸ íŒŒì´í”„ë¼ì¸

ê¸°ëŠ¥:
- PDF ë¬¸ì„œ íŒŒì‹± (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ)
- í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¬¸ì„œ ìš”ì•½
- ChromaDB ì—…ë¡œë“œ
"""

import os
import time
from typing import Dict, Any, Optional
from src.agents.document_analyzer.agent import DocumentAnalyzerAgent
from utils.change_name import normalize_collection_name


class DocumentAnalysisPipeline:
    """ë¬¸ì„œ ë¶„ì„ ì „ìš© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, collection_name: str = None, auto_upload_chromadb: bool = True):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            collection_name: ChromaDB ì»¬ë ‰ì…˜ëª…
            auto_upload_chromadb: ChromaDB ìë™ ì—…ë¡œë“œ ì—¬ë¶€
        """
        self.collection_name = collection_name
        self.auto_upload_chromadb = auto_upload_chromadb
        self.analyzer = None
    
    def run(
        self, 
        pdf_path: str, 
        extract_keywords: bool = True,
        collection_name: str = None
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            pdf_path: ë¶„ì„í•  PDF íŒŒì¼ ê²½ë¡œ
            extract_keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€
            collection_name: ì»¬ë ‰ì…˜ëª… (ì´ˆê¸°í™” ì‹œ ì„¤ì •í•œ ê°’ ìš°ì„ )
            
        Returns:
            Dict: ë¬¸ì„œ ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()
        
        # ì»¬ë ‰ì…˜ëª… ì„¤ì •
        final_collection_name = collection_name or self.collection_name
        if not final_collection_name:
            filename = os.path.splitext(os.path.basename(pdf_path))[0]
            final_collection_name = normalize_collection_name(filename)
        
        print("ğŸ”„ Document Analysis Pipeline ì‹œì‘")
        print(f"ğŸ“„ ë¬¸ì„œ: {pdf_path}")
        print(f"ğŸ“¦ ì»¬ë ‰ì…˜: {final_collection_name}")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ: {'í™œì„±í™”' if extract_keywords else 'ë¹„í™œì„±í™”'}")
        print(f"ğŸ’¾ ChromaDB ì—…ë¡œë“œ: {'í™œì„±í™”' if self.auto_upload_chromadb else 'ë¹„í™œì„±í™”'}")
        print("=" * 60)
        
        try:
            # DocumentAnalyzer ì´ˆê¸°í™” ë° ì‹¤í–‰
            self.analyzer = DocumentAnalyzerAgent(
                collection_name=final_collection_name,
                auto_upload_chromadb=self.auto_upload_chromadb
            )
            
            # ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰
            result = self.analyzer.analyze_document(pdf_path, extract_keywords)
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ êµ¬ì„±
            pipeline_result = {
                "pipeline_info": {
                    "pipeline_type": "document_analysis",
                    "pdf_path": pdf_path,
                    "collection_name": final_collection_name,
                    "extract_keywords": extract_keywords,
                    "auto_upload_chromadb": self.auto_upload_chromadb,
                    "processing_time": round(processing_time, 2),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "analysis_result": result,
                "status": result.get("processing_status", "unknown")
            }
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nâœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ!")
            print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"ğŸ“Š ë¶„ì„ ìƒíƒœ: {result.get('processing_status')}")
            print(f"ğŸ“ ì´ ë¸”ë¡: {result.get('total_blocks', 0)}ê°œ")
            if extract_keywords:
                print(f"ğŸ”‘ í‚¤ì›Œë“œ: {len(result.get('keywords', []))}ê°œ")
                print(f"ğŸ“‹ ì£¼ì œ: {len(result.get('main_topics', []))}ê°œ")
            if self.auto_upload_chromadb:
                print(f"ğŸ’¾ ChromaDB: {result.get('chromadb_upload_count', 0)}ê°œ ì²­í¬ ì—…ë¡œë“œ")
            
            return pipeline_result
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "pipeline_info": {
                    "pipeline_type": "document_analysis",
                    "pdf_path": pdf_path,
                    "collection_name": final_collection_name,
                    "processing_time": round(time.time() - start_time, 2),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "analysis_result": {},
                "status": "failed",
                "error": str(e)
            }


def run_document_analysis(
    pdf_path: str,
    collection_name: str = None,
    extract_keywords: bool = True,
    auto_upload_chromadb: bool = True
) -> Dict[str, Any]:
    """
    ë¬¸ì„œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        collection_name: ì»¬ë ‰ì…˜ëª…
        extract_keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€
        auto_upload_chromadb: ChromaDB ìë™ ì—…ë¡œë“œ ì—¬ë¶€
        
    Returns:
        Dict: ë¶„ì„ ê²°ê³¼
    """
    pipeline = DocumentAnalysisPipeline(collection_name, auto_upload_chromadb)
    return pipeline.run(pdf_path, extract_keywords)


if __name__ == "__main__":
    import glob
    
    print("ğŸ“„ Document Analysis Pipeline")
    print("=" * 50)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ
    pdf_files = glob.glob("data/raw_docs/*.pdf")
    if not pdf_files:
        print("âŒ data/raw_docs/ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)
    
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ:")
    for i, pdf_file in enumerate(pdf_files, 1):
        filename = pdf_file.split('/')[-1]
        print(f"  {i}. {filename}")
    
    # ë¬¸ì„œ ì„ íƒ
    try:
        choice = int(input(f"\në¶„ì„í•  ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-{len(pdf_files)}): "))
        if 1 <= choice <= len(pdf_files):
            selected_pdf = pdf_files[choice - 1]
            print(f"âœ… ì„ íƒëœ ë¬¸ì„œ: {selected_pdf.split('/')[-1]}")
        else:
            print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
            exit(1)
    except ValueError:
        print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        exit(1)
    
    # Collection ëª… ì…ë ¥
    default_collection = selected_pdf.split('/')[-1].replace('.pdf', '').replace(' ', '_').lower()
    collection_name = input(f"\nCollection ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: {default_collection}): ").strip()
    if not collection_name:
        collection_name = default_collection
    
    # ì˜µì…˜ ì„¤ì •
    extract_keywords = input("í‚¤ì›Œë“œ ì¶”ì¶œì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower() in ['y', 'yes']
    auto_upload = input("ChromaDB ìë™ ì—…ë¡œë“œë¥¼ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower() in ['y', 'yes']
    
    print(f"\nğŸ”„ ë¬¸ì„œ ë¶„ì„ ì‹œì‘...")
    print(f"ğŸ“„ ë¬¸ì„œ: {selected_pdf}")
    print(f"ğŸ“¦ Collection: {collection_name}")
    print(f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ: {'í™œì„±í™”' if extract_keywords else 'ë¹„í™œì„±í™”'}")
    print(f"ğŸ’¾ ChromaDB ì—…ë¡œë“œ: {'í™œì„±í™”' if auto_upload else 'ë¹„í™œì„±í™”'}")
    
    result = run_document_analysis(
        pdf_path=selected_pdf,
        collection_name=collection_name,
        extract_keywords=extract_keywords,
        auto_upload_chromadb=auto_upload
    )
    print(f"\nìµœì¢… ê²°ê³¼: {result['status']}")