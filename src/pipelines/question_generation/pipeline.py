"""
Question Generation Pipeline
Î¨∏Ï†ú ÏÉùÏÑ± ÏóêÏù¥Ï†ÑÌä∏ÎßåÏùÑ ÏúÑÌïú ÎèÖÎ¶ΩÏ†ÅÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏

Í∏∞Îä•:
- ÌååÏã±Îêú Î∏îÎ°ù(ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄ, Ìëú)ÏùÑ ÏÇ¨Ïö©Ìïú Î¨∏Ï†ú ÏÉùÏÑ±
- Gemini VisionÏùÑ ÌôúÏö©Ìïú Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò Î¨∏Ï†ú ÏÉùÏÑ±
- ÌÖåÏä§Ìä∏ ÏÑ§Ï†ïÏóê Îî∞Î•∏ ÎßûÏ∂§Ìòï Î¨∏Ï†ú ÏÉùÏÑ±
- ÏÉùÏÑ±Îêú Î¨∏Ï†ú Ï†ÄÏû• Î∞è Í¥ÄÎ¶¨
"""

import os
import json
import time
from typing import Dict, Any, List, Optional
from datetime import datetime
from src.agents.question_generator.agent import QuestionGeneratorAgent


class QuestionGenerationPipeline:
    """Î¨∏Ï†ú ÏÉùÏÑ± Ï†ÑÏö© ÌååÏù¥ÌîÑÎùºÏù∏"""
    
    def __init__(self, collection_name: str = None):
        """
        ÌååÏù¥ÌîÑÎùºÏù∏ Ï¥àÍ∏∞Ìôî
        
        Args:
            collection_name: Ïª¨Î†âÏÖòÎ™Ö (Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú Í≤∞Ï†ï)
        """
        self.collection_name = collection_name
        self.question_generator = None
    
    def run(
        self,
        blocks: List[Dict],
        num_objective: int = 5,
        num_subjective: int = 3,
        source_file: str = "document.pdf",
        keywords: List[str] = None,
        main_topics: List[str] = None,
        summary: str = "",
        test_config: Dict[str, Any] = None,
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        Î¨∏Ï†ú ÏÉùÏÑ± ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
        
        Args:
            blocks: Î¨∏ÏÑú Î∏îÎ°ùÎì§ (ÌååÏã±Îêú ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄ, Ìëú)
            num_objective: Í∞ùÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
            num_subjective: Ï£ºÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
            source_file: ÏõêÎ≥∏ ÌååÏùºÎ™Ö
            keywords: ÌÇ§ÏõåÎìú Î™©Î°ù
            main_topics: Ï£ºÏöî Ï£ºÏ†ú Î™©Î°ù
            summary: Î¨∏ÏÑú ÏöîÏïΩ
            test_config: ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï (ÏòµÏÖò)
            save_results: Í≤∞Í≥º Ï†ÄÏû• Ïó¨Î∂Ä
            
        Returns:
            Dict: Î¨∏Ï†ú ÏÉùÏÑ± Í≤∞Í≥º
        """
        start_time = time.time()
        
        print("ü§ñ Question Generation Pipeline ÏãúÏûë")
        print(f"üìÑ ÏõêÎ≥∏ ÌååÏùº: {source_file}")
        print(f"üì¶ Ïª¨Î†âÏÖò: {self.collection_name or 'default'}")
        print(f"üìù Ï¥ù Î∏îÎ°ù: {len(blocks)}Í∞ú")
        print(f"üéØ Î™©Ìëú Î¨∏Ï†ú: Í∞ùÍ¥ÄÏãù {num_objective}Í∞ú, Ï£ºÍ¥ÄÏãù {num_subjective}Í∞ú")
        if keywords:
            print(f"üîë ÌÇ§ÏõåÎìú: {len(keywords)}Í∞ú")
        if main_topics:
            print(f"üìã Ï£ºÏ†ú: {len(main_topics)}Í∞ú")
        print("=" * 60)
        
        try:
            # QuestionGenerator Ï¥àÍ∏∞Ìôî
            self.question_generator = QuestionGeneratorAgent(self.collection_name)
            
            # Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìñâ
            print("\nüîÑ GPT-4 VisionÏúºÎ°ú Î¨∏Ï†ú ÏÉùÏÑ± Ï§ë...")
            print(f"üîç VectorDB Í≤ÄÏÉâ: {'ÌôúÏÑ±Ìôî' if self.collection_name else 'ÎπÑÌôúÏÑ±Ìôî'}")
            generation_result = self.question_generator.generate_questions_from_blocks(
                blocks=blocks,
                num_objective=num_objective,
                num_subjective=num_subjective,
                source_file=source_file,
                keywords=keywords or [],
                main_topics=main_topics or [],
                summary=summary,
                test_config=test_config,
                use_vectordb_search=bool(self.collection_name)
            )
            
            # Ï≤òÎ¶¨ ÏãúÍ∞Ñ Í≥ÑÏÇ∞
            processing_time = time.time() - start_time
            
            # Í≤∞Í≥º Íµ¨ÏÑ±
            pipeline_result = {
                "pipeline_info": {
                    "pipeline_type": "question_generation",
                    "source_file": source_file,
                    "collection_name": self.collection_name,
                    "total_blocks": len(blocks),
                    "target_objective": num_objective,
                    "target_subjective": num_subjective,
                    "processing_time": round(processing_time, 2),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "input_data": {
                    "blocks_count": len(blocks),
                    "blocks_breakdown": self._analyze_blocks(blocks),
                    "keywords": keywords or [],
                    "main_topics": main_topics or [],
                    "summary": summary,
                    "test_config": test_config
                },
                "generation_result": generation_result,
                "status": generation_result.get("status", "unknown")
            }
            
            # Í≤∞Í≥º Ï†ÄÏû•
            if save_results and generation_result.get("status") == "completed":
                saved_files = self._save_results(pipeline_result)
                pipeline_result["saved_files"] = saved_files
            
            # Í≤∞Í≥º Ï∂úÎ†•
            if generation_result.get("status") == "completed":
                print(f"\n‚úÖ Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å!")
                print(f"‚è±Ô∏è  Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {processing_time:.2f}Ï¥à")
                print(f"üìä ÏÉùÏÑ±Îêú Î¨∏Ï†ú: {generation_result.get('total_questions', 0)}Í∞ú")
                print(f"   - Í∞ùÍ¥ÄÏãù: {generation_result.get('objective_count', 0)}Í∞ú")
                print(f"   - Ï£ºÍ¥ÄÏãù: {generation_result.get('subjective_count', 0)}Í∞ú")
                if save_results:
                    print(f"üíæ Ï†ÄÏû•Îêú ÌååÏùº: {len(pipeline_result.get('saved_files', []))}Í∞ú")
            else:
                print(f"\n‚ùå Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®!")
                if generation_result.get("error"):
                    print(f"Ïò§Î•ò: {generation_result['error']}")
            
            return pipeline_result
            
        except Exception as e:
            print(f"‚ùå Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {
                "pipeline_info": {
                    "pipeline_type": "question_generation",
                    "source_file": source_file,
                    "collection_name": self.collection_name,
                    "processing_time": round(time.time() - start_time, 2),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "input_data": {
                    "blocks_count": len(blocks),
                    "keywords": keywords or [],
                    "main_topics": main_topics or []
                },
                "generation_result": {},
                "status": "failed",
                "error": str(e)
            }
    
    def run_from_analysis_result(
        self,
        analysis_result_file: str,
        num_objective: int = 5,
        num_subjective: int = 3,
        test_config_file: str = None,
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        Î¨∏ÏÑú Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùºÎ°úÎ∂ÄÌÑ∞ Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìñâ
        
        Args:
            analysis_result_file: Î¨∏ÏÑú Î∂ÑÏÑù Í≤∞Í≥º JSON ÌååÏùº Í≤ΩÎ°ú
            num_objective: Í∞ùÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
            num_subjective: Ï£ºÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
            test_config_file: ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú (ÏòµÏÖò)
            save_results: Í≤∞Í≥º Ï†ÄÏû• Ïó¨Î∂Ä
            
        Returns:
            Dict: Î¨∏Ï†ú ÏÉùÏÑ± Í≤∞Í≥º
        """
        try:
            # Î∂ÑÏÑù Í≤∞Í≥º Î°úÎìú
            with open(analysis_result_file, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            analysis_result = analysis_data.get('analysis_result', {})
            
            # ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï Î°úÎìú (ÏòµÏÖò)
            test_config = None
            if test_config_file and os.path.exists(test_config_file):
                with open(test_config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    test_config = config_data.get('test_config', {})
                    
                    # ÌÖåÏä§Ìä∏ ÏÑ§Ï†ïÏóêÏÑú Î¨∏Ï†ú Ïàò Í∞ÄÏ†∏Ïò§Í∏∞
                    if 'num_objective' in test_config:
                        num_objective = test_config['num_objective']
                    if 'num_subjective' in test_config:
                        num_subjective = test_config['num_subjective']
            
            # Ïª¨Î†âÏÖòÎ™Ö ÏÑ§Ï†ï
            pipeline_info = analysis_data.get('pipeline_info', {})
            collection_name = pipeline_info.get('collection_name')
            if collection_name:
                self.collection_name = collection_name
            
            return self.run(
                blocks=analysis_result.get('blocks', []),
                num_objective=num_objective,
                num_subjective=num_subjective,
                source_file=pipeline_info.get('pdf_path', 'document.pdf'),
                keywords=analysis_result.get('keywords', []),
                main_topics=analysis_result.get('main_topics', []),
                summary=analysis_result.get('summary', ''),
                test_config=test_config,
                save_results=save_results
            )
            
        except Exception as e:
            print(f"‚ùå Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº Î°úÎî© Ïã§Ìå®: {e}")
            return {
                "pipeline_info": {
                    "pipeline_type": "question_generation",
                    "analysis_file": analysis_result_file,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "status": "failed",
                "error": f"Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº Î°úÎî© Ïã§Ìå®: {str(e)}"
            }
    
    def _analyze_blocks(self, blocks: List[Dict]) -> Dict[str, int]:
        """Î∏îÎ°ù Ïú†ÌòïÎ≥Ñ Î∂ÑÏÑù"""
        breakdown = {"text": 0, "table": 0, "image": 0, "other": 0}
        
        for block in blocks:
            block_type = block.get("type", "other")
            if block_type in ["paragraph", "heading", "section"]:
                breakdown["text"] += 1
            elif block_type == "table":
                breakdown["table"] += 1
            elif block_type == "image":
                breakdown["image"] += 1
            else:
                breakdown["other"] += 1
        
        return breakdown
    
    def _save_results(self, pipeline_result: Dict[str, Any]) -> List[str]:
        """Í≤∞Í≥º ÌååÏùº Ï†ÄÏû•"""
        saved_files = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        source_name = os.path.splitext(os.path.basename(
            pipeline_result["pipeline_info"]["source_file"]
        ))[0]
        
        try:
            generation_result = pipeline_result["generation_result"]
            
            # Collection Î™Ö Í∏∞Î∞ò ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞
            collection_dir = self.collection_name or "default"
            
            # ÏÉùÏÑ±Îêú Î¨∏Ï†ú Ï†ÄÏû•
            if generation_result.get("questions"):
                questions_dir = f"data/outputs/generated_questions/{collection_dir}"
                os.makedirs(questions_dir, exist_ok=True)
                
                questions_data = {
                    "test_info": {
                        "source_file": pipeline_result["pipeline_info"]["source_file"],
                        "collection_name": pipeline_result["pipeline_info"]["collection_name"],
                        "generation_date": datetime.now().isoformat(),
                        "test_type": "auto_generated"
                    },
                    "question_summary": {
                        "total_questions": len(generation_result["questions"]),
                        "objective_questions": generation_result.get("objective_count", 0),
                        "subjective_questions": generation_result.get("subjective_count", 0)
                    },
                    "questions": generation_result["questions"],
                    "pipeline_info": pipeline_result["pipeline_info"],
                    "input_data": pipeline_result["input_data"]
                }
                
                questions_file = f"{questions_dir}/{source_name}_questions_{timestamp}.json"
                with open(questions_file, 'w', encoding='utf-8') as f:
                    json.dump(questions_data, f, ensure_ascii=False, indent=2)
                saved_files.append(questions_file)
                print(f"üíæ ÏÉùÏÑ±Îêú Î¨∏Ï†ú Ï†ÄÏû•: {questions_file}")
            
            # Ï†ÑÏ≤¥ Í≤∞Í≥º Ï†ÄÏû•
            results_dir = "data/outputs/question_generation_results"
            os.makedirs(results_dir, exist_ok=True)
            
            result_file = f"{results_dir}/{source_name}_question_generation_{timestamp}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(pipeline_result, f, ensure_ascii=False, indent=2)
            saved_files.append(result_file)
            print(f"üíæ Ï†ÑÏ≤¥ Í≤∞Í≥º Ï†ÄÏû•: {result_file}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®: {e}")
        
        return saved_files


def run_question_generation(
    blocks: List[Dict],
    num_objective: int = 5,
    num_subjective: int = 3,
    collection_name: str = None,
    source_file: str = "document.pdf"
) -> Dict[str, Any]:
    """
    Î¨∏Ï†ú ÏÉùÏÑ± ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ Ìé∏Ïùò Ìï®Ïàò
    
    Args:
        blocks: Î¨∏ÏÑú Î∏îÎ°ùÎì§
        num_objective: Í∞ùÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
        num_subjective: Ï£ºÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
        collection_name: Ïª¨Î†âÏÖòÎ™Ö
        source_file: ÏõêÎ≥∏ ÌååÏùºÎ™Ö
        
    Returns:
        Dict: Î¨∏Ï†ú ÏÉùÏÑ± Í≤∞Í≥º
    """
    pipeline = QuestionGenerationPipeline(collection_name)
    return pipeline.run(blocks, num_objective, num_subjective, source_file)


def run_question_generation_from_file(
    analysis_result_file: str,
    num_objective: int = 5,
    num_subjective: int = 3,
    test_config_file: str = None
) -> Dict[str, Any]:
    """
    Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùºÎ°úÎ∂ÄÌÑ∞ Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìñâ Ìé∏Ïùò Ìï®Ïàò
    
    Args:
        analysis_result_file: Î¨∏ÏÑú Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº Í≤ΩÎ°ú
        num_objective: Í∞ùÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
        num_subjective: Ï£ºÍ¥ÄÏãù Î¨∏Ï†ú Ïàò
        test_config_file: ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú
        
    Returns:
        Dict: Î¨∏Ï†ú ÏÉùÏÑ± Í≤∞Í≥º
    """
    pipeline = QuestionGenerationPipeline()
    return pipeline.run_from_analysis_result(
        analysis_result_file, num_objective, num_subjective, test_config_file
    )


if __name__ == "__main__":
    import glob
    import os
    import json
    
    print("ü§ñ Question Generation Pipeline")
    print("=" * 50)
    
    # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú collection Î™©Î°ù ÌëúÏãú
    keywords_base_dir = "data/outputs/keywords_summary"
    configs_base_dir = "data/outputs/test_configs"
    analysis_base_dir = "data/outputs/document_analysis"
    
    if not os.path.exists(keywords_base_dir):
        print("‚ùå keywords_summary ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Î¨∏ÏÑú Î∂ÑÏÑùÏùÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.")
        exit(1)
    
    collections = [d for d in os.listdir(keywords_base_dir) if os.path.isdir(os.path.join(keywords_base_dir, d))]
    if not collections:
        print("‚ùå ÏÇ¨Ïö© Í∞ÄÎä•Ìïú collectionÏù¥ ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Î¨∏ÏÑú Î∂ÑÏÑùÏùÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.")
        exit(1)
    
    print("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Collection:")
    for i, collection in enumerate(collections, 1):
        # ÌÇ§ÏõåÎìú, ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï, Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº Í∞úÏàò ÌôïÏù∏
        keyword_files = glob.glob(f"{keywords_base_dir}/{collection}/*_keywords_summary_*.json")
        config_files = glob.glob(f"{configs_base_dir}/{collection}/*_test_config_*.json") if os.path.exists(f"{configs_base_dir}/{collection}") else []
        analysis_files = glob.glob(f"{analysis_base_dir}/{collection}/*_analysis_result_*.json") if os.path.exists(f"{analysis_base_dir}/{collection}") else []
        print(f"  {i}. {collection} (ÌÇ§ÏõåÎìú: {len(keyword_files)}Í∞ú, ÌÖåÏä§Ìä∏ÏÑ§Ï†ï: {len(config_files)}Í∞ú, Î∂ÑÏÑùÍ≤∞Í≥º: {len(analysis_files)}Í∞ú)")
    
    # Collection ÏÑ†ÌÉù
    try:
        choice = int(input(f"\nÏÇ¨Ïö©Ìï† Collection Î≤àÌò∏Î•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî (1-{len(collections)}): "))
        if 1 <= choice <= len(collections):
            selected_collection = collections[choice - 1]
            print(f"‚úÖ ÏÑ†ÌÉùÎêú Collection: {selected_collection}")
        else:
            print("‚ùå ÏûòÎ™ªÎêú Î≤àÌò∏ÏûÖÎãàÎã§.")
            exit(1)
    except ValueError:
        print("‚ùå Ïà´ÏûêÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
        exit(1)
    
    # ÌÇ§ÏõåÎìú ÌååÏùº Ï∞æÍ∏∞
    collection_keywords_dir = os.path.join(keywords_base_dir, selected_collection)
    keyword_files = glob.glob(f"{collection_keywords_dir}/*_keywords_summary_*.json")
    
    if not keyword_files:
        print(f"‚ùå {selected_collection} collectionÏóê ÌÇ§ÏõåÎìú ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.")
        exit(1)
    
    latest_keywords_file = sorted(keyword_files)[-1]
    print(f"üìÑ ÌÇ§ÏõåÎìú ÌååÏùº: {os.path.basename(latest_keywords_file)}")
    
    # ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï ÌååÏùº Ï∞æÍ∏∞ (ÏÑ†ÌÉùÏ†Å)
    collection_configs_dir = os.path.join(configs_base_dir, selected_collection)
    config_file = None
    if os.path.exists(collection_configs_dir):
        config_files = glob.glob(f"{collection_configs_dir}/*_test_config_*.json")
        if config_files:
            config_file = sorted(config_files)[-1]
            print(f"‚öôÔ∏è ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï ÌååÏùº: {os.path.basename(config_file)}")
        else:
            print("‚ö†Ô∏è ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ ÏÑ§Ï†ïÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
    else:
        print("‚ö†Ô∏è ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä ÌÖåÏä§Ìä∏ ÏÑ§Í≥ÑÎ•º Ïã§ÌñâÌïòÏÑ∏Ïöî.")
    
    # Î¨∏Ï†ú Ïàò ÏûÖÎ†•
    print("\nÎ¨∏Ï†ú ÏÉùÏÑ± ÏÑ§Ï†ï:")
    try:
        num_objective = int(input("Í∞ùÍ¥ÄÏãù Î¨∏Ï†ú Ïàò (Í∏∞Î≥∏Í∞í: 5): ") or "5")
        num_subjective = int(input("Ï£ºÍ¥ÄÏãù Î¨∏Ï†ú Ïàò (Í∏∞Î≥∏Í∞í: 3): ") or "3")
    except ValueError:
        print("‚ùå Ïà´ÏûêÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
        exit(1)
    
    print(f"\nüîÑ Î¨∏Ï†ú ÏÉùÏÑ± ÏãúÏûë...")
    print(f"üì¶ Collection: {selected_collection}")
    print(f"üéØ Î™©Ìëú: Í∞ùÍ¥ÄÏãù {num_objective}Í∞ú, Ï£ºÍ¥ÄÏãù {num_subjective}Í∞ú")
    print(f"üîç VectorDB Í≤ÄÏÉâ: ÌôúÏÑ±Ìôî")
    
    # Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùºÏóêÏÑú Î∏îÎ°ù Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    try:
        # ÌÇ§ÏõåÎìú ÌååÏùº Î°úÎìú
        with open(latest_keywords_file, 'r', encoding='utf-8') as f:
            keywords_data = json.load(f)
        content_analysis = keywords_data.get('content_analysis', {})
        
        # Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº Ï∞æÍ∏∞
        collection_analysis_dir = os.path.join(analysis_base_dir, selected_collection)
        blocks = []
        
        if os.path.exists(collection_analysis_dir):
            analysis_files = glob.glob(f"{collection_analysis_dir}/*_analysis_result_*.json")
            if analysis_files:
                latest_analysis_file = sorted(analysis_files)[-1]
                print(f"üìä Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº: {os.path.basename(latest_analysis_file)}")
                
                with open(latest_analysis_file, 'r', encoding='utf-8') as f:
                    analysis_data = json.load(f)
                
                analysis_result = analysis_data.get('analysis_result', {})
                blocks = analysis_result.get('blocks', [])
                
                if not blocks:
                    print("‚ö†Ô∏è Î∏îÎ°ù Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ Î∏îÎ°ùÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.")
                    blocks = [
                        {
                            'type': 'paragraph',
                            'content': f"Î¨∏ÏÑú ÏöîÏïΩ: {content_analysis.get('summary', '')}",
                            'metadata': {'page': 1, 'source': 'keywords_summary'}
                        }
                    ]
            else:
                print("‚ö†Ô∏è Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ Î∏îÎ°ùÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.")
                blocks = [
                    {
                        'type': 'paragraph',
                        'content': f"Î¨∏ÏÑú ÏöîÏïΩ: {content_analysis.get('summary', '')}",
                        'metadata': {'page': 1, 'source': 'keywords_summary'}
                    }
                ]
        else:
            print("‚ö†Ô∏è Î∂ÑÏÑù Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ Î∏îÎ°ùÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.")
            blocks = [
                {
                    'type': 'paragraph',
                    'content': f"Î¨∏ÏÑú ÏöîÏïΩ: {content_analysis.get('summary', '')}",
                    'metadata': {'page': 1, 'source': 'keywords_summary'}
                }
            ]
        
        print(f"üìù ÏÇ¨Ïö©Ìï† Î∏îÎ°ù: {len(blocks)}Í∞ú")
        
        # ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
        pipeline = QuestionGenerationPipeline(collection_name=selected_collection)
        result = pipeline.run(
            blocks=blocks,
            num_objective=num_objective,
            num_subjective=num_subjective,
            source_file=keywords_data.get('document_info', {}).get('source_file', 'document.pdf'),
            keywords=content_analysis.get('keywords', []),
            main_topics=content_analysis.get('main_topics', []),
            summary=content_analysis.get('summary', ''),
            test_config=json.load(open(config_file, 'r', encoding='utf-8')).get('test_config') if config_file else None
        )
        
        print(f"\nÏµúÏ¢Ö Í≤∞Í≥º: {result['status']}")
        
    except Exception as e:
        print(f"‚ùå Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {e}")