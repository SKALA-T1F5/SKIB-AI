"""
ë¬¸ì œ ìƒì„± íŒŒì´í”„ë¼ì¸
í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ë¸”ë¡ìœ¼ë¡œ ë¶„í•´í•˜ê³  GPT-4o Visionì„ ì‚¬ìš©í•œ ìë™ ë¬¸ì œ ìƒì„± íŒŒì´í”„ë¼ì¸

ì£¼ìš” ê¸°ëŠ¥:
- PDF ë¬¸ì„œ íŒŒì‹± ë° ë¸”ë¡ ë¶„í•´ 
- Vision API ë©”ì‹œì§€ í¬ë§· ë³€í™˜
- ë²¡í„° ì„ë² ë”© ìƒì„±
- GPT-4o Vision ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
- ì§ˆë¬¸ê³¼ ë©”íƒ€ë°ì´í„° ì €ì¥

ìµœì¢…ì ìœ¼ë¡œëŠ” PDF í•œ ê°œì— ëŒ€í•´ ë¬¸í•­ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

from src.agents.document_analyzer.tools.unified_parser import parse_pdf_unified
from src.agents.question_generator.tools.question_generator import QuestionGenerator
from utils.change_name import normalize_collection_name
from sentence_transformers import SentenceTransformer
import os
import sys
import time

# ì„ë² ë”© ëª¨ë¸ ë¡œë”© (bge ëª¨ë¸ ì‚¬ìš©)
embedding_model = SentenceTransformer("BAAI/bge-base-en")

def run_question_generation_pipeline(pdf_path: str, num_objective: int = 3, num_subjective: int = 3):
    """
    ë¬¸ì œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ (ë¬¸ìì—´ ë˜ëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤)
        num_objective: ìƒì„±í•  ê°ê´€ì‹ ë¬¸ì œ ìˆ˜
        num_subjective: ìƒì„±í•  ì£¼ê´€ì‹ ë¬¸ì œ ìˆ˜
        
    Returns:
        List[Dict]: ìƒì„±ëœ ë¬¸ì œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    # PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
    if isinstance(pdf_path, int):
        if pdf_path == 1:
            pdf_path = "data/raw_docs/2.ì—°ë§ì •ì‚°ì‹œìŠ¤í…œ(YETA) ë§¤ë‰´ì–¼.pdf"
        elif pdf_path == 2:
            pdf_path = "data/raw_docs/2_AGS Trouble shooting ê°€ì´ë“œ_v1.1.pdf"
        elif pdf_path == 3:
            pdf_path = "data/raw_docs/alopex_UI_1.1.2_ê°œë°œê°€ì´ë“œ.pdf"
        elif pdf_path == 4:
            pdf_path = "data/raw_docs/To-Be ì¬ë¬´Portal_Process ì •ì˜ì„œ_FP-07_íƒ„ì†Œë°°ì¶œê¶Œ_v1.0.pdf"
        elif pdf_path == 5:
            pdf_path = "data/raw_docs/Process íë¦„ë„_sample_250527.pdf"
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” PDF ì¸ë±ìŠ¤: {pdf_path}")
    
    print(f"ğŸš€ ë¬¸ì œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼: {pdf_path}")
    print(f"ğŸ¯ ëª©í‘œ: ê°ê´€ì‹ {num_objective}ê°œ, ì£¼ê´€ì‹ {num_subjective}ê°œ")
    print("=" * 80)
    
    if not os.path.exists(pdf_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return []
    
    filename = os.path.splitext(os.path.basename(pdf_path))[0]
    collection_name = normalize_collection_name(filename)
    
    # 1. PDFë¥¼ Docling ìŠ¤íƒ€ì¼ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜ (í˜ì´ì§€ ì •ë³´ í¬í•¨)
    print("ğŸ“„ 1ë‹¨ê³„: PDF íŒŒì‹± ë° ë¸”ë¡ ë¶„í•´")
    blocks = parse_pdf_unified(pdf_path)
    print(f"âœ… {len(blocks)}ê°œ ë¸”ë¡ ì¶”ì¶œ ì™„ë£Œ")

    # 2. QuestionGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡ì„ Vision ì²­í¬ë¡œ ë³€í™˜
    print("ğŸ”„ 2ë‹¨ê³„: Vision API ë©”ì‹œì§€ ë³€í™˜")
    generator = QuestionGenerator()
    processed_vision_chunks = generator._blocks_to_vision_chunks(blocks)
    n_chunks = len(processed_vision_chunks)
    print(f"âœ… {n_chunks}ê°œ Vision ì²­í¬ ìƒì„± ì™„ë£Œ")

    # ì²­í¬ê°€ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬
    if n_chunks == 0:
        print("âŒ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë¬¸ì„œ í˜•ì‹ì´ë‚˜ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []

    source_file_name = os.path.basename(pdf_path)

    # ë¬¸ì œ ìˆ˜ ë¶„ë°° ê³„ì‚°
    def distribute(total, n):
        if n == 0:
            return []
        base = total // n
        remainder = total % n
        return [base + 1 if i < remainder else base for i in range(n)]

    obj_per_chunk = distribute(num_objective, n_chunks)
    subj_per_chunk = distribute(num_subjective, n_chunks)
    
    print(f"ğŸ“Š ì²­í¬ë³„ ë¬¸ì œ ë¶„ë°°:")
    print(f"   ê°ê´€ì‹: {obj_per_chunk}")
    print(f"   ì£¼ê´€ì‹: {subj_per_chunk}")

    results = []
    objective_count = 0
    subjective_count = 0

    # 3. ê° processed_vision_chunkì— ëŒ€í•´ ì§ˆë¬¸ ìƒì„± ë° ì €ì¥ ë°˜ë³µ
    print("\nğŸ¤– 3ë‹¨ê³„: GPT-4o Vision ë¬¸ì œ ìƒì„±")
    for i, vision_data in enumerate(processed_vision_chunks):
        if objective_count >= num_objective and subjective_count >= num_subjective:
            break
            
        print(f"  ğŸ“ ì²­í¬ {i+1}/{n_chunks} ì²˜ë¦¬ ì¤‘...")
        
        messages_for_api = vision_data['messages']
        chunk_metadata = vision_data['metadata']

        # chunk_obj êµ¬ì„± ì‹œ, processed_vision_chunksì—ì„œ ë°˜í™˜ëœ ë©”íƒ€ë°ì´í„° í™œìš©
        page_numbers = chunk_metadata.get("pages", [])
        page_info_for_chunk = str(page_numbers[0]) if page_numbers else "N/A"
        
        section_titles = chunk_metadata.get("sections", [])
        section_info_for_chunk = ", ".join(section_titles) if section_titles else ""

        # ì²­í¬ ë©”íƒ€ë°ì´í„° ê°ì²´ êµ¬ì„± (save_question_result ë° DB ì—…ë¡œë“œìš©)
        chunk_obj_for_saving = {
            "chunk_id": f"{collection_name}_vision_c{i}",
            "chunk_type": "vision_processed_chunk",
            "section_title": section_info_for_chunk,
            "source_text": chunk_metadata.get("source_text_combined", ""),
            "project": collection_name,
            "source": source_file_name,
            "page": page_info_for_chunk,
        }

        # ë²¡í„° ì„ë² ë”©ì€ source_text_combined ì „ì²´ì— ëŒ€í•´ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ
        if chunk_obj_for_saving["source_text"]:
            vector = embedding_model.encode(chunk_obj_for_saving["source_text"]).tolist()
            # upload_chunk_to_collection(chunk_obj_for_saving, vector, collection_name) # í•„ìš”ì‹œ DB ì—…ë¡œë“œ
        else:
            vector = []

        # ê° chunkë³„ë¡œ í• ë‹¹ëœ ê°œìˆ˜ë§Œí¼ë§Œ ìš”ì²­
        num_obj = obj_per_chunk[i]
        num_subj = subj_per_chunk[i]

        if num_obj == 0 and num_subj == 0:
            print(f"    â­ï¸ ì²­í¬ {i+1}: í• ë‹¹ëœ ë¬¸ì œ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
            continue

        print(f"    ğŸ¯ ì²­í¬ {i+1}: ê°ê´€ì‹ {num_obj}ê°œ, ì£¼ê´€ì‹ {num_subj}ê°œ ìƒì„± ìš”ì²­")

        try:
            # GPT-4o Vision APIë¥¼ í†µí•´ ì§ˆë¬¸ ìƒì„±
            generator = QuestionGenerator()
            questions_list = generator._generate_question(
                messages=messages_for_api, 
                source=source_file_name, 
                page=page_info_for_chunk,
                num_objective=num_obj,
                num_subjective=num_subj,
            )
            
            print(f"    âœ… ì²­í¬ {i+1}: {len(questions_list)}ê°œ ë¬¸ì œ ìƒì„± ì™„ë£Œ")
            
            # ê²°ê³¼ ì²˜ë¦¬ ë° ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            for question_data in questions_list:
                q_type = question_data["type"]
                if q_type == "OBJECTIVE" and objective_count >= num_objective:
                    continue
                if q_type == "SUBJECTIVE" and subjective_count >= num_subjective:
                    continue

                if q_type == "OBJECTIVE":
                    objective_count += 1
                elif q_type == "SUBJECTIVE":
                    subjective_count += 1

                result = {
                    "type": question_data["type"],
                    "difficulty_level": question_data["difficulty_level"],
                    "question": question_data["question"],
                    "options": question_data.get("options"),
                    "answer": question_data["answer"],
                    "explanation": question_data.get("explanation"),
                    "document_id": 1,  # ë¬¸ì„œ IDëŠ” 1ë¡œ ê³ ì • (ë‚˜ì¤‘ì— ì‹¤ì œ ë¬¸ì„œ IDë¡œ ë³€ê²½ í•„ìš”)
                    "tags": question_data.get("tags", []),
                    "grading_criteria": question_data.get("grading_criteria")
                }

                results.append(result)

            # ìƒì„±ëœ ë¬¸í•­ê³¼ ë©”íƒ€ë°ì´í„° ì €ì¥
            save_question_result(chunk_info=chunk_obj_for_saving, questions_list=questions_list)
            
        except Exception as e:
            print(f"    âŒ ì²­í¬ {i+1} ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # API í˜¸ì¶œ ê°„ ì§€ì—° ì‹œê°„ ìœ ì§€
        time.sleep(1)

    print(f"\nğŸ‰ ë¬¸ì œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   - ì´ ë¬¸ì œ: {len(results)}ê°œ")
    print(f"   - ê°ê´€ì‹: {objective_count}ê°œ (ëª©í‘œ: {num_objective}ê°œ)")
    print(f"   - ì£¼ê´€ì‹: {subjective_count}ê°œ (ëª©í‘œ: {num_subjective}ê°œ)")
    print(f"   - ì»¬ë ‰ì…˜: {collection_name}")
    
    return results


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€
def run_pipeline(pdf_path, num_objective: int = 3, num_subjective: int = 3):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    return run_question_generation_pipeline(pdf_path, num_objective, num_subjective)


# í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  python -m src.pipelines.question_generation.run_pipeline <pdf_path> [num_objective] [num_subjective]")
        print()
        print("ì˜ˆì‹œ:")
        print("  python -m src.pipelines.question_generation.run_pipeline 'data/raw_docs/sample.pdf' 5 3")
        print("  python -m src.pipelines.question_generation.run_pipeline 1  # ì‚¬ì „ ì •ì˜ëœ íŒŒì¼ ì‚¬ìš©")
        sys.exit(1)

    pdf_path = sys.argv[1]
    
    # ìˆ«ìì¸ ê²½ìš° ì •ìˆ˜ë¡œ ë³€í™˜
    try:
        pdf_path = int(pdf_path)
    except ValueError:
        pass  # ë¬¸ìì—´ ê²½ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    num_objective = int(sys.argv[2]) if len(sys.argv) > 2 else 3
    num_subjective = int(sys.argv[3]) if len(sys.argv) > 3 else 3
    
    run_question_generation_pipeline(pdf_path, num_objective, num_subjective)