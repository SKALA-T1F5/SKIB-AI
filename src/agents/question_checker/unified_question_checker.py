#!/usr/bin/env python3
"""
í†µí•© ë¬¸ì œ ê²€ì¦ ë„êµ¬
- ë¬¸ì œ í’ˆì§ˆ í‰ê°€ (LLM ê¸°ë°˜ / ê·œì¹™ ê¸°ë°˜)
- ë¬¸ì„œ ì¶©ì‹¤ë„ ê²€ì¦
- í†µí•© ë³´ê³ ì„œ ìƒì„±
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import glob
from langsmith import traceable

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# VectorDB ê´€ë ¨ import
try:
    from db.vectorDB.chromaDB.search import ChromaDBSearcher
    VECTOR_DB_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ VectorDB ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ğŸ“ ê¸°ë³¸ ì¶©ì‹¤ë„ ê²€ì¦ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    VECTOR_DB_AVAILABLE = False


class UnifiedQuestionChecker:
    """í†µí•© ë¬¸ì œ ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self, use_vector_db: bool = True):
        # ì‹¤ìš©ì  í’ˆì§ˆ í‰ê°€ ê¸°ì¤€ - 1~5ì  ì²™ë„
        self.quality_criteria = {
            "ì ì ˆì„±": "ë¬¸ì œê°€ í•™ìŠµ ëª©í‘œì™€ ìˆ˜ì¤€ì— ì í•©í•œì§€ - 5ì : ë§¤ìš° ì ì ˆ, 3ì : ë³´í†µ, 1ì : ë¶€ì ì ˆ",
            "ë‚œì´ë„ ì ì •ì„±": "ë¬¸ì œ ë‚œì´ë„ê°€ ëª…ì‹œëœ ë ˆë²¨ì— ì í•©í•œì§€ - 5ì : ë§¤ìš° ì ì ˆ, 3ì : ë³´í†µ, 1ì : ë¶€ì ì ˆ", 
            "ëª…í™•ì„± ë° ëª¨í˜¸ì„± ì—†ìŒ": "ë¬¸ì œì™€ ì„ íƒì§€ê°€ ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ì§€ - 5ì : ë§¤ìš° ëª…í™•, 3ì : ë³´í†µ, 1ì : ëª¨í˜¸í•¨",
            "ì •ë‹µ ë° í•´ì„¤ì˜ ì •í™•ì„±": "ì •ë‹µê³¼ í•´ì„¤ì´ ì •í™•í•˜ê³  ë…¼ë¦¬ì ì¸ì§€ - 5ì : ì™„ì „ ì •í™•, 3ì : ëŒ€ì²´ë¡œ ì •í™•, 1ì : ì˜¤ë¥˜ ìˆìŒ",
            "í•™ìŠµì  ê°€ì¹˜ ë° ìœ ìš©ì„±": "êµìœ¡ì  íš¨ê³¼ì™€ ì‹¤ë¬´ í™œìš©ë„ê°€ ë†’ì€ì§€ - 5ì : ë§¤ìš° ìœ ìš©, 3ì : ë³´í†µ, 1ì : ë‚®ìŒ"
        }
        
        self.fidelity_criteria = {
            "ë¬¸ì„œ_ì¶©ì‹¤ë„": "ì›ë³¸ ë¬¸ì„œì™€ì˜ ì¼ì¹˜ì„± - 2ì : ì™„ì „í•œ ê·¼ê±°(ì§ì ‘ ì„œìˆ ), 1ì : ë¶€ë¶„ì  ê·¼ê±°(ì¶”ë¡  í•„ìš”), 0ì : ê·¼ê±° ì—†ìŒ(ë¬´ê´€/ëª¨ìˆœ)"
        }
        
        # VectorDB ì´ˆê¸°í™”
        self.use_vector_db = use_vector_db and VECTOR_DB_AVAILABLE
        self.vector_searcher = None
        
        if self.use_vector_db:
            try:
                self.vector_searcher = ChromaDBSearcher()
                print("âœ… VectorDB ê²€ìƒ‰ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âš ï¸ VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_vector_db = False
                print("ğŸ“ ê¸°ë³¸ ì¶©ì‹¤ë„ ê²€ì¦ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # LLM í”„ë¡¬í”„íŠ¸
        self.quality_prompt = """
ë‹¤ìŒ ë¬¸ì œë¥¼ 5ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”. ê° ë¬¸ì œì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì°¨ë³„í™”ëœ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.

[ê²€ì¦ ê¸°ì¤€]

1ï¸âƒ£ **ì ì ˆì„± (1-5ì )**
   ë¬¸ì œê°€ í•™ìŠµ ëª©í‘œì™€ ìˆ˜ì¤€ì— ì í•©í•œì§€ í‰ê°€í•˜ì„¸ìš”.
   - 5ì : í•™ìŠµ ëª©í‘œì— ë§¤ìš° ì ì ˆí•˜ê³  ìˆ˜ì¤€ì´ ì™„ë²½íˆ ë§ìŒ
   - 4ì : í•™ìŠµ ëª©í‘œì— ì ì ˆí•˜ê³  ìˆ˜ì¤€ì´ ì ë‹¹í•¨
   - 3ì : ë³´í†µ ìˆ˜ì¤€ì˜ ì ì ˆì„±
   - 2ì : ì¼ë¶€ ë¶€ì ì ˆí•œ ìš”ì†Œê°€ ìˆìŒ
   - 1ì : í•™ìŠµ ëª©í‘œë‚˜ ìˆ˜ì¤€ì— ë¶€ì ì ˆí•¨

2ï¸âƒ£ **ë‚œì´ë„ ì ì •ì„± (1-5ì )**
   ë¬¸ì œ ë‚œì´ë„ê°€ ëª…ì‹œëœ ë ˆë²¨(EASY/MEDIUM/HARD)ì— ì í•©í•œì§€ í‰ê°€í•˜ì„¸ìš”.
   - 5ì : ë‚œì´ë„ê°€ ë ˆë²¨ì— ë§¤ìš° ì ì ˆí•¨
   - 4ì : ë‚œì´ë„ê°€ ë ˆë²¨ì— ì ì ˆí•¨
   - 3ì : ë³´í†µ ìˆ˜ì¤€ì˜ ë‚œì´ë„ ì ì •ì„±
   - 2ì : ë‚œì´ë„ê°€ ë ˆë²¨ê³¼ ë‹¤ì†Œ ë§ì§€ ì•ŠìŒ
   - 1ì : ë‚œì´ë„ê°€ ë ˆë²¨ê³¼ ì „í˜€ ë§ì§€ ì•ŠìŒ

3ï¸âƒ£ **ëª…í™•ì„± ë° ëª¨í˜¸ì„± ì—†ìŒ (1-5ì )**
   ë¬¸ì œì™€ ì„ íƒì§€ê°€ ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ì§€ í‰ê°€í•˜ì„¸ìš”.
   - 5ì : ë§¤ìš° ëª…í™•í•˜ê³  ëª¨í˜¸í•¨ì´ ì „í˜€ ì—†ìŒ
   - 4ì : ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
   - 3ì : ë³´í†µ ìˆ˜ì¤€ì˜ ëª…í™•ì„±
   - 2ì : ì¼ë¶€ ëª¨í˜¸í•œ í‘œí˜„ì´ ìˆìŒ
   - 1ì : ëª¨í˜¸í•˜ê±°ë‚˜ ì´í•´í•˜ê¸° ì–´ë ¤ì›€

4ï¸âƒ£ **ì •ë‹µ ë° í•´ì„¤ì˜ ì •í™•ì„± (1-5ì )**
   ì •ë‹µê³¼ í•´ì„¤ì´ ì •í™•í•˜ê³  ë…¼ë¦¬ì ì¸ì§€ í‰ê°€í•˜ì„¸ìš”.
   - 5ì : ì •ë‹µê³¼ í•´ì„¤ì´ ì™„ì „íˆ ì •í™•í•˜ê³  ë…¼ë¦¬ì ì„
   - 4ì : ì •ë‹µê³¼ í•´ì„¤ì´ ì •í™•í•˜ê³  ì ì ˆí•¨
   - 3ì : ë³´í†µ ìˆ˜ì¤€ì˜ ì •í™•ì„±
   - 2ì : ì¼ë¶€ ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆìŒ
   - 1ì : ì •ë‹µì´ë‚˜ í•´ì„¤ì— ëª…ë°±í•œ ì˜¤ë¥˜ê°€ ìˆìŒ

5ï¸âƒ£ **í•™ìŠµì  ê°€ì¹˜ ë° ìœ ìš©ì„± (1-5ì )**
   êµìœ¡ì  íš¨ê³¼ì™€ ì‹¤ë¬´ í™œìš©ë„ê°€ ë†’ì€ì§€ í‰ê°€í•˜ì„¸ìš”.
   - 5ì : ë§¤ìš° ë†’ì€ êµìœ¡ì  íš¨ê³¼ì™€ ì‹¤ë¬´ í™œìš©ë„
   - 4ì : ë†’ì€ êµìœ¡ì  íš¨ê³¼ì™€ ì‹¤ë¬´ í™œìš©ë„
   - 3ì : ë³´í†µ ìˆ˜ì¤€ì˜ í•™ìŠµì  ê°€ì¹˜
   - 2ì : êµìœ¡ì  íš¨ê³¼ê°€ ë‹¤ì†Œ ë¶€ì¡±í•¨
   - 1ì : êµìœ¡ì  íš¨ê³¼ë‚˜ ì‹¤ë¬´ í™œìš©ë„ê°€ ë‚®ìŒ

[í‰ê°€ ì§€ì¹¨]
- **ë‹¤ì–‘í•œ ì ìˆ˜ ë¶„í¬**: 5ê°œ ê¸°ì¤€ì—ì„œ ë‹¤ì–‘í•œ ì ìˆ˜(1-5ì )ë¥¼ í™œìš©í•˜ì—¬ ì°¨ë³„í™”ëœ í‰ê°€ ìˆ˜í–‰
- **êµ¬ì²´ì  ê·¼ê±° ì œì‹œ**: ê° ì ìˆ˜ì— ëŒ€í•œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í‰ê°€ ê·¼ê±° ì‘ì„±
- **ê°ê´€ì  í‰ê°€**: ë¬¸ì œì˜ ì‹¤ì œ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ê´€ì  í¸í–¥ ì—†ì´ í‰ê°€
- **ê°œì„ ì  ì œì‹œ**: ë¶€ì¡±í•œ ë¶€ë¶„ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥ ì œì•ˆ

[ì´ì  ë° íŒì •]
- ì´ì : 25ì  ë§Œì  (ê° ê¸°ì¤€ 5ì ì”©)
- 22-25ì : ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ (Excellent)
- 18-21ì : ê²€í†  í›„ ì‚¬ìš© (Good)
- 14-17ì : ìˆ˜ì • í›„ ì‚¬ìš© (Fair)
- 13ì  ì´í•˜: ì¬ìƒì„± í•„ìš” (Poor)

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:
{
  "ì ì ˆì„±": {
    "ì ìˆ˜": [1-5 ì¤‘ í•´ë‹¹ ì ìˆ˜],
    "í‰ê°€ì‚¬ìœ ": "[í•™ìŠµ ëª©í‘œì™€ ìˆ˜ì¤€ ì í•©ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„]"
  },
  "ë‚œì´ë„ ì ì •ì„±": {
    "ì ìˆ˜": [1-5 ì¤‘ í•´ë‹¹ ì ìˆ˜],
    "í‰ê°€ì‚¬ìœ ": "[ë‚œì´ë„ì™€ ë ˆë²¨ ì¼ì¹˜ì„±ì— ëŒ€í•œ êµ¬ì²´ì  í‰ê°€]"
  },
  "ëª…í™•ì„± ë° ëª¨í˜¸ì„± ì—†ìŒ": {
    "ì ìˆ˜": [1-5 ì¤‘ í•´ë‹¹ ì ìˆ˜],
    "í‰ê°€ì‚¬ìœ ": "[ë¬¸ì œì™€ ì„ íƒì§€ì˜ ëª…í™•ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„]"
  },
  "ì •ë‹µ ë° í•´ì„¤ì˜ ì •í™•ì„±": {
    "ì ìˆ˜": [1-5 ì¤‘ í•´ë‹¹ ì ìˆ˜],
    "í‰ê°€ì‚¬ìœ ": "[ì •ë‹µê³¼ í•´ì„¤ì˜ ì •í™•ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ê²€ì¦]"
  },
  "í•™ìŠµì  ê°€ì¹˜ ë° ìœ ìš©ì„±": {
    "ì ìˆ˜": [1-5 ì¤‘ í•´ë‹¹ ì ìˆ˜],
    "í‰ê°€ì‚¬ìœ ": "[êµìœ¡ì  íš¨ê³¼ì™€ ì‹¤ë¬´ í™œìš©ë„ì— ëŒ€í•œ êµ¬ì²´ì  í‰ê°€]"
  },
  "Quality Score": [5ê°œ ê¸°ì¤€ ì ìˆ˜ì˜ í•©ê³„],
  "Quality Score ìµœëŒ€": 25,
  "í’ˆì§ˆë“±ê¸‰": "[Excellent/Good/Fair/Poor ì¤‘ í•´ë‹¹ ë“±ê¸‰]",
  "ì¢…í•©í‰ê°€": "[ì „ì²´ì ì¸ í‰ê°€ ìš”ì•½ê³¼ ê°œì„  ë°©í–¥ ì œì‹œ]"
}

**ì¤‘ìš”**: ê° ë¬¸ì œì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì–‘í•œ ì ìˆ˜ ë¶„í¬ë¡œ ì°¨ë³„í™”ëœ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.
"""

        self.fidelity_prompt = """
ì œì‹œëœ ë¬¸ì œê°€ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆëŠ”ì§€ "Attributed Question Answering" ë…¼ë¬¸ ê¸°ì¤€ì— ë”°ë¼ ê²€ì¦í•´ ì£¼ì„¸ìš”.

[ê²€ì¦ ì›ì¹™]
1. **êµ¬ì²´ì  ë¹„êµ**: ë¬¸ì œì˜ ëª¨ë“  ìš”ì†Œë¥¼ ì›ë³¸ ë¬¸ì„œì˜ êµ¬ì²´ì  ë¶€ë¶„ê³¼ ì§ì ‘ ë¹„êµ
2. **ìƒì„¸í•œ ê·¼ê±°**: ì ìˆ˜ íŒë‹¨ì˜ ê·¼ê±°ë¥¼ ì›ë³¸ ë¬¸ì„œì˜ êµ¬ì²´ì  ë‚´ìš© ì¸ìš©ìœ¼ë¡œ ì œì‹œ
3. **ì°¨ì´ì  ëª…ì‹œ**: ì›ë³¸ê³¼ ë‹¤ë¥¸ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì •í™•íˆ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì 
4. **ì´ë¯¸ì§€ ì •ë³´**: ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì œëŠ” ì´ë¯¸ì§€ ì„¤ëª…ê³¼ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ì„¸ë°€í•˜ê²Œ ê²€ì¦
5. **êµìœ¡ì  ê´€ì **: ì›ë³¸ ì¶©ì‹¤ë„ì™€ ë”ë¶ˆì–´ êµìœ¡ì  íš¨ê³¼ë„ í•¨ê»˜ ê³ ë ¤

[ê²€ì¦ ê¸°ì¤€]

ğŸŸ¢ 2ì  (ì™„ì „í•œ ê·¼ê±°)
âœ… ë‹µë³€ ë‚´ìš©ì´ ì¶œì²˜ì— ì§ì ‘ì ìœ¼ë¡œ ì„œìˆ ë¨
âœ… ë¬¸ì¥ì˜ í‘œí˜„ë§Œ ë‹¤ë¥´ê³  ì‚¬ì‹¤ê´€ê³„/ì •ë³´ê°€ ì™„ì „íˆ ì¼ì¹˜
âœ… ì¶œì²˜ë§Œ ë³´ê³ ë„ ê·¸ ë‹µì„ ì¬êµ¬ì„±í•  ìˆ˜ ìˆìŒ
âœ… ë…¼ë¦¬ì  ì¶”ë¡ ì´ ê±°ì˜ í•„ìš” ì—†ìŒ

ì˜ˆì‹œ
ì¶œì²˜: "ê³µë£¡ì€ ì•½ 2ì–µ 3ì²œë§Œ ë…„ ì „ì— ì²˜ìŒ ì¶œí˜„í–ˆë‹¤."
ì§ˆë¬¸: "ê³µë£¡ì€ ì–¸ì œ ë“±ì¥í–ˆë‚˜ìš”?"
ë‹µë³€: "ê³µë£¡ì€ ì•½ 2ì–µ 3ì²œë§Œ ë…„ ì „ì— ë“±ì¥í–ˆìŠµë‹ˆë‹¤."
â†’ 2ì  (ì •ë³´ ê·¸ëŒ€ë¡œ)

ğŸŸ¡ 1ì  (ë¶€ë¶„ì  ê·¼ê±°)
âœ… ì¶œì²˜ì— ë¶€ë¶„ ì •ë³´ ë˜ëŠ” ë‹¨ì„œê°€ ì¡´ì¬í•¨
âœ… ë‹µë³€ì´ ì¶œì²˜ì— ìˆëŠ” ì •ë³´ì— ì¶”ë¡ Â·í•´ì„Â·í™•ì¥ì„ ë”í•´ ì‘ì„±ë¨
âœ… ì¶œì²˜ì— "íŒíŠ¸"ëŠ” ìˆì§€ë§Œ, ë‹µë³€ì„ 100% ì¬êµ¬ì„±í•˜ê¸°ëŠ” ì–´ë ¤ì›€
âœ… ì •ë³´ì˜ ì¼ë¶€ëŠ” ì¶œì²˜ì— ì—†ê±°ë‚˜, ì¶œì²˜ì— ëª…í™•íˆ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

ì˜ˆì‹œ
ì¶œì²˜: "ê³µë£¡ì€ ì¤‘ìƒëŒ€ íŠ¸ë¼ì´ì•„ìŠ¤ê¸°ì— ì¶œí˜„í–ˆë‹¤."
ì§ˆë¬¸: "ê³µë£¡ì€ ì–¸ì œ ë“±ì¥í–ˆë‚˜ìš”?"
ë‹µë³€: "ê³µë£¡ì€ ì•½ 2ì–µ 3ì²œë§Œ ë…„ ì „ì— ë“±ì¥í–ˆìŠµë‹ˆë‹¤."
â†’ 1ì  (íŠ¸ë¼ì´ì•„ìŠ¤ê¸° = ì•½ 2ì–µ 3ì²œë§Œ ë…„ ì „ì´ë¼ëŠ” ì¶”ë¡ ì´ í•„ìš”)

ë˜ëŠ”:
ì¶œì²˜: "ê³µë£¡ì€ ìœ¡ì‹ê³¼ ì´ˆì‹ì´ ëª¨ë‘ ì¡´ì¬í–ˆë‹¤."
ì§ˆë¬¸: "ê³µë£¡ì€ ë¬´ì—‡ì„ ë¨¹ì—ˆë‚˜ìš”?"
ë‹µë³€: "ê³µë£¡ì€ ìœ¡ì‹ì„± ì¢…ë¥˜ë„ ìˆì—ˆê³  ì´ˆì‹ì„± ì¢…ë¥˜ë„ ìˆì—ˆìŠµë‹ˆë‹¤."
â†’ 1ì  (ì¶œì²˜ì— ìš”ì•½ ë‹¨ì„œ ìˆì§€ë§Œ ì„¸ë¶€ ì •ë³´ëŠ” ì¼ë¶€ í™•ì¥ë¨)

ğŸ”´ 0ì  (ê·¼ê±° ì—†ìŒ)
âœ… ì¶œì²˜ì— ê´€ë ¨ ì •ë³´ê°€ ì „í˜€ ì—†ìŒ
âœ… ì¶œì²˜ ë‚´ìš©ê³¼ ë‹µë³€ì´ ëª¨ìˆœë˜ê±°ë‚˜ ì¶©ëŒ
âœ… ë‹µë³€ì´ ì „ì ìœ¼ë¡œ ì™¸ë¶€ ì •ë³´ì— ê¸°ë°˜í•˜ê±°ë‚˜ ì¶”ì¸¡ì ì„

ì˜ˆì‹œ
ì¶œì²˜: "ê³µë£¡ì€ ì•½ 2ì–µ ë…„ ì „ì— ì¶œí˜„í–ˆë‹¤."
ì§ˆë¬¸: "ê³µë£¡ì€ ì–´ë””ì—ì„œ ë“±ì¥í–ˆë‚˜ìš”?"
ë‹µë³€: "ê³µë£¡ì€ ë¶ì•„ë©”ë¦¬ì¹´ì—ì„œ ì²˜ìŒ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤."
â†’ 0ì  (ì¶œì²˜ì— ì¥ì†Œ ì •ë³´ ì—†ìŒ)

ë˜ëŠ”:
ì¶œì²˜: "ê³µë£¡ì€ ì´ˆì‹ì„±ì´ì—ˆë‹¤."
ì§ˆë¬¸: "ê³µë£¡ì€ ë¬´ì—‡ì„ ë¨¹ì—ˆë‚˜ìš”?"
ë‹µë³€: "ê³µë£¡ì€ ìœ¡ì‹ì„±ì´ì—ˆë‹¤."
â†’ 0ì  (ì¶œì²˜ì™€ ëª¨ìˆœ)

ğŸ“ íŒë‹¨ í”„ë¡œì„¸ìŠ¤ ê°€ì´ë“œ
íŒë‹¨í•  ë•Œ ì•„ë˜ ë‹¨ê³„ë¡œ ê²€í† í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤:

1ï¸âƒ£ ì¶œì²˜ì— ë‹µë³€ ì •ë³´ê°€ ì§ì ‘ì ìœ¼ë¡œ ì„œìˆ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
2ï¸âƒ£ ë§Œì•½ ì§ì ‘ ì„œìˆ ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì¶œì²˜ì˜ ì •ë³´ì—ì„œ í•©ë¦¬ì  ì¶”ë¡ ì´ ê°€ëŠ¥í•œì§€ í‰ê°€
3ï¸âƒ£ ì •ë³´ì˜ ì¼ë¶€ë§Œ ì¶œì²˜ì— ìˆê±°ë‚˜, ë‹¤ì†Œ ëª¨í˜¸í•˜ë©´ ë¶€ë¶„ì  ê·¼ê±°
4ï¸âƒ£ ì¶œì²˜ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ê±°ë‚˜ ëª¨ìˆœë˜ë©´ ê·¼ê±° ì—†ìŒ


**ì¤‘ìš”**: ê° ë¬¸ì œë³„ë¡œ ì›ë³¸ ë¬¸ì„œì™€ì˜ ì‹¤ì œ ë¹„êµë¥¼ í†µí•´ ì°¨ë³„í™”ëœ ê²€ì¦ ê²°ê³¼ë¥¼ ì œì‹œí•˜ì„¸ìš”.

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:
{
  "ë¬¸ì„œ_ì¶©ì‹¤ë„": {
    "ì ìˆ˜": [0, 1, 2 ì¤‘ í•´ë‹¹ ì ìˆ˜],
    "ê²€ì¦ì‚¬ìœ ": "[ì›ë³¸ ë¬¸ì„œì™€ì˜ ë¹„êµ ë¶„ì„ ë° íŒë‹¨ ê·¼ê±°]",
    "ë¬¸ì œì ": "[ë°œê²¬ëœ ë¬¸ì œì ì´ë‚˜ ê°œì„ ì‚¬í•­]"
  },
  "ì¢…í•©ê²€ì¦": "[ì „ì²´ì ì¸ ì¶©ì‹¤ë„ í‰ê°€ ìš”ì•½ ë° ê°œì„  ë°©í–¥]"
}
"""

    # ========================================================================
    # 1. ë¬¸ì œ ë¡œë”© ê´€ë ¨ ë©”ì„œë“œë“¤
    # ========================================================================
    
    def load_questions_from_files(self, directory_path: str, max_questions: int = None) -> List[Dict]:
        """generated_questions ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì œë“¤ì„ ë¡œë“œ"""
        questions = []
        
        # basic_questions íŒŒì¼ë“¤ ë¡œë“œ
        basic_files = glob.glob(os.path.join(directory_path, "basic_questions_*.json"))
        for file_path in basic_files:
            questions.extend(self._load_questions_from_file(file_path, 'basic', max_questions))
        
        # extra_questions íŒŒì¼ë“¤ ë¡œë“œ  
        extra_files = glob.glob(os.path.join(directory_path, "extra_questions_*.json"))
        for file_path in extra_files:
            questions.extend(self._load_questions_from_file(file_path, 'extra', max_questions))
            
        return questions[:max_questions] if max_questions else questions
    
    def load_questions_from_file(self, file_path: str) -> List[Dict]:
        """ë‹¨ì¼ íŒŒì¼ì—ì„œ ë¬¸ì œë“¤ì„ ë¡œë“œ"""
        file_type = 'basic' if 'basic_questions' in file_path else 'extra'
        return self._load_questions_from_file(file_path, file_type)
    
    def load_sample_questions(self, directory_path: str, max_questions: int = 10) -> List[Dict]:
        """ìµœì‹  íŒŒì¼ì—ì„œ ìƒ˜í”Œ ë¬¸ì œë“¤ë§Œ ë¡œë“œ"""
        questions = []
        
        # ìµœì‹  basic_questions íŒŒì¼ 1ê°œë§Œ ë¡œë“œ
        basic_files = sorted(glob.glob(os.path.join(directory_path, "basic_questions_*.json")), reverse=True)
        if basic_files:
            file_path = basic_files[0]
            questions = self._load_questions_from_file(file_path, 'basic', max_questions)
                
        return questions
    
    def _load_questions_from_file(self, file_path: str, file_type: str, max_questions: int = None) -> List[Dict]:
        """ë‹¨ì¼ íŒŒì¼ì—ì„œ ë¬¸ì œë“¤ ë¡œë“œ"""
        questions = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                count = 0
                # ìƒˆë¡œìš´ êµ¬ì¡°: questions_by_document
                if isinstance(data, dict) and 'questions_by_document' in data:
                    for doc_name, doc_questions in data['questions_by_document'].items():
                        for q in doc_questions:
                            if max_questions and count >= max_questions:
                                break
                            q['file_type'] = file_type
                            q['source_file'] = os.path.basename(file_path)
                            q['document_name'] = doc_name
                            questions.append(q)
                            count += 1
                        if max_questions and count >= max_questions:
                            break
                # ê¸°ì¡´ êµ¬ì¡°: questions ë°°ì—´
                elif isinstance(data, dict) and 'questions' in data:
                    for q in data['questions']:
                        if max_questions and count >= max_questions:
                            break
                        q['file_type'] = file_type
                        q['source_file'] = os.path.basename(file_path)
                        questions.append(q)
                        count += 1
                # ì§ì ‘ ë°°ì—´
                elif isinstance(data, list):
                    for q in data:
                        if max_questions and count >= max_questions:
                            break
                        q['file_type'] = file_type
                        q['source_file'] = os.path.basename(file_path)
                        questions.append(q)
                        count += 1
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
        
        return questions


    # ========================================================================
    # 2. ë¬¸ì œ í’ˆì§ˆ í‰ê°€ ê´€ë ¨ ë©”ì„œë“œë“¤ (LLM ê¸°ë°˜)
    # ========================================================================
    
    @traceable(
        run_type="chain",
        name="Evaluate Question Quality",
        metadata={"agent_type": "question_checker"}
    )
    def evaluate_question_quality_llm(self, question: Dict) -> Dict:
        """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œ í’ˆì§ˆ í‰ê°€ (gpt-3.5-turbo, openai>=1.0.0)"""
        try:
            import openai
            from openai import OpenAI
            from langsmith.wrappers import wrap_openai
            from dotenv import load_dotenv
            load_dotenv(override=True)
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                return self._get_default_quality_evaluation()
            client = wrap_openai(OpenAI(api_key=openai_api_key))
            question_text = self._format_question_for_evaluation(question)
            prompt = f"{self.quality_prompt}\n\ní‰ê°€í•  ë¬¸ì œ:\n{question_text}"
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì œ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            response_text = response.choices[0].message.content.strip()
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```", 1)[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```", 1)[1].split("```", 1)[0].strip()
            evaluation = json.loads(response_text)
            return evaluation
        except Exception as e:
            print(f"âš ï¸ OpenAI í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return self._get_default_quality_evaluation()

    # ========================================================================
    # 3. ë¬¸ì„œ ì¶©ì‹¤ë„ ê²€ì¦ ê´€ë ¨ ë©”ì„œë“œë“¤
    # ========================================================================
    
    def _is_image_based_question(self, question: Dict) -> bool:
        """ë¬¸ì œê°€ ì´ë¯¸ì§€ ê¸°ë°˜ì¸ì§€ í™•ì¸"""
        question_text = question.get('question', '').lower()
        explanation = question.get('explanation', '').lower()
        
        # ì´ë¯¸ì§€ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        image_keywords = [
            'ê·¸ë¦¼', 'ë„í‘œ', 'ì°¨íŠ¸', 'ë‹¤ì´ì–´ê·¸ë¨', 'í”Œë¡œìš°ì°¨íŠ¸', 'ìˆœì„œë„', 'êµ¬ì¡°ë„',
            'ì´ë¯¸ì§€', 'ì‚¬ì§„', 'ê·¸ë˜í”„', 'í‘œ', 'ë„ë©´', 'ìŠ¤í¬ë¦°ìƒ·', 'í™”ë©´',
            'ìœ„ ê·¸ë¦¼', 'ì•„ë˜ ê·¸ë¦¼', 'ë‹¤ìŒ ê·¸ë¦¼', 'ìœ„ ë„í‘œ', 'ì•„ë˜ ë„í‘œ'
        ]
        
        text_to_check = f"{question_text} {explanation}"
        return any(keyword in text_to_check for keyword in image_keywords)
    
    @traceable(
        run_type="chain",
        name="Evaluate Document Fidelity",
        metadata={"agent_type": "question_checker"}
    )
    def evaluate_document_fidelity_llm(self, question: Dict, source_documents: Dict[str, Dict]) -> Dict:
        """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¶©ì‹¤ë„ ê²€ì¦ (VectorDB ë˜ëŠ” íŒŒì¼ ê¸°ë°˜)"""
        try:
            import openai
            from openai import OpenAI
            from langsmith.wrappers import wrap_openai
            from dotenv import load_dotenv
            load_dotenv(override=True)
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                return self._get_default_fidelity_evaluation()
            client = wrap_openai(OpenAI(api_key=openai_api_key))
            
            # VectorDB ê²€ìƒ‰ë§Œ ì‚¬ìš©
            document_content = ""
            if self.use_vector_db and self.vector_searcher:
                try:
                    question_text = question.get('question', '')
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ ì°¾ê¸°
                    collections = self.vector_searcher.client.get_client().list_collections()
                    if collections:
                        collection_name = collections[0].name
                        search_results = self.vector_searcher.search_similar(
                            query=question_text,
                            collection_name=collection_name,
                            n_results=3
                        )
                        if search_results and len(search_results) > 0:
                            document_content = ' '.join([result.get('content', '') for result in search_results])
                            print(f"ğŸ“ VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                        else:
                            print("âš ï¸ VectorDB ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    else:
                        print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ì´ ì—†ìŒ")
                except Exception as e:
                    print(f"âš ï¸ VectorDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            else:
                print("âš ï¸ VectorDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
                
            if not document_content:
                print("âš ï¸ VectorDBì—ì„œ ë¹„êµí•  ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return self._get_default_fidelity_evaluation()
                
            question_text = self._format_question_for_evaluation(question)
            
            # ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì œì¸ì§€ í™•ì¸
            is_image_based = self._is_image_based_question(question)
            image_note = ""
            if is_image_based:
                image_note = "\n\n**ì£¼ì˜: ì´ ë¬¸ì œëŠ” ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì œì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ì •ë³´ì™€ ë¬¸ì œ ë‚´ìš©ì˜ ì¼ì¹˜ì„±ì„ íŠ¹ë³„íˆ ê²€ì¦í•´ì£¼ì„¸ìš”.**"
            
            prompt = f"{self.fidelity_prompt}\n\nì›ë³¸ ë¬¸ì„œ ë‚´ìš©:\n{document_content[:2000]}...\n\nê²€ì¦í•  ë¬¸ì œ:\n{question_text}{image_note}"
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ì¶©ì‹¤ë„ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )
            response_text = response.choices[0].message.content.strip()
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```", 1)[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```", 1)[1].split("```", 1)[0].strip()
            evaluation = json.loads(response_text)
            return evaluation
        except Exception as e:
            print(f"âš ï¸ OpenAI ì¶©ì‹¤ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return self._get_default_fidelity_evaluation()


    # ========================================================================
    # 4. í†µí•© ë³´ê³ ì„œ ìƒì„± ë©”ì„œë“œë“¤
    # ========================================================================
    
    def generate_comprehensive_report(self, questions: List[Dict], check_fidelity: bool = True, 
                                    output_file: str = None) -> Dict:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print(f"ğŸ” {len(questions)}ê°œ ë¬¸ì œ ì¢…í•© ê²€ì¦ ì‹œì‘...")
        
        # VectorDBë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ë³„ë„ ë¬¸ì„œ ë¡œë”© ë¶ˆí•„ìš”
        
        evaluations = []
        quality_totals = {criterion: 0 for criterion in self.quality_criteria.keys()}
        fidelity_totals = {criterion: 0 for criterion in self.fidelity_criteria.keys()}
        
        for i, question in enumerate(questions, 1):
            print(f"  ğŸ“ ë¬¸ì œ {i}/{len(questions)} ê²€ì¦ ì¤‘...")
            
            # 1. í’ˆì§ˆ í‰ê°€ (LLM ê¸°ë°˜)
            quality_eval = self.evaluate_question_quality_llm(question)
            
            # 2. ì¶©ì‹¤ë„ ê²€ì¦ (VectorDB ê¸°ë°˜)
            fidelity_eval = {}
            if check_fidelity:
                fidelity_eval = self.evaluate_document_fidelity_llm(question, {})
            
            # ê²°ê³¼ í†µí•©
            evaluation = {
                'question_id': i,
                'question_info': {
                    'type': question.get('type'),
                    'difficulty_level': question.get('difficulty_level'),
                    'tags': question.get('tags', []),
                    'source_file': question.get('source_file'),
                    'document_name': question.get('document_name'),
                    'question': question.get('question', '')[:100] + '...' if len(question.get('question', '')) > 100 else question.get('question', '')
                },
                'quality_evaluation': quality_eval,
                'fidelity_evaluation': fidelity_eval if check_fidelity else {}
            }
            
            evaluations.append(evaluation)
            
            # ì ìˆ˜ ì§‘ê³„
            for criterion in self.quality_criteria.keys():
                if criterion in quality_eval and 'ì ìˆ˜' in quality_eval[criterion]:
                    quality_totals[criterion] += quality_eval[criterion]['ì ìˆ˜']
            
            if check_fidelity:
                for criterion in self.fidelity_criteria.keys():
                    if criterion in fidelity_eval and 'ì ìˆ˜' in fidelity_eval[criterion]:
                        fidelity_totals[criterion] += fidelity_eval[criterion]['ì ìˆ˜']
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        quality_averages = {criterion: round(score / len(questions), 2) for criterion, score in quality_totals.items()}
        fidelity_averages = {criterion: round(score / len(questions), 2) for criterion, score in fidelity_totals.items()} if check_fidelity else {}
        
        # Quality Score ê³„ì‚° (5ì  ì²™ë„ ê¸°ì¤€)
        total_quality_score = 0
        quality_score_count = 0
        for evaluation in evaluations:
            quality_eval = evaluation['quality_evaluation']
            if 'Quality Score' in quality_eval:
                total_quality_score += quality_eval['Quality Score']
                quality_score_count += 1
        
        average_quality_score = round(total_quality_score / quality_score_count, 2) if quality_score_count > 0 else 0
        
        # ë³´ê³ ì„œ ìƒì„±
        report = {
            "ê²€ì¦_ìš”ì•½": {
                "ì´_ë¬¸ì œìˆ˜": len(questions),
                "ê²€ì¦_ì¼ì‹œ": datetime.now().isoformat(),
                "ê²€ì¦_ë°©ì‹": "5ì _ì²™ë„_í’ˆì§ˆí‰ê°€",
                "í’ˆì§ˆ_í‰ê°€_í‰ê· ": quality_averages,
                "Quality_Score_í‰ê· ": average_quality_score,
                "Quality_Score_ìµœëŒ€": 25,
                "í’ˆì§ˆë“±ê¸‰": "Excellent" if average_quality_score >= 22 else "Good" if average_quality_score >= 18 else "Fair" if average_quality_score >= 14 else "Poor",
                "ì¶©ì‹¤ë„_ê²€ì¦_í‰ê· ": fidelity_averages,
                "ì¶©ì‹¤ë„_ì „ì²´_í‰ê· ": round(sum(fidelity_averages.values()) / len(fidelity_averages), 2) if fidelity_averages else 0,
                "ì¶©ì‹¤ë„_ìµœëŒ€ì ìˆ˜": 2
            },
            "ê²€ì¦_ê¸°ì¤€": {
                "í’ˆì§ˆ_í‰ê°€_ê¸°ì¤€": self.quality_criteria,
                "ì¶©ì‹¤ë„_ê²€ì¦_ê¸°ì¤€": self.fidelity_criteria if check_fidelity else {}
            },
            "ë¬¸ì œë³„_ê²€ì¦ê²°ê³¼": evaluations
        }
        
        # íŒŒì¼ ì €ì¥
        if not output_file:
            output_file = f"output/comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… 5ì  ì²™ë„ ê¸°ë°˜ ì¢…í•© ê²€ì¦ ì™„ë£Œ! ë³´ê³ ì„œ ì €ì¥: {output_file}")
        print(f"ğŸ“Š Quality Score í‰ê· : {average_quality_score}/25.0 ({report['ê²€ì¦_ìš”ì•½']['í’ˆì§ˆë“±ê¸‰']})")
        if check_fidelity:
            print(f"ğŸ“‹ ì¶©ì‹¤ë„ ê²€ì¦ í‰ê· : {report['ê²€ì¦_ìš”ì•½']['ì¶©ì‹¤ë„_ì „ì²´_í‰ê· ']}/2.0")
        
        return report

    # ========================================================================
    # 5. ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ========================================================================
    
    def _format_question_for_evaluation(self, question: Dict) -> str:
        """ë¬¸ì œë¥¼ í‰ê°€ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        formatted = f"=== ë¬¸ì œ ===\n{question.get('question', '')}\n"
        
        if question.get('type') == 'OBJECTIVE':
            formatted += f"\n=== ì„ íƒì§€ ===\n"
            for i, option in enumerate(question.get('options', []), 1):
                formatted += f"{i}. {option}\n"
            formatted += f"\nì •ë‹µ: {question.get('answer', '')}"
        else:
            formatted += f"\n=== ì˜ˆì‹œ ë‹µì•ˆ ===\n{question.get('answer', '')}"
            
        if question.get('explanation'):
            formatted += f"\n\n=== í•´ì„¤ ===\n{question.get('explanation', '')}"
            
        return formatted


    def _get_default_quality_evaluation(self) -> Dict:
        """ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ (LLM ì‚¬ìš© ë¶ˆê°€ ì‹œ) - 5ì  ì²™ë„ ê¸°ì¤€"""
        return {
            "ì ì ˆì„±": {"ì ìˆ˜": 3, "í‰ê°€ì‚¬ìœ ": "ìë™ í‰ê°€ ë¶ˆê°€ - í•™ìŠµ ëª©í‘œ ì í•©ì„± ì¶”ì •"},
            "ë‚œì´ë„ ì ì •ì„±": {"ì ìˆ˜": 3, "í‰ê°€ì‚¬ìœ ": "ìë™ í‰ê°€ ë¶ˆê°€ - ë‚œì´ë„ ë ˆë²¨ ì ì •ì„± ì¶”ì •"},
            "ëª…í™•ì„± ë° ëª¨í˜¸ì„± ì—†ìŒ": {"ì ìˆ˜": 3, "í‰ê°€ì‚¬ìœ ": "ìë™ í‰ê°€ ë¶ˆê°€ - ëª…í™•ì„± ì¶”ì •"},
            "ì •ë‹µ ë° í•´ì„¤ì˜ ì •í™•ì„±": {"ì ìˆ˜": 3, "í‰ê°€ì‚¬ìœ ": "ìë™ í‰ê°€ ë¶ˆê°€ - ì •í™•ì„± ì¶”ì •"},
            "í•™ìŠµì  ê°€ì¹˜ ë° ìœ ìš©ì„±": {"ì ìˆ˜": 3, "í‰ê°€ì‚¬ìœ ": "ìë™ í‰ê°€ ë¶ˆê°€ - êµìœ¡ì  ê°€ì¹˜ ì¶”ì •"},
            "Quality Score": 15,
            "Quality Score ìµœëŒ€": 25,
            "í’ˆì§ˆë“±ê¸‰": "Fair",
            "ì¢…í•©í‰ê°€": "ìë™ í‰ê°€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìˆ˜ë™ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤. 15/25ì ìœ¼ë¡œ ìˆ˜ì • í›„ ì‚¬ìš© ê¶Œì¥."
        }

    def _get_default_fidelity_evaluation(self) -> Dict:
        """ê¸°ë³¸ ì¶©ì‹¤ë„ í‰ê°€ (LLM ì‚¬ìš© ë¶ˆê°€ ì‹œ) - Attributed QA ê¸°ì¤€"""
        return {
            "ë¬¸ì„œ_ì¶©ì‹¤ë„": {"ì ìˆ˜": 1, "ê²€ì¦ì‚¬ìœ ": "ìë™ ê²€ì¦ ë¶ˆê°€ - ì›ë³¸ ë¬¸ì„œì™€ì˜ ì§ì ‘ ë¹„êµê°€ í•„ìš”í•¨", "ë¬¸ì œì ": "ìˆ˜ë™ í™•ì¸ í•„ìš”"},
            "ì¢…í•©ê²€ì¦": "ìë™ ê²€ì¦ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìˆ˜ë™ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œì™€ì˜ ì§ì ‘ ë¹„êµë¥¼ í†µí•´ ì¶©ì‹¤ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”."
        }

    def print_statistics(self, questions: List[Dict]):
        """ë¬¸ì œ í†µê³„ ì¶œë ¥"""
        type_counts = {}
        difficulty_counts = {}
        tag_counts = {}
        
        for q in questions:
            q_type = q.get('type', 'UNKNOWN')
            difficulty = q.get('difficulty_level', 'UNKNOWN')
            tags = q.get('tags', [])
            
            type_counts[q_type] = type_counts.get(q_type, 0) + 1
            difficulty_counts[difficulty] = difficulty_counts.get(difficulty, 0) + 1
            
            for tag in tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        print(f"ğŸ“Š ë¬¸ì œ ìœ í˜•ë³„: {type_counts}")
        print(f"ğŸ“Š ë‚œì´ë„ë³„: {difficulty_counts}")
        print(f"ğŸ“Š íƒœê·¸ë³„: {dict(sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:5])}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ í†µí•© ë¬¸ì œ ê²€ì¦ ë„êµ¬ (LLM ê¸°ë°˜)")
    print("=" * 80)
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        # íŠ¹ì • íŒŒì¼ì´ ì§€ì •ëœ ê²½ìš°
        questions_file = sys.argv[1]
        if not os.path.exists(questions_file):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {questions_file}")
            return
        
        # UnifiedQuestionChecker ì´ˆê¸°í™”
        checker = UnifiedQuestionChecker()
        
        # íŒŒì¼ì—ì„œ ë¬¸ì œ ë¡œë“œ
        questions = checker.load_questions_from_file(questions_file)
        if questions:
            checker.print_statistics(questions)
            report = checker.generate_comprehensive_report(
                questions, check_fidelity=True,
                output_file=f"output/llm_file_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
        else:
            print("âŒ í‰ê°€í•  ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²½ë¡œ ì„¤ì •
    questions_dir = "../../../data/outputs/generated_questions"
    
    # UnifiedQuestionChecker ì´ˆê¸°í™”
    checker = UnifiedQuestionChecker()
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\nê²€ì¦ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ìƒ˜í”Œ ê²€ì¦ (10ê°œ, ë¹ ë¦„)")
    print("2. ì „ì²´ ê²€ì¦ (ëª¨ë“  ë¬¸ì œ, ëŠë¦¼)")
    print("3. ì¶©ì‹¤ë„ ê²€ì¦ ì—†ì´ í’ˆì§ˆë§Œ í‰ê°€")
    
    choice = input("ì„ íƒ (1-3): ").strip()
    
    if choice == "1":
        # ìƒ˜í”Œ ê²€ì¦ (LLM ì‚¬ìš©)
        questions = checker.load_sample_questions(questions_dir, 10)
        if questions:
            checker.print_statistics(questions)
            report = checker.generate_comprehensive_report(
                questions, check_fidelity=True,
                output_file=f"output/llm_sample_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
        else:
            print("âŒ í‰ê°€í•  ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    elif choice == "2":
        # ì „ì²´ ê²€ì¦ (LLM ì‚¬ìš©)
        questions = checker.load_questions_from_files(questions_dir)
        if questions:
            checker.print_statistics(questions)
            report = checker.generate_comprehensive_report(
                questions, check_fidelity=True,
                output_file=f"output/llm_full_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
        else:
            print("âŒ í‰ê°€í•  ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    elif choice == "3":
        # í’ˆì§ˆ í‰ê°€ë§Œ (ì¶©ì‹¤ë„ ê²€ì¦ ì—†ìŒ)
        questions = checker.load_sample_questions(questions_dir, 10)
        if questions:
            checker.print_statistics(questions)
            report = checker.generate_comprehensive_report(
                questions, check_fidelity=False,
                output_file=f"output/llm_quality_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
        else:
            print("âŒ í‰ê°€í•  ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()