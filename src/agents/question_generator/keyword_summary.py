"""
ê° ë¬¸ì„œì— ëŒ€í•´ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ì„ ìˆ˜í–‰í•˜ê³  JSON í˜•íƒœë¡œ ì¶œë ¥í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
Doclingìœ¼ë¡œ íŒŒì‹±ëœ ë¸”ë¡ë“¤ì„ ë¶„ì„í•˜ì—¬ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import json
import os
from typing import List, Dict
from openai import OpenAI
from dotenv import load_dotenv
from src.agents.question_generator.change_name import normalize_collection_name

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)
api_key = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=api_key)


def extract_keywords_and_summary(blocks: List[Dict], source_file: str) -> Dict:
    """
    Docling ë¸”ë¡ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        blocks: Docling íŒŒì„œì—ì„œ ìƒì„±ëœ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
        source_file: ì›ë³¸ íŒŒì¼ëª…
    
    Returns:
        Dict: í‚¤ì›Œë“œ, ìš”ì•½, ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    # í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ì—ì„œ ë‚´ìš© ì¶”ì¶œ
    text_content = []
    total_pages = set()
    sections = []
    
    for block in blocks:
        block_type = block.get("type", "")
        page_no = block.get("metadata", {}).get("page")
        
        if page_no:
            total_pages.add(page_no)
        
        if block_type in ["paragraph", "heading"]:  # heading íƒ€ì… ì¶”ê°€
            content = block.get("content", "").strip()
            if content:
                text_content.append(content)
        elif block_type == "section":
            title = block.get("title", "").strip()
            if title:
                sections.append(title)
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
    combined_text = "\n".join(text_content)
    
    # LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½
    llm_result = _extract_keywords_summary_with_llm(combined_text, source_file)
    
    
    # ê²°ê³¼ êµ¬ì„±
    result = {
        "document_info": {
            "filename": source_file
        },
        "content_analysis": {
            "summary": llm_result.get("summary", ""),
            "main_topics": llm_result.get("main_topics", []),
            "key_concepts": llm_result.get("key_concepts", []),
            "technical_terms": llm_result.get("technical_terms", [])
        }
    }
    
    return result


def _extract_keywords_summary_with_llm(text: str, filename: str) -> Dict:
    """
    GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ ìˆ˜í–‰
    """
    # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ë° ì œí•œ
    if not text or len(text.strip()) < 50:
        print("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {
            "summary": "í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ì—¬ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "main_topics": [],
            "key_concepts": [],
            "technical_terms": []
        }
    
    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í•  ì²˜ë¦¬ (GPT-4 í† í° ì œí•œ ê³ ë ¤)
    max_length = 4000  # ë” ì•ˆì „í•œ ê¸¸ì´ë¡œ ì¶•ì†Œ
    if len(text) > max_length:
        # ì•ë¶€ë¶„ê³¼ ë’·ë¶€ë¶„ ì¼ë¶€ ì‚¬ìš©í•˜ì—¬ ëŒ€í‘œì„± í™•ë³´
        front_part = text[:max_length//2]
        back_part = text[-(max_length//2):]
        text = front_part + "\n\n[ì¤‘ê°„ ë‚´ìš© ìƒëµ...]\n\n" + back_part
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •: ì›ë³¸ {len(text)}ì â†’ ì••ì¶• {len(text)}ì")
    
    prompt = f"""
ë¬¸ì„œ "{filename}"ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

1. summary: ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
2. main_topics: ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œ/í† í”½ (ìµœëŒ€ 5ê°œ)
3. key_concepts: í•µì‹¬ ê°œë…ì´ë‚˜ ìš©ì–´ (ìµœëŒ€ 10ê°œ)
4. technical_terms: ì „ë¬¸ ìš©ì–´ë‚˜ ê¸°ìˆ  ìš©ì–´ (ìµœëŒ€ 8ê°œ)

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”:

{{
    "summary": "ë¬¸ì„œ ìš”ì•½ ë‚´ìš©",
    "main_topics": ["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3"],
    "key_concepts": ["ê°œë…1", "ê°œë…2", "ê°œë…3"],
    "technical_terms": ["ìš©ì–´1", "ìš©ì–´2", "ìš©ì–´3"]
}}

ë¶„ì„í•  ë¬¸ì„œ ë‚´ìš©:
{text}
"""

    try:
        print(f"ğŸ¤– GPT-4 ë¶„ì„ ì‹œì‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì)")
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1000,  # í† í° ìˆ˜ ì¤„ì„
            timeout=30  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        print("âœ… GPT-4 ë¶„ì„ ì™„ë£Œ")
        
        raw_content = response.choices[0].message.content.strip()
        
        # JSON íŒŒì‹±
        try:
            print(f"ğŸ“„ ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {raw_content[:100]}...")
            
            # ì½”ë“œ ë¸”ë¡ ì œê±°
            if "```json" in raw_content:
                raw_content = raw_content.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_content:
                raw_content = raw_content.split("```")[1].split("```")[0].strip()
            
            result = json.loads(raw_content)
            print("âœ… JSON íŒŒì‹± ì„±ê³µ")
            return result
            
        except json.JSONDecodeError as e:
            print(f"âŒ LLM JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì›ë³¸ ì‘ë‹µ: {raw_content}")
            return {
                "summary": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ìš”ì•½ ìƒì„± ë¶ˆê°€",
                "main_topics": [],
                "key_concepts": [],
                "technical_terms": []
            }
            
    except Exception as e:
        print(f"LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
            "main_topics": [],
            "key_concepts": [],
            "technical_terms": []
        }




# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        from src.agents.question_generator.unified_parser import parse_pdf_unified
        
        pdf_path = sys.argv[1]
        output_dir = sys.argv[2] if len(sys.argv) > 2 else "data/outputs"
        
        if os.path.exists(pdf_path):
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(output_dir, exist_ok=True)
            
            # PDF íŒŒì‹± (í†µí•© íŒŒì„œ ì‚¬ìš©)
            source_file = os.path.basename(pdf_path)
            collection_name = os.path.splitext(source_file)[0]
            
            print(f"ğŸ“„ PDF íŒŒì‹± ì¤‘: {source_file}")
            blocks = parse_pdf_unified(pdf_path, collection_name)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½
            print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ ì¤‘...")
            result = extract_keywords_and_summary(blocks, source_file)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥ (ì»¬ë ‰ì…˜ëª… ì •ê·œí™”)
            normalized_name = normalize_collection_name(collection_name)
            output_filename = f"{normalized_name}_keywords_summary.json"
            output_path = os.path.join(output_dir, output_filename)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… í‚¤ì›Œë“œ ë° ìš”ì•½ ì™„ë£Œ!")
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
            print()
            print("ğŸ“Š ê²°ê³¼ ìš”ì•½:")
            print(f"  - íŒŒì¼ëª…: {result['document_info']['filename']}")
            print(f"  - ìš”ì•½: {result['content_analysis']['summary'][:100]}...")
            print(f"  - ì£¼ìš” ì£¼ì œ: {len(result['content_analysis']['main_topics'])}ê°œ")
            print(f"  - í•µì‹¬ ê°œë…: {len(result['content_analysis']['key_concepts'])}ê°œ")
            print(f"  - ê¸°ìˆ  ìš©ì–´: {len(result['content_analysis']['technical_terms'])}ê°œ")
            
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    else:
        print("ì‚¬ìš©ë²•: python keyword_summary.py <pdf_path> [output_dir]")
        print("ì˜ˆì‹œ: python keyword_summary.py 'file.pdf' 'outputs/'")
        print("ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: data/outputs")