"""
ê²°ê³¼ ì €ì¥ ê´€ë ¨ ë„êµ¬
"""

import json
import os
from datetime import datetime
from typing import Dict, List, Any


class ResultSaver:
    """ë¬¸ì œ ìƒì„± ê²°ê³¼ ì €ì¥ í´ë˜ìŠ¤"""
    
    @staticmethod
    def save_enhanced_questions(
        questions: List[Dict], 
        summary: Dict,
        total_plan: Dict,
        document_plan: Dict
    ) -> Dict[str, Any]:
        """í–¥ìƒëœ ë¬¸ì œ ìƒì„± ê²°ê³¼ ì €ì¥ (ê¸°ë³¸/ì—¬ë¶„ ë¬¸ì œ ë¶„ë¦¬)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê¸°ë³¸ ë¬¸ì œì™€ ì—¬ë¶„ ë¬¸ì œ ë¶„ë¦¬
        basic_questions = [q for q in questions if q.get('generation_type') == 'basic']
        extra_questions = [q for q in questions if q.get('generation_type') == 'advanced']
        
        # ê³µí†µ ë©”íƒ€ë°ì´í„°
        common_metadata = {
            "generation_info": {
                "method": "enhanced_vector_search",
                "generated_at": datetime.now().isoformat(),
                "generation_summary": summary,
                "test_plans_used": {
                    "total_plan_name": total_plan.get('test_plan', {}).get('name'),
                    "document_count": summary['total_documents']
                }
            }
        }
        
        output_dir = "data/outputs/generated_questions"
        os.makedirs(output_dir, exist_ok=True)
        files_created = []
        
        # 1. ê¸°ë³¸ ë¬¸ì œ ì €ì¥
        if basic_questions:
            basic_data = {
                **common_metadata,
                "question_type": "basic",
                "total_questions": len(basic_questions),
                "questions_by_document": {},
                "all_questions": basic_questions
            }
            
            # ë¬¸ì„œë³„ ê¸°ë³¸ ë¬¸ì œ ë¶„ë¥˜
            for question in basic_questions:
                doc_name = question.get('document_source', 'Unknown')
                if doc_name not in basic_data["questions_by_document"]:
                    basic_data["questions_by_document"][doc_name] = []
                basic_data["questions_by_document"][doc_name].append(question)
            
            basic_file = f"{output_dir}/basic_questions_{timestamp}.json"
            with open(basic_file, 'w', encoding='utf-8') as f:
                json.dump(basic_data, f, ensure_ascii=False, indent=2)
            files_created.append(basic_file)
            print(f"ğŸ’¾ ê¸°ë³¸ ë¬¸ì œ ì €ì¥: {basic_file}")
            print(f"ğŸ“ˆ ê¸°ë³¸ ë¬¸ì œ ìˆ˜: {len(basic_questions)}ê°œ")
        
        # 2. ì—¬ë¶„ ë¬¸ì œ ì €ì¥
        if extra_questions:
            extra_data = {
                **common_metadata,
                "question_type": "extra",
                "total_questions": len(extra_questions),
                "questions_by_document": {},
                "all_questions": extra_questions
            }
            
            # ë¬¸ì„œë³„ ì—¬ë¶„ ë¬¸ì œ ë¶„ë¥˜
            for question in extra_questions:
                doc_name = question.get('document_source', 'Unknown')
                if doc_name not in extra_data["questions_by_document"]:
                    extra_data["questions_by_document"][doc_name] = []
                extra_data["questions_by_document"][doc_name].append(question)
            
            extra_file = f"{output_dir}/extra_questions_{timestamp}.json"
            with open(extra_file, 'w', encoding='utf-8') as f:
                json.dump(extra_data, f, ensure_ascii=False, indent=2)
            files_created.append(extra_file)
            print(f"ğŸ’¾ ì—¬ë¶„ ë¬¸ì œ ì €ì¥: {extra_file}")
            print(f"ğŸ¯ ì—¬ë¶„ ë¬¸ì œ ìˆ˜: {len(extra_questions)}ê°œ")
        
        
        print(f"\nğŸ“Š ì´ ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ")
        print(f"ğŸ“ˆ ê¸°ë³¸ ë¬¸ì œ: {len(basic_questions)}ê°œ")
        print(f"ğŸ¯ ì—¬ë¶„ ë¬¸ì œ: {len(extra_questions)}ê°œ")
        print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼: {len(files_created)}ê°œ")
        
        return {
            "status": "completed",
            "total_questions": len(questions),
            "basic_questions_count": len(basic_questions),
            "extra_questions_count": len(extra_questions),
            "summary": summary,
            "questions": questions,
            "files_created": files_created,
            "file_details": {
                "basic_file": files_created[0] if basic_questions else None,
                "extra_file": files_created[1] if extra_questions and len(files_created) > 1 else files_created[0] if extra_questions else None
            }
        }
    
    @staticmethod
    def save_standard_question_results(
        questions: List[Dict],
        source_file: str,
        keywords: List[str],
        main_topics: List[str],
        summary: str
    ) -> List[str]:
        """í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë¬¸ì œ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.splitext(os.path.basename(source_file))[0]
        files_created = []

        # 1. ìƒì„±ëœ ë¬¸ì œ íŒŒì¼ ì €ì¥
        questions_dir = "data/outputs/generated_questions"
        os.makedirs(questions_dir, exist_ok=True)

        questions_data = {
            "test_info": {
                "source_file": source_file,
                "generation_date": datetime.now().isoformat(),
                "test_type": "auto_generated",
            },
            "question_summary": {
                "total_questions": len(questions),
                "objective_questions": len(
                    [q for q in questions if q.get("type") == "OBJECTIVE"]
                ),
                "subjective_questions": len(
                    [q for q in questions if q.get("type") == "SUBJECTIVE"]
                ),
                "difficulty_distribution": {
                    "easy": len(
                        [q for q in questions if q.get("difficulty_level") == "EASY"]
                    ),
                    "normal": len(
                        [q for q in questions if q.get("difficulty_level") == "NORMAL"]
                    ),
                    "hard": len(
                        [q for q in questions if q.get("difficulty_level") == "HARD"]
                    ),
                },
            },
            "questions": questions,
            "source_keywords": keywords,
            "source_topics": main_topics,
        }

        questions_file = f"{questions_dir}/{filename}_questions_{timestamp}.json"
        with open(questions_file, "w", encoding="utf-8") as f:
            json.dump(questions_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ìƒì„±ëœ ë¬¸ì œ ì €ì¥: {questions_file}")
        files_created.append(questions_file)

        # 2. í…ŒìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ìƒì„±
        summary_file = ResultSaver._save_test_summary(
            questions, source_file, keywords, main_topics, summary, timestamp
        )
        if summary_file:
            files_created.append(summary_file)

        # 3. í…ŒìŠ¤íŠ¸ config íŒŒì¼ ìƒì„±
        config_file = ResultSaver._save_test_config(questions, source_file, timestamp)
        if config_file:
            files_created.append(config_file)

        return files_created
    
    @staticmethod
    def _save_test_summary(
        questions: List[Dict],
        source_file: str,
        keywords: List[str],
        main_topics: List[str],
        summary: str,
        timestamp: str,
    ) -> str:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ì €ì¥"""
        try:
            filename = os.path.splitext(os.path.basename(source_file))[0]
            summary_dir = "data/outputs/test_summaries"
            os.makedirs(summary_dir, exist_ok=True)

            objective_questions = [q for q in questions if q.get("type") == "OBJECTIVE"]
            subjective_questions = [
                q for q in questions if q.get("type") == "SUBJECTIVE"
            ]

            test_summary_data = {
                "test_overview": {
                    "title": f"{filename} - ìë™ ìƒì„± í…ŒìŠ¤íŠ¸",
                    "description": f"'{filename}' ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ìë™ ìƒì„±í•œ í‰ê°€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                    "source_document": source_file,
                    "creation_date": datetime.now().isoformat(),
                    "test_type": "ì¢…í•© í‰ê°€",
                    "estimated_duration": f"{max(30, len(questions) * 2)}ë¶„",
                },
                "content_analysis": {
                    "document_summary": (
                        summary[:300] + "..." if len(summary) > 300 else summary
                    ),
                    "key_concepts": keywords[:10],
                    "main_topics": main_topics[:5],
                    "content_complexity": ResultSaver._analyze_content_complexity(
                        keywords, main_topics, questions
                    ),
                },
                "test_structure": {
                    "total_questions": len(questions),
                    "question_breakdown": {
                        "objective": {
                            "count": len(objective_questions),
                            "percentage": (
                                round(
                                    len(objective_questions) / len(questions) * 100, 1
                                )
                                if questions
                                else 0
                            ),
                            "focus_areas": ResultSaver._extract_question_topics(
                                objective_questions
                            ),
                        },
                        "subjective": {
                            "count": len(subjective_questions),
                            "percentage": (
                                round(
                                    len(subjective_questions) / len(questions) * 100, 1
                                )
                                if questions
                                else 0
                            ),
                            "focus_areas": ResultSaver._extract_question_topics(
                                subjective_questions
                            ),
                        },
                    },
                    "difficulty_distribution": {
                        "easy": len(
                            [
                                q
                                for q in questions
                                if q.get("difficulty_level") == "EASY"
                            ]
                        ),
                        "normal": len(
                            [
                                q
                                for q in questions
                                if q.get("difficulty_level") == "NORMAL"
                            ]
                        ),
                        "hard": len(
                            [
                                q
                                for q in questions
                                if q.get("difficulty_level") == "HARD"
                            ]
                        ),
                    },
                },
                "assessment_guidelines": {
                    "objective_scoring": "ê° ê°ê´€ì‹ ë¬¸í•­ë‹¹ 1ì , ì •ë‹µ/ì˜¤ë‹µìœ¼ë¡œ ì±„ì ",
                    "subjective_scoring": "ë¬¸í•­ë³„ ë°°ì ì— ë”°ë¼ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬ ê°€ëŠ¥",
                    "total_points": len(objective_questions)
                    + len(subjective_questions) * 5,
                    "passing_criteria": "ì´ì ì˜ 60% ì´ìƒ íšë“ ì‹œ í•©ê²©",
                },
            }

            summary_file = f"{summary_dir}/{filename}_test_summary_{timestamp}.json"
            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(test_summary_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì €ì¥: {summary_file}")
            return summary_file

        except Exception as e:
            print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    @staticmethod
    def _save_test_config(
        questions: List[Dict], source_file: str, timestamp: str
    ) -> str:
        """í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ì €ì¥"""
        try:
            filename = os.path.splitext(os.path.basename(source_file))[0]
            config_dir = "data/outputs/test_configs"
            os.makedirs(config_dir, exist_ok=True)

            objective_questions = [q for q in questions if q.get("type") == "OBJECTIVE"]
            subjective_questions = [
                q for q in questions if q.get("type") == "SUBJECTIVE"
            ]

            test_config_data = {
                "test_metadata": {
                    "config_version": "1.0",
                    "test_id": f"auto_test_{timestamp}",
                    "source_document": source_file,
                    "generation_system": "SKIB-AI Question Generator",
                    "creation_timestamp": datetime.now().isoformat(),
                },
                "test_settings": {
                    "time_limit": {
                        "total_minutes": max(30, len(questions) * 2),
                        "warning_at_minutes": max(25, len(questions) * 2 - 5),
                        "automatic_submit": True,
                    },
                    "question_settings": {
                        "randomize_order": False,
                        "allow_review": True,
                        "show_progress": True,
                    },
                },
                "scoring_configuration": {
                    "objective_questions": {
                        "points_per_question": 1,
                        "negative_marking": False,
                    },
                    "subjective_questions": {
                        "points_per_question": 5,
                        "allow_partial_credit": True,
                        "manual_grading_required": True,
                    },
                    "total_points": len(objective_questions)
                    + len(subjective_questions) * 5,
                },
            }

            config_file = f"{config_dir}/{filename}_test_config_{timestamp}.json"
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(test_config_data, f, ensure_ascii=False, indent=2)
            print(f"âš™ï¸ í…ŒìŠ¤íŠ¸ ì„¤ì • ì €ì¥: {config_file}")
            return config_file

        except Exception as e:
            print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def _analyze_content_complexity(
        keywords: List[str], main_topics: List[str], questions: List[Dict]
    ) -> str:
        """ì½˜í…ì¸  ë³µì¡ë„ ë¶„ì„"""
        keywords_count = len(keywords)
        topics_count = len(main_topics)
        hard_questions = len(
            [q for q in questions if q.get("difficulty_level") == "HARD"]
        )

        if keywords_count > 10 and topics_count > 5 and hard_questions > 2:
            return "ê³ ê¸‰"
        elif keywords_count > 5 and topics_count > 3:
            return "ì¤‘ê¸‰"
        else:
            return "ì´ˆê¸‰"

    @staticmethod
    def _extract_question_topics(questions: List[Dict]) -> List[str]:
        """ë¬¸ì œì—ì„œ ì£¼ìš” ì£¼ì œ ì¶”ì¶œ"""
        topics = []
        for q in questions[:3]:  # ìƒìœ„ 3ê°œ ë¬¸ì œë§Œ ë¶„ì„
            question_text = q.get("question", "")
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if "í”„ë¡œì„¸ìŠ¤" in question_text:
                topics.append("í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬")
            if "ì—…ë¬´" in question_text:
                topics.append("ì—…ë¬´ ì²˜ë¦¬")
            if "ê³„ì•½" in question_text:
                topics.append("ê³„ì•½ ê´€ë¦¬")
            if "ë“±ë¡" in question_text:
                topics.append("ë“±ë¡ ì ˆì°¨")

        return list(set(topics))[:3]  # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 3ê°œ ë°˜í™˜