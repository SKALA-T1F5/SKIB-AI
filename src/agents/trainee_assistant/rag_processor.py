from typing import List, Dict, Any
from src.agents.trainee_assistant.models import MessageType, SearchResult
from src.agents.trainee_assistant.tools import VectorSearchTool, WebSearchTool
from src.agents.trainee_assistant.memory_manager import ConversationMemoryManager
from config.settings import settings 
import openai  # ì‹¤ì œ LLM ì‚¬ìš© ì‹œ í•„ìš”

# OpenAI ì„¤ì •
openai.api_key = settings.api_key

class RAGProcessor:
    """RAG(Retrieval-Augmented Generation) ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, document_name: str, similarity_threshold: float = 0.7):
        self.similarity_threshold = similarity_threshold
        self.vector_search_tool = VectorSearchTool(document_name=document_name)
        self.web_search_tool = WebSearchTool(
            api_key=settings.google_api_key,
            cx=settings.google_cx_id
        )
        self.last_workflow_info = {}
    
    async def info_node(self, query: str, memory_manager: ConversationMemoryManager) -> Dict[str, Any]:
        """
        ğŸ”µ [2] info ë…¸ë“œ ì‹¤í–‰
        ì§ˆë¬¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ChromaDBì— ìœ ì‚¬ë„ ê²€ìƒ‰í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
        """
        # ë„êµ¬ í˜¸ì¶œ ë¡œê·¸ ì¶”ê°€
        memory_manager.add_message(
            MessageType.TOOL_CALL, 
            f"ê´€ë ¨ í•™ìŠµ ìë£Œ ê²€ìƒ‰ ì‹¤í–‰: {query}",
            {"tool": "vector_search", "query": query}
        )
        
        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        vector_results = await self.vector_search_tool.search(query, n_results=5)
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸ ì¶”ê°€
        memory_manager.add_message(
            MessageType.TOOL_RESULT,
            f"í•™ìŠµ ìë£Œ ê²€ìƒ‰ ê²°ê³¼: {len(vector_results)}ê°œ ìë£Œ ë°œê²¬",
            {"tool": "vector_search", "results_count": len(vector_results)}
        )
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì˜ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        high_similarity_results = [
            result for result in vector_results 
            if result.score >= self.similarity_threshold
        ]
        
        # ì›Œí¬í”Œë¡œìš° ì •ë³´ ì—…ë°ì´íŠ¸
        if high_similarity_results:
            # âœ… ê²€ìƒ‰ ê²°ê³¼ ìˆìŒ â†’ vector_search_tool í˜¸ì¶œ ì¤€ë¹„
            workflow_info = {
                "search_executed": True,
                "search_query": query,
                "search_result": {
                    "source": "vector_search",
                    "results_count": len(high_similarity_results),
                    "avg_similarity": sum(r.score for r in high_similarity_results) / len(high_similarity_results),
                    "threshold_met": True
                },
                "next_action": "use_vector_search"
            }
            
            self.last_workflow_info = workflow_info
            
            return {
                "next_action": "use_vector_search",
                "results": high_similarity_results,
                "message": f"ê´€ë ¨ í•™ìŠµ ìë£Œ {len(high_similarity_results)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                "workflow_info": workflow_info
            }
        else:
            # âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ web_search_tool í˜¸ì¶œ ì¤€ë¹„
            workflow_info = {
                "search_executed": True,
                "search_query": query,
                "search_result": {
                    "source": "vector_search",
                    "results_count": len(vector_results),
                    "threshold_met": False,
                    "will_fallback_to_web": True
                },
                "next_action": "use_web_search",
            }
            
            self.last_workflow_info = workflow_info
            
            return {
                "next_action": "use_web_search",
                "results": [],
                "message": "ê´€ë ¨ í•™ìŠµ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜¨ë¼ì¸ ìë£Œ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.",
                "workflow_info": workflow_info
            }
    
    async def vector_search_node(self, query: str, results: List[SearchResult], system_prompt: str) -> str:
        """
        ğŸŸ¡ [3A] vector_search ë…¸ë“œ ì‹¤í–‰
        ChromaDB ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        """
        # ì›Œí¬í”Œë¡œìš° ì •ë³´ ì—…ë°ì´íŠ¸
        self.last_workflow_info.update({
            "final_action": "vector_search_response_generated",
            "documents_used": len(results[:3])
        })
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        context_content = "\n\n".join([
            f"[í•™ìŠµ ìë£Œ {i+1}] (ê´€ë ¨ë„: {result.score:.2f})\n{result.content}"
            for i, result in enumerate(results[:3])  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
        ])
        
        enhanced_prompt = f"""{system_prompt}

ê´€ë ¨ í•™ìŠµ ìë£Œ:
{context_content}

ìœ„ í•™ìŠµ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ìë£Œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        
        # LLM API í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
        response = await self._generate_response_with_context(query, enhanced_prompt)
        
        return response
    
    async def web_search_node(self, query: str, system_prompt: str, memory_manager: ConversationMemoryManager) -> str:
        """
        ğŸŸ¡ [3B] web_search ë…¸ë“œ ì‹¤í–‰
        ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        """
        # ì›¹ ê²€ìƒ‰ ì‹¤í–‰
        memory_manager.add_message(
            MessageType.TOOL_CALL,
            f"ì˜¨ë¼ì¸ í•™ìŠµ ìë£Œ ê²€ìƒ‰ ì‹¤í–‰: {query}",
            {"tool": "web_search", "query": query}
        )
        
        web_results = await self.web_search_tool.search(query, num_results=3)
        
        memory_manager.add_message(
            MessageType.TOOL_RESULT,
            f"ì˜¨ë¼ì¸ ê²€ìƒ‰ ê²°ê³¼: {len(web_results)}ê°œ ìë£Œ ë°œê²¬",
            {"tool": "web_search", "results_count": len(web_results)}
        )
        
        # ì›Œí¬í”Œë¡œìš° ì •ë³´ ì—…ë°ì´íŠ¸
        self.last_workflow_info.update({
            "web_search_executed": True,
            "web_results_count": len(web_results),
            "final_action": "web_search_response_generated"
        })
        
        if not web_results:
            self.last_workflow_info.update({"final_action": "no_results_found"})
            return """ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ í•™ìŠµ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 
í•˜ì§€ë§Œ ë¬¸ì œì™€ ì •ë‹µ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœëŒ€í•œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. 
êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ì§€ ë” ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ ë” ë‚˜ì€ ì„¤ëª…ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        context_content = "\n\n".join([
            f"[ì˜¨ë¼ì¸ ìë£Œ {i+1}]\n{result.content}"
            for i, result in enumerate(web_results)
        ])
        
        enhanced_prompt = f"""{system_prompt}

ì˜¨ë¼ì¸ í•™ìŠµ ìë£Œ:
{context_content}

ìœ„ ì˜¨ë¼ì¸ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ìë£Œì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
        
        response = await self._generate_response_with_context(query, enhanced_prompt)
        
        return response
    
    def get_workflow_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ì›Œí¬í”Œë¡œìš° ì •ë³´ ë°˜í™˜"""
        return self.last_workflow_info.copy()
    
    async def _generate_response_with_context(self, user_message: str, system_prompt: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì‘ë‹µ ìƒì„±"""
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            response = await openai.ChatCompletion.acreate(
                model="gpt-4o",
                messages=messages,
                temperature=0.7
            )

            return response.choices[0].message.content
        
        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
