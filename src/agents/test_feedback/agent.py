# agents/test_feedback/agent.py
import os
import openai
from google import genai
import json
from collections import defaultdict
import re

from openai import AsyncOpenAI
from typing import List, Dict, Any
from api.grading.schemas.subjective_grading import GradingCriterion
from utils.parse_json_response import parse_json_response

from dotenv import load_dotenv
from src.agents.test_feedback.prompt import SYSTEM_PROMPT, build_user_prompt


#openai ë¡œë“œ
load_dotenv(override=True) 
api_key = os.getenv("OPENAI_API_KEY") 
openai_client = AsyncOpenAI(api_key=api_key) 
AGENT_MODEL = os.getenv("AGENT_TEST_FEEDBACK_MODEL") #.envì— ëª¨ë¸ëª… ì €ì¥ (AGENT_TEST_FEEDBACK_MODEL=gpt-4)âœ…

#gemini ë¡œë“œ
# load_dotenv(override=True) 
gemini_api_key = os.getenv("GEMINI_API_KEY") 
gemini_client = genai.Client(api_key=gemini_api_key)
GEMINI_MODEL = os.getenv("GEMINI_AGENT_TEST_FEEDBACK_MODEL") #.envì— ëª¨ë¸ëª… ì €ì¥ (GEMINI_AGENT_TEST_FEEDBACK_MODEL=gemini-2.5-flash)âœ…


def calc_performance_by_document(question_results: List[Dict[str, Any]]):
    doc_map = defaultdict(list)
    for q in question_results:
        doc_map[q['documentName']].append(q)
    performance = []
    for doc, questions in doc_map.items():
        avg = sum(q['correctRate'] for q in questions) / len(questions)
        keywords = list({q['keyword'] for q in questions if 'keyword' in q})
        performance.append({
            "documentName": doc,
            "averageCorrectRate": round(avg, 2),
            "countQuestions": len(questions),  # ë¬¸ì„œë³„ ì´ ë¬¸ì œ ê°œìˆ˜ ì¶”ê°€
            "keywords": keywords
        })
    return performance

def select_top_bottom_questions(question_results: List[Dict[str, Any]], top_count: int = 5, bottom_count: int = 5) -> List[Dict[str, Any]]:
    """
    ì „ì²´ ë¬¸ì œ ì¤‘ ì •ë‹µë¥  ê¸°ì¤€ ìƒìœ„ 5ê°œ, í•˜ìœ„ 5ê°œ ë¬¸ì œë¥¼ ì„ íƒí•˜ì—¬ ì´ 10ê°œ ë¬¸ì œë¥¼ ë°˜í™˜
    """
    # ì •ë‹µë¥  ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ë¬¸ì œ ì •ë ¬
    sorted_questions = sorted(question_results, key=lambda x: x['correctRate'], reverse=True)
    
    selected_questions = []
    
    # ìƒìœ„ 5ê°œ ì„ íƒ
    top_questions = sorted_questions[:top_count]
    selected_questions.extend(top_questions)
    
    # í•˜ìœ„ 5ê°œ ì„ íƒ (ì¤‘ë³µ ë°©ì§€)
    if len(sorted_questions) > top_count + bottom_count:
        bottom_questions = sorted_questions[-(bottom_count):]
    elif len(sorted_questions) > top_count:
        bottom_questions = sorted_questions[top_count:]
    else:
        bottom_questions = []
    
    selected_questions.extend(bottom_questions)
    
    return selected_questions

def extract_json_from_gemini(content: str) -> str:
    # ì½”ë“œë¸”ë¡ ë‚´ JSON ì¶”ì¶œ
    match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", content)
    if match:
        return match.group(1)
    # ì¼ë°˜ ì½”ë“œë¸”ë¡ (json ëª…ì‹œX)
    match = re.search(r"```\s*(\{[\s\S]*?\})\s*```", content)
    if match:
        return match.group(1)
    # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ì²« JSON ê°ì²´ ì¶”ì¶œ
    match = re.search(r"(\{[\s\S]*\})", content)
    if match:
        return match.group(1)
    # ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    return content.strip()

async def test_feedback(exam_goal: str, question_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    OpenAIë¥¼ ì´ìš©í•˜ì—¬ ì‹œí—˜ëª©í‘œì™€ ë¬¸í•­ë³„ ì‘ì‹œ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì¢…í•©ì ì¸ í”¼ë“œë°±ì„ ë°˜í™˜
    """
    # 1. ì‚¬ì „ ë°ì´í„° ê³„ì‚°
    performance_by_document = calc_performance_by_document(question_results)
    selected_questions = select_top_bottom_questions(question_results)

    # 2. ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    USER_PROMPT = build_user_prompt(exam_goal, selected_questions, performance_by_document)

    # 3. MODEL í˜¸ì¶œ
    try:
        # RAW INPUT ì¶œë ¥ #########################################
        # print("\n" + "="*80)
        # print("ğŸ¤– MODEL INPUT (RAW)")
        # print("="*80)
        # messages=[
        #         {"role": "system", "content": SYSTEM_PROMPT},
        #         {"role": "user", "content": USER_PROMPT}
        #     ]
        # print(messages)
        ########################################################
        
        #OPENAI ë°©ì‹
        # response = await openai_client.chat.completions.create(
        #     model=AGENT_MODEL,
        #     messages=[
        #         {"role": "system", "content": SYSTEM_PROMPT},
        #         {"role": "user", "content": USER_PROMPT}
        #     ],
        #     temperature=0.2,
        #     stream=False,
        # )
        # content = response.choices[0].message.content.strip()


        #GEMINI ë°©ì‹
        response = gemini_client.models.generate_content(
            model=GEMINI_MODEL,
            contents=[
                {"role": "model", "parts": [{"text": SYSTEM_PROMPT}]},
                {"role": "user", "parts": [{"text": USER_PROMPT}]}
            ],
        )
        content = response.text
        content = response.choices[0].message.content.strip()
        # Gemini ë“± LLMì˜ ì½”ë“œë¸”ë¡/í…ìŠ¤íŠ¸ í˜¼í•© ì‘ë‹µì—ì„œ JSONë§Œ ì¶”ì¶œ
        content = extract_json_from_gemini(content)
        try:
            result = json.loads(content)
        except Exception as e:
            print("AI ì›ë³¸ ì‘ë‹µ:", content)
            raise RuntimeError(f"ì‹œí—˜ í”¼ë“œë°± ìƒì„± ì˜¤ë¥˜: {str(e)}")
        

        # RAW OUTPUT ì¶œë ¥ #########################################
        print("\n" + "="*80)
        print("ğŸ¤– MODEL OUTPUT (RAW)")
        print("="*80)
        print(content)
        print("="*80)
        ########################################################

        result = json.loads(content)

        # 4. AI ê²°ê³¼ í›„ì²˜ë¦¬
        # 4-1. projectReadinessë¥¼ ë¬¸ì„œë³„ ìµœì†Œ ì •ë‹µë¥  ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ê²°ê³¼ì— ì¶”ê°€
        min_rate = min(doc['averageCorrectRate'] for doc in performance_by_document) if performance_by_document else 0
        if min_rate >= 90:
            project_readiness_result = "Excellent"
        elif min_rate >= 60:
            project_readiness_result = "Pass"
        else:
            project_readiness_result = "Fail"
        result['projectReadiness'] = project_readiness_result
        
        # 4-2. averageCorrectRateë§Œ ì‹¤ì œ ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
        doc_rate_map = {doc['documentName']: doc['averageCorrectRate'] for doc in performance_by_document}
        for doc in result.get('performanceByDocument', []):
            name = doc.get('documentName')
            if name in doc_rate_map:
                doc['averageCorrectRate'] = doc_rate_map[name]


        # í† í° ì‚¬ìš©ëŸ‰ (ì°¨í›„ ì£¼ì„ì²˜ë¦¬ âœ… )
        usage = response.usage
        print("ğŸŸ¨ ì‚¬ìš© í† í°:", usage.total_tokens)
        print("â””â”€ prompt_tokens:", usage.prompt_tokens)
        print("â””â”€ completion_tokens:", usage.completion_tokens)
        
        return result

    except Exception as e:
        raise RuntimeError(f"ì‹œí—˜ í”¼ë“œë°± ìƒì„± ì˜¤ë¥˜: {str(e)}")