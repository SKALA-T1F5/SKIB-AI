import os
from typing import Dict, List

import pdfplumber
from docling.document_converter import DocumentConverter


def _extract_structured_text_with_docling(pdf_path: str) -> List[Dict]:
    """
    Doclingì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text_analyzer.pyì˜ _extract_with_doclingê³¼ í†µí•©ëœ ë¡œì§
    """
    converter = DocumentConverter()
    result = converter.convert(pdf_path)
    document = result.document
    blocks = []
    
    try:
        # í…ìŠ¤íŠ¸ ê°ì²´ë“¤ ì²˜ë¦¬ (text_analyzer.pyì™€ ë™ì¼í•œ ë¡œì§)
        if hasattr(document, "texts"):
            for text_obj in document.texts:
                content = getattr(text_obj, "text", str(text_obj)).strip()
                if content and len(content) > 10:  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
                    page_no = getattr(text_obj, "page_no", 0) + 1  # 1-indexed
                    text_type = _classify_text_type(content)  # í†µí•©ëœ ë¶„ë¥˜ í•¨ìˆ˜ ì‚¬ìš©
                    
                    blocks.append({
                        "type": text_type,
                        "content": content,
                        "metadata": {
                            "page": page_no,
                            "extraction_method": "docling_structured",
                            "source_file": os.path.basename(pdf_path),
                            "text_length": len(content),  # text_analyzer.pyì™€ ì¼ê´€ì„±
                        },
                    })

        # ì œëª© ê°ì²´ë“¤ ì²˜ë¦¬ (text_analyzer.pyì™€ ë™ì¼í•œ ë¡œì§)
        if hasattr(document, "titles"):
            for title_obj in document.titles:
                title = getattr(title_obj, "text", str(title_obj)).strip()
                if title:
                    page_no = getattr(title_obj, "page_no", 0) + 1
                    
                    blocks.append({
                        "type": "section",
                        "title": title,
                        "content": title,  # content í•„ë“œ ì¶”ê°€ (ì¼ê´€ì„±ì„ ìœ„í•´)
                        "metadata": {
                            "page": page_no,
                            "extraction_method": "docling_section",
                            "source_file": os.path.basename(pdf_path),
                            "text_length": len(title),  # text_analyzer.pyì™€ ì¼ê´€ì„±
                        },
                    })
                    
    except Exception as e:
        print(f"âš ï¸ Docling êµ¬ì¡°í™” ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return _fallback_text_extraction(pdf_path)
    
    print(f"  ğŸ“ Docling í…ìŠ¤íŠ¸ ë¸”ë¡: {len(blocks)}ê°œ")
    return blocks


def _fallback_text_extraction(pdf_path: str) -> List[Dict]:
    """
    Docling ì‹¤íŒ¨ ì‹œ pdfplumberë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text_analyzer.pyì˜ _extract_with_pdfplumberì™€ í†µí•©ëœ ë¡œì§
    """
    blocks = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            page_no = page_num + 1
            text_content = page.extract_text()
            
            if text_content and text_content.strip():
                # ë¬¸ë‹¨ë³„ë¡œ ë¶„í•  (text_analyzer.pyì™€ ë™ì¼í•œ ë¡œì§)
                paragraphs = [
                    p.strip()
                    for p in text_content.split("\n\n")
                    if p.strip() and len(p) > 20  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
                ]
                
                for para in paragraphs:
                    # í…ìŠ¤íŠ¸ íƒ€ì… ë¶„ë¥˜ ì ìš©
                    text_type = _classify_text_type(para)
                    
                    blocks.append({
                        "type": text_type,  # ë‹¨ìˆœíˆ "paragraph"ê°€ ì•„ë‹Œ ë¶„ë¥˜ëœ íƒ€ì… ì‚¬ìš©
                        "content": para,
                        "metadata": {
                            "page": page_no,
                            "extraction_method": "pdfplumber_fallback",
                            "source_file": os.path.basename(pdf_path),
                            "text_length": len(para),  # text_analyzer.pyì™€ ì¼ê´€ì„±
                        },
                    })
    
    return blocks


def _classify_text_type(content: str) -> str:
    """í…ìŠ¤íŠ¸ íƒ€ì… ë¶„ë¥˜ - text_analyzer.pyì˜ classify_text_typeê³¼ í†µí•©ëœ ë¡œì§"""
    if not content or not content.strip():
        return "paragraph"

    content = content.strip()
    content_lower = content.lower()

    # ì œëª©/í—¤ë”© íŒ¨í„´ (text_analyzer.pyì™€ ë™ì¼í•œ ë¡œì§)
    if len(content) < 100 and (
        content.isupper()
        or any(
            keyword in content_lower
            for keyword in ["ì±•í„°", "chapter", "ëª©ì°¨", "ì œ", "ë¶€"]
        )
        or content.endswith(":")
        or content.count("\n") == 0
    ):
        return "heading"

    # ì„¹ì…˜ íŒ¨í„´ (text_analyzer.pyì™€ ë™ì¼í•œ ë¡œì§)
    if len(content) < 200 and any(
        keyword in content_lower
        for keyword in ["ê·¸ë¦¼", "figure", "í‘œ", "table", "ë¶€ë¡", "appendix"]
    ):
        return "section"

    return "paragraph"
