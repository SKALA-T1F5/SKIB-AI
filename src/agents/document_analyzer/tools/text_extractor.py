import logging
import os
from typing import Dict, List

import pdfplumber
from docling.document_converter import DocumentConverter

logger = logging.getLogger(__name__)


def _extract_structured_text_with_docling(pdf_path: str) -> List[Dict]:
    """
    Doclingì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    logger.info(
        f"ğŸ” Doclingìœ¼ë¡œ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {os.path.basename(pdf_path)}"
    )

    converter = DocumentConverter()
    result = converter.convert(pdf_path)
    document = result.document
    blocks = []

    try:
        # í…ìŠ¤íŠ¸ ê°ì²´ë“¤ ì²˜ë¦¬
        if hasattr(document, "texts"):
            logger.debug(f"  ğŸ“„ í…ìŠ¤íŠ¸ ê°ì²´ ìˆ˜: {len(document.texts)}ê°œ")
            text_count = 0
            for text_obj in document.texts:
                content = getattr(text_obj, "text", str(text_obj)).strip()
                if content and len(content) > 10:  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
                    text_count += 1
                    page_no = getattr(text_obj, "page_no", 0) + 1  # 1-indexed
                    text_type = _classify_text_type(content)  # í†µí•©ëœ ë¶„ë¥˜ í•¨ìˆ˜ ì‚¬ìš©

                    blocks.append(
                        {
                            "type": text_type,
                            "content": content,
                            "metadata": {
                                "page": page_no,
                                "extraction_method": "docling_structured",
                                "source_file": os.path.basename(pdf_path),
                                "text_length": len(content),
                            },
                        }
                    )
            logger.info(f"  âœ… ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë¸”ë¡: {text_count}ê°œ ì¶”ì¶œ")
        else:
            logger.warning("  âš ï¸ ë¬¸ì„œì— í…ìŠ¤íŠ¸ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ì œëª© ê°ì²´ë“¤ ì²˜ë¦¬
        if hasattr(document, "titles"):
            logger.debug(f"  ğŸ“ ì œëª© ê°ì²´ ìˆ˜: {len(document.titles)}ê°œ")
            title_count = 0
            for title_obj in document.titles:
                title = getattr(title_obj, "text", str(title_obj)).strip()
                if title:
                    title_count += 1
                    page_no = getattr(title_obj, "page_no", 0) + 1

                    blocks.append(
                        {
                            "type": "section",
                            "title": title,
                            "content": title,  # content í•„ë“œ ì¶”ê°€ (ì¼ê´€ì„±ì„ ìœ„í•´)
                            "metadata": {
                                "page": page_no,
                                "extraction_method": "docling_section",
                                "source_file": os.path.basename(pdf_path),
                                "text_length": len(title),
                            },
                        }
                    )
            logger.info(f"  âœ… ì œëª© ë¸”ë¡: {title_count}ê°œ ì¶”ì¶œ")
        else:
            logger.warning("  âš ï¸ ë¬¸ì„œì— ì œëª© ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤")

    except Exception as e:
        logger.warning(f"âš ï¸ Docling êµ¬ì¡°í™” ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.info("  ğŸ”„ pdfplumber í´ë°± ëª¨ë“œë¡œ ì „í™˜")
        return _fallback_text_extraction(pdf_path)

    logger.info(f"  ğŸ“ Docling í…ìŠ¤íŠ¸ ë¸”ë¡: {len(blocks)}ê°œ")
    return blocks


def _fallback_text_extraction(pdf_path: str) -> List[Dict]:
    """
    Docling ì‹¤íŒ¨ ì‹œ pdfplumberë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    logger.info(f"ğŸ”„ pdfplumber í´ë°± í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {os.path.basename(pdf_path)}")
    blocks = []

    with pdfplumber.open(pdf_path) as pdf:
        logger.debug(f"  ğŸ“„ ì´ í˜ì´ì§€ ìˆ˜: {len(pdf.pages)}ê°œ")
        for page_num, page in enumerate(pdf.pages):
            page_no = page_num + 1
            logger.debug(f"  ğŸ“„ í˜ì´ì§€ {page_no} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            text_content = page.extract_text()

            if text_content and text_content.strip():
                # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
                paragraphs = [
                    p.strip()
                    for p in text_content.split("\n\n")
                    if p.strip() and len(p) > 20  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
                ]

                logger.debug(f"    ğŸ“ í˜ì´ì§€ {page_no}: {len(paragraphs)}ê°œ ë¬¸ë‹¨ ì¶”ì¶œ")

                for para in paragraphs:
                    # í…ìŠ¤íŠ¸ íƒ€ì… ë¶„ë¥˜ ì ìš©
                    text_type = _classify_text_type(para)

                    blocks.append(
                        {
                            "type": text_type,  # ë‹¨ìˆœíˆ "paragraph"ê°€ ì•„ë‹Œ ë¶„ë¥˜ëœ íƒ€ì… ì‚¬ìš©
                            "content": para,
                            "metadata": {
                                "page": page_no,
                                "extraction_method": "pdfplumber_fallback",
                                "source_file": os.path.basename(pdf_path),
                                "text_length": len(para),
                            },
                        }
                    )
            else:
                logger.debug(f"    âš ï¸ í˜ì´ì§€ {page_no}: ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì—†ìŒ")

    logger.info(f"  ğŸ“ pdfplumber í´ë°± í…ìŠ¤íŠ¸ ë¸”ë¡: {len(blocks)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    return blocks


def _classify_text_type(content: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ìœ í˜•ì„ 'heading', 'section', 'paragraph' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.

    - contentê°€ ë¹„ì–´ìˆê±°ë‚˜ ê³µë°±ë§Œ ìˆìœ¼ë©´ 'paragraph' ë°˜í™˜
    - ê¸¸ì´ê°€ ì§§ê³ (100ì ë¯¸ë§Œ), ëŒ€ë¬¸ìì´ê±°ë‚˜ ì œëª© ê´€ë ¨ í‚¤ì›Œë“œ(ì±•í„°, ëª©ì°¨ ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, ì½œë¡ (:)ìœ¼ë¡œ ëë‚˜ê±°ë‚˜ í•œ ì¤„ì´ë©´ 'heading' ë°˜í™˜
    - ê¸¸ì´ê°€ 200ì ë¯¸ë§Œì´ê³ , ì„¹ì…˜ ê´€ë ¨ í‚¤ì›Œë“œ(ê·¸ë¦¼, í‘œ, ë¶€ë¡ ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ 'section' ë°˜í™˜
    - ê·¸ ì™¸ì—ëŠ” 'paragraph' ë°˜í™˜
    """
    if not content or not content.strip():
        return "paragraph"

    content = content.strip()
    content_lower = content.lower()

    # ì œëª©/í—¤ë”© íŒ¨í„´
    if len(content) < 100 and (
        content.isupper()
        or any(
            keyword in content_lower
            for keyword in ["ì±•í„°", "chapter", "ëª©ì°¨", "ì œ", "ë¶€"]
        )
        or content.endswith(":")
        or content.count("\n") == 0
    ):
        return "heading"

    # ì„¹ì…˜ íŒ¨í„´
    if len(content) < 200 and any(
        keyword in content_lower
        for keyword in ["ê·¸ë¦¼", "figure", "í‘œ", "table", "ë¶€ë¡", "appendix"]
    ):
        return "section"

    return "paragraph"
