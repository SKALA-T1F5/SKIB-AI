import logging

"""
ê° ë¬¸ì„œì— ëŒ€í•´ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ì„ ìˆ˜í–‰í•˜ê³  JSON í˜•íƒœë¡œ ì¶œë ¥í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
Doclingìœ¼ë¡œ íŒŒì‹±ëœ ë¸”ë¡ë“¤ì„ ë¶„ì„í•˜ì—¬ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import json
from typing import Dict, List

from langsmith import traceable
from langsmith.wrappers import wrap_openai
from openai import OpenAI

from config.settings import settings

openai_client = wrap_openai(OpenAI(api_key=settings.api_key))
logger = logging.getLogger(__name__)


def extract_keywords_and_summary(blocks: List[Dict], source_file: str) -> Dict:
    """
    Docling ë¸”ë¡ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        blocks: Docling íŒŒì„œì—ì„œ ìƒì„±ëœ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
        source_file: ì›ë³¸ íŒŒì¼ëª…

    Returns:
        Dict: í‚¤ì›Œë“œ, ìš”ì•½, ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    # í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ì—ì„œ ë‚´ìš© ì¶”ì¶œ
    text_content = []
    total_pages = set()
    sections = []

    for block in blocks:
        block_type = block.get("type", "")
        page_no = block.get("metadata", {}).get("page")

        if page_no:
            total_pages.add(page_no)

        if block_type in ["paragraph", "heading"]:  # heading íƒ€ì… ì¶”ê°€
            content = block.get("content", "").strip()
            if content:
                text_content.append(content)
        elif block_type == "section":
            title = block.get("title", "").strip()
            if title:
                sections.append(title)

    # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
    combined_text = "\n".join(text_content)

    # LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½
    llm_result = _extract_keywords_summary_with_llm(combined_text, source_file)

    # ê²°ê³¼ êµ¬ì„±
    result = {
        "document_info": {"filename": source_file},
        "content_analysis": {
            "summary": llm_result.get("summary", ""),
            "main_topics": llm_result.get("main_topics", []),
            "key_concepts": llm_result.get("key_concepts", []),
            "technical_terms": llm_result.get("technical_terms", []),
        },
    }

    return result


@traceable(
    run_type="chain",
    name="Extract Keywords Summary with LLM",
    metadata={"agent_type": "document_analyzer"},
)
def _extract_keywords_summary_with_llm(text: str, filename: str) -> Dict:
    """
    GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ ìˆ˜í–‰
    """
    # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ë° ì œí•œ
    if not text or len(text.strip()) < 50:
        logger.warning("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {
            "summary": "í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ì—¬ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "main_topics": [],
            "key_concepts": [],
            "technical_terms": [],
        }

    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í•  ì²˜ë¦¬ (GPT-4 í† í° ì œí•œ ê³ ë ¤)
    max_length = 4000  # ë” ì•ˆì „í•œ ê¸¸ì´ë¡œ ì¶•ì†Œ
    if len(text) > max_length:
        # ì•ë¶€ë¶„ê³¼ ë’·ë¶€ë¶„ ì¼ë¶€ ì‚¬ìš©í•˜ì—¬ ëŒ€í‘œì„± í™•ë³´
        front_part = text[: max_length // 2]
        back_part = text[-(max_length // 2) :]
        text = front_part + "\n\n[ì¤‘ê°„ ë‚´ìš© ìƒëµ...]\n\n" + back_part
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •: ì›ë³¸ {len(text)}ì â†’ ì••ì¶• {len(text)}ì")

    prompt = f"""
ë¬¸ì„œ "{filename}"ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

1. summary: ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
2. main_topics: ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œ/í† í”½ (ìµœëŒ€ 5ê°œ)
3. key_concepts: í•µì‹¬ ê°œë…ì´ë‚˜ ìš©ì–´ (ìµœëŒ€ 10ê°œ)
4. technical_terms: ì „ë¬¸ ìš©ì–´ë‚˜ ê¸°ìˆ  ìš©ì–´ (ìµœëŒ€ 8ê°œ)

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”:

{{
    "summary": "ë¬¸ì„œ ìš”ì•½ ë‚´ìš©",
    "main_topics": ["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3"],
    "key_concepts": ["ê°œë…1", "ê°œë…2", "ê°œë…3"],
    "technical_terms": ["ìš©ì–´1", "ìš©ì–´2", "ìš©ì–´3"]
}}

ë¶„ì„í•  ë¬¸ì„œ ë‚´ìš©:
{text}
"""

    try:
        logger.info(f"ğŸ¤– GPT-4 ë¶„ì„ ì‹œì‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì)")
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1000,  # í† í° ìˆ˜ ì¤„ì„
            timeout=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        logger.info("âœ… GPT-4 ë¶„ì„ ì™„ë£Œ")

        raw_content = response.choices[0].message.content
        if raw_content is None:
            logger.error("âŒ GPT-4 ì‘ë‹µ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return {
                "summary": "GPT-4 ì‘ë‹µ ì˜¤ë¥˜",
                "main_topics": [],
                "key_concepts": [],
                "technical_terms": [],
            }

        raw_content = raw_content.strip()

        # JSON íŒŒì‹±
        try:
            logger.debug(f"ğŸ“„ ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {raw_content[:100]}...")

            # ì½”ë“œ ë¸”ë¡ ì œê±°
            if "```json" in raw_content:
                raw_content = raw_content.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_content:
                raw_content = raw_content.split("```")[1].split("```")[0].strip()

            result = json.loads(raw_content)
            logger.info("âœ… JSON íŒŒì‹± ì„±ê³µ")
            return result

        except json.JSONDecodeError as e:
            logger.error(f"âŒ LLM JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"ì›ë³¸ ì‘ë‹µ: {raw_content}")
            return {
                "summary": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ìš”ì•½ ìƒì„± ë¶ˆê°€",
                "main_topics": [],
                "key_concepts": [],
                "technical_terms": [],
            }

    except Exception as e:
        logger.error(f"LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
            "main_topics": [],
            "key_concepts": [],
            "technical_terms": [],
        }
