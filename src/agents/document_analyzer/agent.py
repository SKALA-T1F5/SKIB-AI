"""
ë¬¸ì„œ ë¶„ì„ Agent
- ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±
- ì§ˆë¬¸ ìƒì„±
- í‚¤ì›Œë“œ ì¶”ì¶œ
- ë¬¸ì„œ ìš”ì•½
"""

from typing import Dict, List

from .state import DocumentAnalyzerState, create_document_analyzer_state
from .tools.text_analyzer import TextAnalyzer
from .tools.unified_parser import parse_pdf_unified


class DocumentAnalyzerAgent:
    """ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ Agent"""

    """
    ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ Agent
    ì£¼ìš” ê¸°ëŠ¥:
    - PDF ë¬¸ì„œì˜ êµ¬ì¡° íŒŒì‹± (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ)
    - í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¬¸ì„œ ìš”ì•½
    - ChromaDB ìë™ ì—…ë¡œë“œ
    - ë¶„ì„ ê²°ê³¼ ì €ì¥ ë° ê´€ë¦¬
    """

    def __init__(self, collection_name: str = None, auto_upload_chromadb: bool = True):
        """
        DocumentAnalyzer ì´ˆê¸°í™”

        Args:
            collection_name: ChromaDB ì»¬ë ‰ì…˜ëª…
            auto_upload_chromadb: ChromaDB ìë™ ì—…ë¡œë“œ í™œì„±í™” ì—¬ë¶€
        """
        self.collection_name = collection_name
        self.auto_upload_chromadb = auto_upload_chromadb

        # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if collection_name:
            from utils.naming import filename_to_collection

            normalized_name = filename_to_collection(collection_name)
            self.image_save_dir = f"data/images/{normalized_name}"
        else:
            self.image_save_dir = "data/images/unified"

        self.text_analyzer = TextAnalyzer()

    def analyze_document(
        self, pdf_path: str, extract_keywords: bool = True
    ) -> DocumentAnalyzerState:
        """
        ë¬¸ì„œ ì¢…í•© ë¶„ì„


        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            extract_keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€


        Returns:
            DocumentAnalyzerState: ë¶„ì„ ê²°ê³¼
        """
        state = create_document_analyzer_state(pdf_path, self.collection_name)

        try:
            # 1. ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±
            print("ğŸ“„ 1ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±")
            blocks = parse_pdf_unified(
                pdf_path, self.collection_name, generate_questions=False
            )
            state["blocks"] = blocks
            state["total_blocks"] = len(blocks)

            # ë¸”ë¡ íƒ€ì…ë³„ í†µê³„
            state["text_blocks"] = len(
                [
                    b
                    for b in blocks
                    if b.get("type") in ["paragraph", "section", "heading"]
                ]
            )
            state["table_blocks"] = len([b for b in blocks if b.get("type") == "table"])
            state["image_blocks"] = len([b for b in blocks if b.get("type") == "image"])

            # 2. í…ìŠ¤íŠ¸ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (ì„ íƒì )
            if extract_keywords:
                print("\nğŸ“ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ")
                # keyword_summary.py í•¨ìˆ˜ ì‚¬ìš© (document_analyzerë¡œ ì´ë™)
                from .tools.keyword_summary import extract_keywords_and_summary

                try:
                    analysis_result = extract_keywords_and_summary(
                        blocks, pdf_path.split("/")[-1]
                    )
                    content_analysis = analysis_result.get("content_analysis", {})

                    state["keywords"] = content_analysis.get("key_concepts", [])
                    state["summary"] = content_analysis.get("summary", "")
                    state["main_topics"] = content_analysis.get("main_topics", [])

                    print(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ:")
                    print(f"   - í‚¤ì›Œë“œ: {len(state['keywords'])}ê°œ")
                    print(f"   - ì£¼ì œ: {len(state['main_topics'])}ê°œ")
                    print(f"   - ìš”ì•½: {state['summary'][:50]}...")

                except Exception as e:
                    print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    state["keywords"] = []
                    state["summary"] = f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                    state["main_topics"] = []

            state["processing_status"] = "completed"
            state["error_message"] = None

            # 3. ChromaDB ìë™ ì—…ë¡œë“œ (ì„ íƒì )
            if self.auto_upload_chromadb and self.collection_name:
                print("\nğŸ“¤ 3ë‹¨ê³„: ChromaDB ìë™ ì—…ë¡œë“œ")
                uploaded_count = self._upload_to_chromadb(blocks, pdf_path)
                state["chromadb_uploaded"] = uploaded_count > 0
                state["chromadb_upload_count"] = uploaded_count
                if uploaded_count > 0:
                    print(f"âœ… ChromaDB ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}ê°œ ì²­í¬")
                else:
                    print("âš ï¸ ChromaDB ì—…ë¡œë“œ ì‹¤íŒ¨")
            else:
                state["chromadb_uploaded"] = False
                state["chromadb_upload_count"] = 0

            # 4. ê²°ê³¼ ì €ì¥
            self._save_results(state, pdf_path, extract_keywords)

        except Exception as e:
            state["processing_status"] = "failed"
            state["error_message"] = str(e)
            print(f"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {e}")

        return state

    def parse_structure_only(self, pdf_path: str) -> List[Dict]:
        """êµ¬ì¡° íŒŒì‹±ë§Œ ìˆ˜í–‰"""
        return parse_pdf_unified(
            pdf_path, self.collection_name, generate_questions=False
        )

        """
        ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±ë§Œ ìˆ˜í–‰ (í‚¤ì›Œë“œ ì¶”ì¶œ ì—†ì´)

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            List[Dict]: êµ¬ì¡°í™”ëœ ë¸”ë¡ë“¤
        """
        return parse_pdf_unified(
            pdf_path, self.collection_name, generate_questions=False
        )

    def analyze_text_only(self, text: str, collection_name: str = None) -> Dict:
        """í…ìŠ¤íŠ¸ ë¶„ì„ë§Œ ìˆ˜í–‰"""
        return self.text_analyzer.analyze_text(
            text, collection_name or self.collection_name or "unknown"
        )

        """
        í…ìŠ¤íŠ¸ ë¶„ì„ë§Œ ìˆ˜í–‰ (êµ¬ì¡° íŒŒì‹± ì—†ì´)

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            collection_name: ì»¬ë ‰ì…˜ëª… (ì˜µì…˜)

        Returns:
            Dict: í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        """
        return self.text_analyzer.analyze_text(
            text, collection_name or self.collection_name or "unknown"
        )

    def _extract_all_text(self, blocks: List[Dict]) -> str:
        """
        ë¸”ë¡ë“¤ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ

        Args:
            blocks: ë¬¸ì„œ ë¸”ë¡ë“¤

        Returns:
            str: ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸
        """
        text_parts = []

        for block in blocks:
            block_type = block.get("type", "")
            content = block.get("content", "")

            if block_type in ["paragraph", "heading", "section"] and content:
                text_parts.append(str(content))
            elif block_type == "table" and isinstance(content, dict):
                # í‘œ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                table_text = self._table_to_text(content)
                if table_text:
                    text_parts.append(table_text)

        return "\n".join(text_parts)

    def _save_results(
        self, state: DocumentAnalyzerState, pdf_path: str, extract_keywords: bool
    ):
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ë¶„ëœ ë””ë ‰í† ë¦¬ì— ì €ì¥

        Args:
            state: ë¶„ì„ ìƒíƒœ
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            extract_keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€
        """
        import json
        import os
        from datetime import datetime

        filename = os.path.basename(pdf_path).replace(".pdf", "")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # í‚¤ì›Œë“œ/ìš”ì•½ ê²°ê³¼ ì €ì¥

        # í‚¤ì›Œë“œ/ìš”ì•½ ê²°ê³¼ ì €ì¥ (collection ëª… ê¸°ë°˜ ë””ë ‰í† ë¦¬)
        if extract_keywords and (state.get("keywords") or state.get("summary")):
            # Collection ëª… ê¸°ë°˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°
            collection_dir = self.collection_name or "default"
            keywords_dir = f"data/outputs/keywords_summary/{collection_dir}"
            os.makedirs(keywords_dir, exist_ok=True)

            keywords_data = {
                "document_info": {
                    "source_file": os.path.basename(pdf_path),
                    "collection_name": self.collection_name,
                    "processing_date": datetime.now().isoformat(),
                    "analysis_type": "keywords_and_summary",
                    "analysis_type": "keywords_and_summary",
                },
                "content_analysis": {
                    "keywords": state.get("keywords", []),
                    "main_topics": state.get("main_topics", []),
                    "summary": state.get("summary", ""),
                    "keywords_count": len(state.get("keywords", [])),
                    "topics_count": len(state.get("main_topics", [])),
                },
                "document_stats": {
                    "total_blocks": state.get("total_blocks", 0),
                    "text_blocks": state.get("text_blocks", 0),
                    "table_blocks": state.get("table_blocks", 0),
                    "image_blocks": state.get("image_blocks", 0),
                },
            }

            keywords_file = (
                f"{keywords_dir}/{filename}_keywords_summary_{timestamp}.json"
            )
            with open(keywords_file, "w", encoding="utf-8") as f:
                json.dump(keywords_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ í‚¤ì›Œë“œ/ìš”ì•½ ì €ì¥: {keywords_file}")

        # ì „ì²´ ë¶„ì„ ê²°ê³¼ ì €ì¥ (ë¸”ë¡ í¬í•¨) - question_generationì„ ìœ„í•´
        analysis_results_dir = (
            f"data/outputs/document_analysis/{self.collection_name or 'default'}"
        )
        os.makedirs(analysis_results_dir, exist_ok=True)

        analysis_result_data = {
            "document_info": {
                "source_file": os.path.basename(pdf_path),
                "collection_name": self.collection_name,
                "processing_date": datetime.now().isoformat(),
                "analysis_type": "full_analysis",
            },
            "analysis_result": state,
            "pipeline_info": {
                "pipeline_type": "document_analysis",
                "pdf_path": pdf_path,
                "collection_name": self.collection_name,
                "extract_keywords": extract_keywords,
                "processing_time": 0,  # ì‹¤ì œ ì‹œê°„ì€ pipelineì—ì„œ ê³„ì‚°
                "timestamp": datetime.now().isoformat(),
            },
        }

        analysis_file = (
            f"{analysis_results_dir}/{filename}_analysis_result_{timestamp}.json"
        )
        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump(analysis_result_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ì „ì²´ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_file}")

    def _table_to_text(self, table_data: Dict) -> str:
        """
        í‘œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            table_data: í‘œ ë°ì´í„°

        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        if not isinstance(table_data, dict) or "data" not in table_data:
            return ""

        headers = table_data.get("headers", [])
        data = table_data.get("data", [])

        text_parts = []
        if headers:
            text_parts.append(" ".join(str(h) for h in headers))

        for row in data:
            text_parts.append(" ".join(str(cell) for cell in row))

        return " ".join(text_parts)

    def _save_test_summary(
        self,
        state: DocumentAnalyzerState,
        pdf_path: str,
        questions: List[Dict],
        timestamp: str,
    ):
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ì €ì¥"""
        import json
        import os
        from datetime import datetime

        filename = os.path.basename(pdf_path).replace(".pdf", "")
        summary_dir = "data/outputs/test_summaries"
        os.makedirs(summary_dir, exist_ok=True)

        # ë¬¸ì œ ë‚œì´ë„ë³„ ë¶„ì„
        objective_questions = [q for q in questions if q.get("type") == "OBJECTIVE"]
        subjective_questions = [q for q in questions if q.get("type") == "SUBJECTIVE"]

        # ì£¼ìš” í‚¤ì›Œë“œì™€ ì£¼ì œ ë¶„ì„
        keywords = state.get("keywords", [])
        main_topics = state.get("main_topics", [])

        # í…ŒìŠ¤íŠ¸ ìš”ì•½ ë°ì´í„° êµ¬ì„±
        test_summary_data = {
            "test_overview": {
                "title": f"{filename} - ìë™ ìƒì„± í…ŒìŠ¤íŠ¸",
                "description": f"'{filename}' ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ìë™ ìƒì„±í•œ í‰ê°€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                "source_document": os.path.basename(pdf_path),
                "creation_date": datetime.now().isoformat(),
                "test_type": "ì¢…í•© í‰ê°€",
                "estimated_duration": "30-45ë¶„",
            },
            "content_analysis": {
                "document_summary": (
                    state.get("summary", "")[:300] + "..."
                    if len(state.get("summary", "")) > 300
                    else state.get("summary", "")
                ),
                "key_concepts": keywords[:10],  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
                "main_topics": main_topics[:5],  # ìƒìœ„ 5ê°œ ì£¼ì œ
                "content_complexity": self._analyze_content_complexity(
                    state, questions
                ),
            },
            "test_structure": {
                "total_questions": len(questions),
                "question_breakdown": {
                    "objective": {
                        "count": len(objective_questions),
                        "percentage": (
                            round(len(objective_questions) / len(questions) * 100, 1)
                            if questions
                            else 0
                        ),
                        "focus_areas": self._extract_question_topics(
                            objective_questions
                        ),
                    },
                    "subjective": {
                        "count": len(subjective_questions),
                        "percentage": (
                            round(len(subjective_questions) / len(questions) * 100, 1)
                            if questions
                            else 0
                        ),
                        "focus_areas": self._extract_question_topics(
                            subjective_questions
                        ),
                    },
                },
                "difficulty_distribution": {
                    "easy": len(
                        [q for q in questions if q.get("difficulty_level") == "EASY"]
                    ),
                    "normal": len(
                        [q for q in questions if q.get("difficulty_level") == "NORMAL"]
                    ),
                    "hard": len(
                        [q for q in questions if q.get("difficulty_level") == "HARD"]
                    ),
                },
            },
            "assessment_guidelines": {
                "objective_scoring": "ê° ê°ê´€ì‹ ë¬¸í•­ë‹¹ 1ì , ì •ë‹µ/ì˜¤ë‹µìœ¼ë¡œ ì±„ì ",
                "subjective_scoring": "ë¬¸í•­ë³„ ë°°ì ì— ë”°ë¼ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬ ê°€ëŠ¥",
                "total_points": len(objective_questions)
                + len(subjective_questions) * 5,  # ê°ê´€ì‹ 1ì , ì£¼ê´€ì‹ 5ì 
                "passing_criteria": "ì´ì ì˜ 60% ì´ìƒ íšë“ ì‹œ í•©ê²©",
                "special_instructions": [
                    "ì£¼ê´€ì‹ ë¬¸í•­ì€ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ì™€ ë…¼ë¦¬ì  êµ¬ì„±ì„ ì¤‘ì  í‰ê°€",
                    "ë¬¸ì„œì˜ í•µì‹¬ ê°œë… ì´í•´ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨",
                    "ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•œ í‰ê°€ ê¶Œì¥",
                ],
            },
            "usage_recommendations": {
                "target_audience": "ë¬¸ì„œ ë‚´ìš© í•™ìŠµì ë° ê´€ë ¨ ì—…ë¬´ ë‹´ë‹¹ì",
                "prerequisite_knowledge": "ê¸°ë³¸ì ì¸ ë¬¸ì„œ ë‚´ìš© ì´í•´",
                "application_scenarios": [
                    "í•™ìŠµ ì™„ë£Œ í›„ ì´í•´ë„ ì ê²€",
                    "ì—…ë¬´ ìˆ™ë ¨ë„ í‰ê°€",
                    "êµìœ¡ í”„ë¡œê·¸ë¨ íš¨ê³¼ ì¸¡ì •",
                ],
                "follow_up_actions": [
                    "ì˜¤ë‹µ ë¬¸í•­ì— ëŒ€í•œ ì¶”ê°€ í•™ìŠµ",
                    "ì•½ì  ì˜ì—­ ì§‘ì¤‘ ë³´ì™„",
                    "ì‹¤ë¬´ ì ìš© ì—°ìŠµ",
                ],
            },
        }

        summary_file = f"{summary_dir}/{filename}_test_summary_{timestamp}.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(test_summary_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì €ì¥: {summary_file}")

    def _save_test_config(
        self,
        state: DocumentAnalyzerState,
        pdf_path: str,
        questions: List[Dict],
        timestamp: str,
    ):
        """í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ì €ì¥"""
        import json
        import os
        from datetime import datetime

        filename = os.path.basename(pdf_path).replace(".pdf", "")
        config_dir = "data/outputs/test_configs"
        os.makedirs(config_dir, exist_ok=True)

        objective_questions = [q for q in questions if q.get("type") == "OBJECTIVE"]
        subjective_questions = [q for q in questions if q.get("type") == "SUBJECTIVE"]

        # í…ŒìŠ¤íŠ¸ ì„¤ì • ë°ì´í„° êµ¬ì„±
        test_config_data = {
            "test_metadata": {
                "config_version": "1.0",
                "test_id": f"auto_test_{timestamp}",
                "source_document": os.path.basename(pdf_path),
                "generation_system": "SKIB-AI Document Analyzer",
                "creation_timestamp": datetime.now().isoformat(),
            },
            "test_settings": {
                "time_limit": {
                    "total_minutes": max(
                        30, len(questions) * 2
                    ),  # ìµœì†Œ 30ë¶„, ë¬¸í•­ë‹¹ 2ë¶„
                    "warning_at_minutes": max(25, len(questions) * 2 - 5),
                    "automatic_submit": True,
                },
                "question_settings": {
                    "randomize_order": False,
                    "allow_review": True,
                    "show_progress": True,
                    "one_question_per_page": False,
                },
                "submission_settings": {
                    "allow_multiple_attempts": False,
                    "save_progress": True,
                    "require_all_answers": False,
                },
            },
            "scoring_configuration": {
                "objective_questions": {
                    "points_per_question": 1,
                    "negative_marking": False,
                    "partial_credit": False,
                },
                "subjective_questions": {
                    "points_per_question": 5,
                    "allow_partial_credit": True,
                    "manual_grading_required": True,
                    "grading_rubric": "í‚¤ì›Œë“œ ê¸°ë°˜ + ë…¼ë¦¬ì  êµ¬ì„± í‰ê°€",
                },
                "total_points": len(objective_questions)
                + len(subjective_questions) * 5,
                "grade_scale": {
                    "A": {"min_percentage": 90, "description": "ìš°ìˆ˜"},
                    "B": {"min_percentage": 80, "description": "ì–‘í˜¸"},
                    "C": {"min_percentage": 70, "description": "ë³´í†µ"},
                    "D": {"min_percentage": 60, "description": "ë¯¸í¡"},
                    "F": {"min_percentage": 0, "description": "ë¶ˆí•©ê²©"},
                },
            },
            "question_configuration": {
                "total_questions": len(questions),
                "question_types": {
                    "objective": {
                        "count": len(objective_questions),
                        "format": "multiple_choice",
                        "options_per_question": 4,
                        "scoring_method": "correct_answer_only",
                    },
                    "subjective": {
                        "count": len(subjective_questions),
                        "format": "essay",
                        "max_characters": 1000,
                        "scoring_method": "rubric_based",
                    },
                },
                "difficulty_levels": {
                    "easy": len(
                        [q for q in questions if q.get("difficulty_level") == "EASY"]
                    ),
                    "normal": len(
                        [q for q in questions if q.get("difficulty_level") == "NORMAL"]
                    ),
                    "hard": len(
                        [q for q in questions if q.get("difficulty_level") == "HARD"]
                    ),
                },
            },
            "grading_criteria": {
                "objective_grading": {
                    "method": "automatic",
                    "correct_answer_points": 1,
                    "incorrect_answer_points": 0,
                },
                "subjective_grading": {
                    "method": "manual_with_ai_assistance",
                    "evaluation_criteria": [
                        {
                            "criterion": "ë‚´ìš© ì •í™•ì„±",
                            "weight": 40,
                            "description": "ë‹µë³€ ë‚´ìš©ì˜ ì‚¬ì‹¤ì  ì •í™•ì„±",
                        },
                        {
                            "criterion": "í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨",
                            "weight": 30,
                            "description": "ë¬¸ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ìš©ì–´ ì‚¬ìš©",
                        },
                        {
                            "criterion": "ë…¼ë¦¬ì  êµ¬ì„±",
                            "weight": 20,
                            "description": "ë‹µë³€ì˜ ë…¼ë¦¬ì  íë¦„ê³¼ êµ¬ì¡°",
                        },
                        {
                            "criterion": "ì™„ì„±ë„",
                            "weight": 10,
                            "description": "ë‹µë³€ì˜ ì™„ì „ì„±ê³¼ ì¶©ì‹¤ë„",
                        },
                    ],
                },
            },
            "accessibility_settings": {
                "font_size_adjustable": True,
                "high_contrast_mode": True,
                "screen_reader_compatible": True,
                "keyboard_navigation": True,
            },
            "security_settings": {
                "prevent_copy_paste": False,
                "disable_print_screen": False,
                "session_timeout_minutes": 120,
                "ip_restriction": False,
            },
        }

        config_file = f"{config_dir}/{filename}_test_config_{timestamp}.json"
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(test_config_data, f, ensure_ascii=False, indent=2)
        print(f"âš™ï¸ í…ŒìŠ¤íŠ¸ ì„¤ì • ì €ì¥: {config_file}")

    def _analyze_content_complexity(
        self, state: DocumentAnalyzerState, questions: List[Dict]
    ) -> str:
        """ì½˜í…ì¸  ë³µì¡ë„ ë¶„ì„"""
        keywords_count = len(state.get("keywords", []))
        topics_count = len(state.get("main_topics", []))
        hard_questions = len(
            [q for q in questions if q.get("difficulty_level") == "HARD"]
        )

        if keywords_count > 10 and topics_count > 5 and hard_questions > 2:
            return "ê³ ê¸‰"
        elif keywords_count > 5 and topics_count > 3:
            return "ì¤‘ê¸‰"
        else:
            return "ì´ˆê¸‰"

    def _extract_question_topics(self, questions: List[Dict]) -> List[str]:
        """ë¬¸ì œì—ì„œ ì£¼ìš” ì£¼ì œ ì¶”ì¶œ"""
        topics = []
        for q in questions[:3]:  # ìƒìœ„ 3ê°œ ë¬¸ì œë§Œ ë¶„ì„
            question_text = q.get("question", "")
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥)
            if "í”„ë¡œì„¸ìŠ¤" in question_text:
                topics.append("í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬")
            if "ì—…ë¬´" in question_text:
                topics.append("ì—…ë¬´ ì²˜ë¦¬")
            if "ê³„ì•½" in question_text:
                topics.append("ê³„ì•½ ê´€ë¦¬")
            if "ë“±ë¡" in question_text:
                topics.append("ë“±ë¡ ì ˆì°¨")

        return list(set(topics))[:3]  # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 3ê°œ ë°˜í™˜

    def _upload_to_chromadb(self, blocks: List[Dict], pdf_path: str) -> int:
        """
        ë¸”ë¡ë“¤ì„ ChromaDBì— ì—…ë¡œë“œ

        Args:
            blocks: ì—…ë¡œë“œí•  ë¸”ë¡ë“¤
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            int: ì—…ë¡œë“œëœ ë¸”ë¡ ìˆ˜
        """
        try:
            import os

            from db.vectorDB.chromaDB import upload_documents

            source_file = os.path.basename(pdf_path)
            uploaded_count = upload_documents(blocks, self.collection_name, source_file)
            return uploaded_count

        except ImportError:
            print(
                "âš ï¸ ChromaDB ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            )
            return 0
        except Exception as e:
            print(f"âŒ ChromaDB ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return 0


# í¸ì˜ í•¨ìˆ˜ë“¤
def analyze_document_complete(
    pdf_path: str,
    collection_name: str = None,
    extract_keywords: bool = True,
    auto_upload_chromadb: bool = True,
) -> DocumentAnalyzerState:
    """
    ë¬¸ì„œ ì¢…í•© ë¶„ì„ í¸ì˜ í•¨ìˆ˜ (ChromaDB ìë™ ì—…ë¡œë“œ í¬í•¨)

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        collection_name: ì»¬ë ‰ì…˜ëª…
        extract_keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€
        auto_upload_chromadb: ChromaDB ìë™ ì—…ë¡œë“œ ì—¬ë¶€

    Returns:
        DocumentAnalyzerState: ë¶„ì„ ê²°ê³¼
    """
    agent = DocumentAnalyzerAgent(collection_name, auto_upload_chromadb)
    return agent.analyze_document(pdf_path, extract_keywords)


def parse_document_structure(pdf_path: str, collection_name: str = None) -> List[Dict]:
    """
    ë¬¸ì„œ êµ¬ì¡° íŒŒì‹± í¸ì˜ í•¨ìˆ˜

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        collection_name: ì»¬ë ‰ì…˜ëª…

    Returns:
        List[Dict]: êµ¬ì¡°í™”ëœ ë¸”ë¡ë“¤
    """
    return parse_pdf_unified(pdf_path, collection_name, generate_questions=False)
