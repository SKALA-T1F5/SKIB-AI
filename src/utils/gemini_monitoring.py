"""
Gemini ëª¨ë¸ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°

Google Gemini API ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„:
https://ai.google.dev/gemini-api/docs/tokens?hl=ko&lang=python
"""

import json
import os
from datetime import datetime
from typing import Dict, Any, Optional
import google.generativeai as genai
from langsmith import traceable


class GeminiMonitor:
    """Gemini ëª¨ë¸ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    # 2024ë…„ ê¸°ì¤€ Gemini API ê°€ê²© ì •ë³´ (USD per 1M tokens)
    PRICING = {
        # Gemini 1.5 Pro
        "gemini-1.5-pro": {
            "input": 1.25,  # $1.25 per 1M tokens
            "output": 5.00,  # $5.00 per 1M tokens
            "context_cache": 0.3125,  # $0.3125 per 1M tokens
        },
        "gemini-1.5-pro-latest": {
            "input": 1.25,
            "output": 5.00,
            "context_cache": 0.3125,
        },
        
        # Gemini 1.5 Flash (Free tier available)
        "gemini-1.5-flash": {
            "input": 0.075,  # $0.075 per 1M tokens (after free tier)
            "output": 0.30,   # $0.30 per 1M tokens (after free tier)
            "context_cache": 0.01875,
        },
        "gemini-1.5-flash-latest": {
            "input": 0.075,
            "output": 0.30,
            "context_cache": 0.01875,
        },
        "gemini-1.5-flash-8b": {
            "input": 0.0375,  # $0.0375 per 1M tokens
            "output": 0.15,   # $0.15 per 1M tokens
            "context_cache": 0.009375,
        },
        
        # Gemini 2.0 Flash
        "gemini-2.0-flash": {
            "input": 0.10,   # $0.10 per 1M tokens
            "output": 0.40,  # $0.40 per 1M tokens
        },
        "gemini-2.0-flash-exp": {
            "input": 0.10,
            "output": 0.40,
        },
        
        # Gemini 2.5 Flash
        "gemini-2.5-flash": {
            "input": 0.075,
            "output": 0.30,
        },
        "gemini-2.5-flash-exp": {
            "input": 0.075,
            "output": 0.30,
        },
        
        # Gemini 2.5 Pro (Most expensive)
        "gemini-2.5-pro": {
            "input": 1.25,   # $1.25 per 1M tokens (up to 200k tokens)
            "output": 10.00,  # $10.00 per 1M tokens (up to 200k tokens)
            "input_large": 2.50,   # $2.50 per 1M tokens (>200k tokens)
            "output_large": 15.00,  # $15.00 per 1M tokens (>200k tokens)
        },
        
        # Gemini 1.0 Pro (Legacy)
        "gemini-1.0-pro": {
            "input": 0.50,   # $0.50 per 1M tokens
            "output": 1.50,  # $1.50 per 1M tokens
        },
        "gemini-pro": {  # Alias for gemini-1.0-pro
            "input": 0.50,
            "output": 1.50,
        },
    }
    
    # ë©€í‹°ëª¨ë‹¬ í† í° ë¹„ìš©
    MULTIMODAL_TOKENS = {
        "image_small": 258,      # Images â‰¤ 384x384
        "video_per_second": 263, # Video tokens per second
        "audio_per_second": 32,  # Audio tokens per second
    }
    
    def __init__(self, log_file: str = "data/logs/gemini_usage.jsonl"):
        """
        Args:
            log_file: ì‚¬ìš©ëŸ‰ ë¡œê·¸ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        self.log_file = log_file
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    def count_tokens_before_request(self, model_name: str, prompt: str) -> int:
        """
        ìš”ì²­ ì „ í† í° ìˆ˜ ê³„ì‚° (ê³µì‹ API ì‚¬ìš©)
        
        Args:
            model_name: Gemini ëª¨ë¸ëª…
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            
        Returns:
            ì˜ˆìƒ í† í° ìˆ˜
        """
        try:
            model = genai.GenerativeModel(model_name)
            result = model.count_tokens(prompt)
            return result.total_tokens
        except Exception as e:
            print(f"âš ï¸ í† í° ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ëŒ€ëµì ì¸ ì¶”ì •ì¹˜ ë°˜í™˜ (4ìë‹¹ 1í† í°)
            return len(prompt) // 4
    
    def calculate_cost(self, model_name: str, usage_metadata: Any) -> Dict[str, float]:
        """
        ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ê³„ì‚°
        
        Args:
            model_name: ì‚¬ìš©ëœ Gemini ëª¨ë¸ëª…
            usage_metadata: response.usage_metadata ê°ì²´
            
        Returns:
            ë¹„ìš© ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if model_name not in self.PRICING:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return {
                "input_cost": 0.0,
                "output_cost": 0.0,
                "total_cost": 0.0,
                "currency": "USD"
            }
        
        pricing = self.PRICING[model_name]
        
        # í† í° ìˆ˜ ì¶”ì¶œ
        input_tokens = getattr(usage_metadata, 'prompt_token_count', 0)
        output_tokens = getattr(usage_metadata, 'candidates_token_count', 0)
        
        # Gemini 2.5 Proì˜ ê²½ìš° ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ìš”ê¸ˆ ì ìš©
        if model_name == "gemini-2.5-pro" and input_tokens > 200000:
            input_price = pricing["input_large"]
            output_price = pricing["output_large"]
        else:
            input_price = pricing["input"]
            output_price = pricing["output"]
        
        # ë¹„ìš© ê³„ì‚° (1M í† í°ë‹¹ ê°€ê²©)
        input_cost = (input_tokens / 1_000_000) * input_price
        output_cost = (output_tokens / 1_000_000) * output_price
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
            "currency": "USD",
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "input_price_per_1m": input_price,
            "output_price_per_1m": output_price,
        }
    
    def log_usage(
        self, 
        model_name: str, 
        usage_metadata: Any, 
        function_name: str = None,
        additional_metadata: Dict = None
    ):
        """
        ì‚¬ìš©ëŸ‰ ë¡œê·¸ ì €ì¥
        
        Args:
            model_name: ëª¨ë¸ëª…
            usage_metadata: ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„°
            function_name: í˜¸ì¶œí•œ í•¨ìˆ˜ëª…
            additional_metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        """
        cost_info = self.calculate_cost(model_name, usage_metadata)
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "model": model_name,
            "function": function_name,
            "usage": {
                "input_tokens": cost_info["input_tokens"],
                "output_tokens": cost_info["output_tokens"],
                "total_tokens": cost_info["total_tokens"],
            },
            "cost": {
                "input_cost": cost_info["input_cost"],
                "output_cost": cost_info["output_cost"],
                "total_cost": cost_info["total_cost"],
                "currency": cost_info["currency"],
                "input_price_per_1m": cost_info["input_price_per_1m"],
                "output_price_per_1m": cost_info["output_price_per_1m"],
            },
            "metadata": additional_metadata or {}
        }
        
        # ë¡œê·¸ íŒŒì¼ì— ì €ì¥
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        return log_entry
    
    def print_usage_summary(self, model_name: str, usage_metadata: Any):
        """ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥"""
        cost_info = self.calculate_cost(model_name, usage_metadata)
        
        print(f"ğŸ“Š Gemini ì‚¬ìš©ëŸ‰ ìš”ì•½ ({model_name})")
        print(f"  ğŸ”¤ Input tokens: {cost_info['input_tokens']:,}")
        print(f"  ğŸ“ Output tokens: {cost_info['output_tokens']:,}")
        print(f"  ğŸ“Š Total tokens: {cost_info['total_tokens']:,}")
        print(f"  ğŸ’° Input cost: ${cost_info['input_cost']:.6f}")
        print(f"  ğŸ’° Output cost: ${cost_info['output_cost']:.6f}")
        print(f"  ğŸ’° Total cost: ${cost_info['total_cost']:.6f}")
        
        # ë¬´ë£Œ í‹°ì–´ ì •ë³´ í‘œì‹œ
        if model_name in ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-1.0-pro"]:
            print(f"  â„¹ï¸  Note: {model_name}ì€ ë¬´ë£Œ í‹°ì–´ ì œí•œì´ ìˆìŠµë‹ˆë‹¤")


# ë°ì½”ë ˆì´í„° í•¨ìˆ˜
def monitor_gemini_usage(model_name: str, function_name: str = None):
    """
    Gemini ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°
    
    Usage:
        @monitor_gemini_usage("gemini-2.0-flash-exp", "question_generator")
        def my_function():
            # Gemini API í˜¸ì¶œ
            response = model.generate_content(...)
            return response
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            monitor = GeminiMonitor()
            result = func(*args, **kwargs)
            
            # response ê°ì²´ì—ì„œ usage_metadata ì¶”ì¶œ
            if hasattr(result, 'usage_metadata'):
                monitor.print_usage_summary(model_name, result.usage_metadata)
                monitor.log_usage(
                    model_name, 
                    result.usage_metadata, 
                    function_name or func.__name__
                )
            
            return result
        return wrapper
    return decorator


# LangSmithì™€ í†µí•©ëœ ë˜í¼ í´ë˜ìŠ¤
class TrackedGeminiModel:
    """LangSmith ì¶”ì ê³¼ ë¹„ìš© ëª¨ë‹ˆí„°ë§ì´ í†µí•©ëœ Gemini ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model_name: str, monitor: GeminiMonitor = None):
        self.model_name = model_name
        self.model = genai.GenerativeModel(model_name)
        self.monitor = monitor or GeminiMonitor()
    
    @traceable(
        run_type="llm",
        name="Gemini Generate Content"
    )
    def generate_content(self, prompt: str, **kwargs):
        """LangSmith ì¶”ì ê³¼ ë¹„ìš© ëª¨ë‹ˆí„°ë§ì´ í¬í•¨ëœ ì½˜í…ì¸  ìƒì„±"""
        
        # ìš”ì²­ ì „ í† í° ìˆ˜ ê³„ì‚°
        estimated_tokens = self.monitor.count_tokens_before_request(
            self.model_name, prompt
        )
        print(f"ğŸ“ ì˜ˆìƒ ì…ë ¥ í† í°: {estimated_tokens:,}")
        
        # ì½˜í…ì¸  ìƒì„±
        response = self.model.generate_content(prompt, **kwargs)
        
        # ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        if hasattr(response, 'usage_metadata'):
            self.monitor.print_usage_summary(self.model_name, response.usage_metadata)
            self.monitor.log_usage(
                self.model_name, 
                response.usage_metadata,
                function_name="generate_content"
            )
        
        return response


# ê°„í¸ ì‚¬ìš©ì„ ìœ„í•œ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_monitored_model(model_name: str) -> TrackedGeminiModel:
    """ëª¨ë‹ˆí„°ë§ì´ í™œì„±í™”ëœ Gemini ëª¨ë¸ ìƒì„±"""
    return TrackedGeminiModel(model_name)


# ì‚¬ìš©ëŸ‰ ë¶„ì„ í•¨ìˆ˜
def analyze_usage_logs(log_file: str = "data/logs/gemini_usage.jsonl") -> Dict[str, Any]:
    """ì‚¬ìš©ëŸ‰ ë¡œê·¸ ë¶„ì„"""
    if not os.path.exists(log_file):
        return {"error": "ë¡œê·¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
    
    total_cost = 0.0
    total_tokens = 0
    model_usage = {}
    function_usage = {}
    
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                entry = json.loads(line.strip())
                
                # ì´ ë¹„ìš© ë° í† í° ìˆ˜
                total_cost += entry["cost"]["total_cost"]
                total_tokens += entry["usage"]["total_tokens"]
                
                # ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰
                model = entry["model"]
                if model not in model_usage:
                    model_usage[model] = {"count": 0, "cost": 0.0, "tokens": 0}
                model_usage[model]["count"] += 1
                model_usage[model]["cost"] += entry["cost"]["total_cost"]
                model_usage[model]["tokens"] += entry["usage"]["total_tokens"]
                
                # í•¨ìˆ˜ë³„ ì‚¬ìš©ëŸ‰
                func = entry.get("function", "unknown")
                if func not in function_usage:
                    function_usage[func] = {"count": 0, "cost": 0.0, "tokens": 0}
                function_usage[func]["count"] += 1
                function_usage[func]["cost"] += entry["cost"]["total_cost"]
                function_usage[func]["tokens"] += entry["usage"]["total_tokens"]
                
            except json.JSONDecodeError:
                continue
    
    return {
        "summary": {
            "total_cost": total_cost,
            "total_tokens": total_tokens,
            "currency": "USD"
        },
        "by_model": model_usage,
        "by_function": function_usage
    }